{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def getGithubRepos(username: str) -> str:\n",
    "    details = \"\"  # string to return that has GitHub details such as latest projects, contributions, etc\n",
    "\n",
    "    url = f\"https://github.com/{username}?tab=repositories\"\n",
    "    response = requests.get(url)\n",
    "\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    repo_names = soup.find_all('a', {'itemprop': 'name codeRepository'})\n",
    "    hrefs = [element.get('href') for element in repo_names]\n",
    "    links=[\"https://github.com\"+str(element) for element in hrefs]\n",
    "    mds = [\"https://raw.githubusercontent.com\"+str(element)+\"/master/README.md\" for element in hrefs]\n",
    "    mds=mds[:6]\n",
    "    \n",
    "    # print(mds)\n",
    "    for md in mds:\n",
    "        response = requests.get(md)\n",
    "        if response.status_code == 200:\n",
    "            details+=(response.text[500:])\n",
    "\n",
    "\n",
    "    return details\n",
    "\n",
    "print(getGithubRepos('danikhan632'))\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def getGithubPinned(username: str):\n",
    "    details = \"\"  # string to return that has GitHub details such as latest projects, contributions, etc\n",
    "\n",
    "    url = f\"https://github.com/{username}?tab=overview&from=2023-09-01&to=2023-09-30\"\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        pinned_items = soup.find_all('div', class_='pinned-item-list-item-content')\n",
    "        \n",
    "        repos = []\n",
    "        for item in pinned_items:\n",
    "            repo_name = item.find('span', class_='repo').text.strip()\n",
    "            description = item.find('p', class_='pinned-item-desc').text.strip() if item.find('p', class_='pinned-item-desc') else 'No description'\n",
    "            programming_language = item.find('span', itemprop='programmingLanguage').text.strip() if item.find('span', itemprop='programmingLanguage') else 'Not specified'\n",
    "            stars_tag = item.select_one('a[href*=\"/stargazers\"]')\n",
    "            forks_tag = item.select_one('a[href*=\"/forks\"]')\n",
    "            stars = (stars_tag.text.strip()) if stars_tag else 0\n",
    "            forks = (forks_tag.text.strip()) if forks_tag else 0\n",
    "            repo_info = { \n",
    "                'repository_name': repo_name,\n",
    "                'repository_url': f'{username}/{repo_name}',\n",
    "                'description': description, \n",
    "                'programming_language': programming_language, \n",
    "                'stars': stars, \n",
    "                'forks': forks \n",
    "            }\n",
    "            repos.append(repo_info)\n",
    "            details=repos\n",
    "\n",
    "    else:\n",
    "        details = \"Failed to retrieve the webpage\"\n",
    "    \n",
    "    return details\n",
    "\n",
    "# print(getGithubPinned('danikhan632'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def getGithubPinned(username: str):\n",
    "    repos = []\n",
    "\n",
    "    url = f\"https://github.com/{username}?tab=overview&from=2023-09-01&to=2023-09-30\"\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        pinned_items = soup.find_all('div', class_='pinned-item-list-item-content')\n",
    "        \n",
    "        repos = []\n",
    "        for item in pinned_items:\n",
    "            repo_name = item.find('span', class_='repo').text.strip()\n",
    "\n",
    "            repos.append(repo_name)\n",
    "    return repos\n",
    "\n",
    "# print(getGithubPinned('danikhan632'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def getGithubDetails(username: str):\n",
    "    details = \"\"  # string to return that has GitHub details such as latest projects, contributions, etc\n",
    "\n",
    "    url = f\"https://github.com/{username}?tab=overview&from=2023-09-01&to=2023-01-30\"\n",
    "    response = requests.get(url)\n",
    "\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        root = soup.find_all('div', class_='pinned-item-list-item-content')\n",
    "        pinned_items = soup.find_all('div', class_='pinned-item-desc color-fg-muted text-small mt-2 mb-0')\n",
    "        # print(pinned_items)\n",
    "        \n",
    "        p_tags = []\n",
    "        for item in pinned_items:\n",
    "            p_tags.extend(item.find_all('p'))\n",
    "        \n",
    "        # Extracting text from p tags\n",
    "        p_texts = [p.get_text(separator='\\n', strip=True) for p in p_tags]\n",
    "        details += '\\n'.join(p_texts)\n",
    "        \n",
    "        return details\n",
    "        \n",
    "    else:\n",
    "        return \"Failed to retrieve the webpage\"\n",
    "    \n",
    "    \n",
    "    \n",
    "# print(getGithubDetails('danikhan632'))\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def getGithubRepos(username: str):\n",
    "    url = f\"https://api.github.com/users/{username}/repos\"\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        repos = response.json()\n",
    "        print(len(repos))\n",
    "        sorted_repos = sorted(repos, key=lambda x: x['updated_at'], reverse=True)\n",
    "        return sorted_repos\n",
    "    else:\n",
    "        return \"Failed to retrieve the repositories\"\n",
    "\n",
    "sorted_repos = getGithubRepos('danikhan632')\n",
    "\n",
    "# Print the name and updated_at field of each repository\n",
    "for repo in sorted_repos:\n",
    "    print(f\"{repo['name']}: {repo['pushed_at']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from github import Github\n",
    "\n",
    "# Authentication is defined via github.Auth\n",
    "from github import Auth\n",
    "\n",
    "# using an access token\n",
    "# auth = Auth.Token(\"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\")\n",
    "auth = Auth.Token(\"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\")\n",
    "\n",
    "\n",
    "# Public Web Github\n",
    "g = Github(auth=auth)\n",
    "repo = g.get_repo(\"PyGithub/PyGithub\")\n",
    "# print(repo.stargazers_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinned_repos = getGithubPinned('danikhan632')\n",
    "# print(pinned_repos)\n",
    "for pinned in pinned_repos:\n",
    "    url = pinned['repository_url']\n",
    "    repo = g.get_repo(url)\n",
    "    # print(repo.get_clones_traffic())\n",
    "    print(repo.get_issues().get_page(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processPR(url:str):\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        repos = response.json()\n",
    "        print(repos['commit']['tree'])\n",
    "        # print(repos['author'])\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = g.get_user(\"Significant-Gravitas\")\n",
    "user_events=user.get_events().get_page(0)\n",
    "# print(help(user))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = g.get_user(\"taliu02\")\n",
    "user_events=user.get_events().get_page(0)\n",
    "\n",
    "for event in user_events:\n",
    "    # print((event.actor.name,event.created_at,event.payload))\n",
    "    print(event.type)\n",
    "    if event.type==\"PushEvent\":\n",
    "        print(event.payload)\n",
    "        for commit in event.payload['commits']:\n",
    "            print(commit['message'])\n",
    "        \n",
    "    elif event.type==\"WatchEvent\":\n",
    "        print(event.payload)\n",
    "    elif event.type==\"IssueCommentEvent\":\n",
    "        print(event.payload['issue']['title'])\n",
    "        print(event.payload['issue']['body'])\n",
    "    elif event.type==\"PullRequestEvent\":\n",
    "        # print(event.payload)\n",
    "        print(event.payload['pull_request']['title'])\n",
    "        print(event.payload['pull_request']['body'])\n",
    "# print(user_events[0].payload['commits'][0]['message'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "user = g.get_user(\"danikhan632\")\n",
    "repos=[]\n",
    "for repo in user.get_repos():\n",
    "    repos.append(repo)\n",
    "    \n",
    "print(len(repos))\n",
    "\n",
    "print((repos))\n",
    "print(repos[0].stargazers_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repo Name: danikhan632/apex\n",
      "Commit Message: added sparisty density\n",
      "Repo Name: danikhan632/vk-backend-for-triton\n",
      "Commit Message: first commit\n",
      "Issue Title: codellama support?\n",
      "Issue Body: is this possible? or have to redo training? or ?\n",
      "Issue Title: AttributeError: 'NoneType' object has no attribute 'config'\n",
      "Issue Body: Hi,\n",
      "\n",
      "Thank you for your work on this project. I really need Guidance support.\n",
      "\n",
      "I'm having issues with loading your extension, I installed the requirements, but I'm getting the following error:\n",
      "\n",
      "```\n",
      "2023-09-03 13:24:32 INFO:Loading the extension \"guidance_api\"...\n",
      "starting guidance server\n",
      "2023-09-03 13:24:32 ERROR:Failed to load the extension \"guidance_api\".\n",
      "Traceback (most recent call last):\n",
      "  File \"E:\\Apps\\oobabooga_windows\\oobabooga_windows\\text-generation-webui\\modules\\extensions.py\", line 40, in load_extensions\n",
      "    extension.setup()\n",
      "  File \"E:\\Apps\\oobabooga_windows\\oobabooga_windows\\text-generation-webui\\extensions\\guidance_api\\script.py\", line 134, in setup\n",
      "    gen =GuidanceGenerator()\n",
      "  File \"E:\\Apps\\oobabooga_windows\\oobabooga_windows\\text-generation-webui\\extensions\\guidance_api\\guidance_gen.py\", line 36, in __init__\n",
      "    self.data= setup_model_data()\n",
      "  File \"E:\\Apps\\oobabooga_windows\\oobabooga_windows\\text-generation-webui\\extensions\\guidance_api\\model_info.py\", line 13, in setup_model_data\n",
      "    config_dict = vars(shared.model.config)\n",
      "AttributeError: 'NoneType' object has no attribute 'config'\n",
      "```\n",
      "\n",
      "Issue Title: DeepSpeed Triton backend failure on GPT-J inference\n",
      "Issue Body: Hi, I succesfully built triton 2.1.0 on commit 5df904233c11a65bd131ead7268f84cca7804275 with xpu-backend at 0bcc485f82b34d49494bd0264bacc24a20aafb7a and added some modifications to be compatible with ipex-2.0. And I can successfully pass xpu pytest in triton with `pytest -xvs test/backend/third_party_backends/test_xpu_backend.py --backend xpu`.\n",
      "\n",
      "However, when I run DeepSpeed inference on GPT-J, there are some errors like `Translate to SPIRV IR failedLLVM ERROR: Failed to translate TritonGPU to SPIRV IR.` Is the support still limited to specific operations or it's a bug? Also, I cannot successfully compile lastest main branch of intel-xpu-backend-for-triton with error:\n",
      "```bash\n",
      "/home/xxx/Project/triton/third_party/intel_xpu_backend/lib/Conversion/TritonGPUToSPIRV/Utility.h:11:62: error: template argument 1 is invalid\n",
      "   11 | #define ptrtoint(...) rewriter.create<spirv::ConvertPtrToUOp>(loc, __VA_ARGS__)\n",
      "      |                       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~\n",
      "/home/xxx/Project/triton/third_party/intel_xpu_backend/lib/Conversion/TritonGPUToSPIRV/ConvertLayoutOpToSPIRV.cpp:220:25: note: in expansion of macro ‘ptrtoint’\n",
      "  220 |               currVal = ptrtoint(llvmElemTy, currVal);\n",
      "      |                         ^~~~~~~~\n",
      "/home/xxx/Project/triton/third_party/intel_xpu_backend/lib/Conversion/TritonGPUToSPIRV/Utility.h:10:46: error: ‘ConvertUToPtrOp’ is not a member of ‘mlir::spirv’; did you mean ‘ConvertUToFOp’?\n",
      "   10 | #define inttoptr(...) rewriter.create<spirv::ConvertUToPtrOp>(loc, __VA_ARGS__)\n",
      "      |                                              ^~~~~~~~~~~~~~~\n",
      "/home/xxx/Project/triton/third_party/intel_xpu_backend/lib/Conversion/TritonGPUToSPIRV/Utility.h:10:46: note: in definition of macro ‘inttoptr’\n",
      "   10 | #define inttoptr(...) rewriter.create<spirv::ConvertUToPtrOp>(loc, __VA_ARGS__)\n",
      "      |                                              ^~~~~~~~~~~~~~~\n",
      "/home/xxx/Project/triton/third_party/intel_xpu_backend/lib/Conversion/TritonGPUToSPIRV/Utility.h:10:62: error: no matching function for call to ‘mlir::ConversionPatternRewriter::create<<expression error> >(mlir::Location&, mlir::Type&, mlir::Value&)’\n",
      "   10 | #define inttoptr(...) rewriter.create<spirv::ConvertUToPtrOp>(loc, __VA_ARGS__)\n",
      "      |                       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~\n",
      "```\n",
      "\n",
      "Complete log:\n",
      "```bash\n",
      "[0] [2023-08-03 01:28:35,085] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.10.1+7de73909, git-hash=7de73909, git-branch=master\n",
      "[0] [2023-08-03 01:28:35,085] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter mp_size is deprecated use tensor_parallel.tp_size instead\n",
      "[0] [2023-08-03 01:28:35,085] [INFO] [logging.py:96:log_dist] [Rank 0] quantize_bits = 8 mlp_extra_grouping = False, quantize_groups = 1\n",
      "[0] [2023-08-03 01:28:35,130] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed-Inference config: {'layer_id': 0, 'hidden_size': 4096, 'intermediate_size': 16384, 'heads': 16, 'num_hidden_layers': -1, 'dtype': torch.float16, 'pre_layer_norm': True, 'norm_type': <NormType.LayerNorm: 1>, 'local_rank': -1, 'stochastic_mode': False, 'epsilon': 1e-05, 'mp_size': 1, 'scale_attention': True, 'triangular_masking': True, 'local_attention': False, 'window_size': 1, 'rotary_dim': 64, 'rotate_half': False, 'rotate_every_two': True, 'return_tuple': True, 'mlp_after_attn': False, 'mlp_act_func_type': <ActivationFuncType.GELU: 1>, 'specialized_mode': False, 'training_mp_size': 1, 'bigscience_bloom': False, 'max_out_tokens': 1024, 'min_out_tokens': 1, 'scale_attn_by_inverse_layer_idx': False, 'enable_qkv_quantization': False, 'use_mup': False, 'return_single_tuple': False, 'set_empty_params': False, 'transposed_mode': False, 'use_triton': True, 'triton_autotune': False}\n",
      "[0] [2023-08-03 01:28:35,130] [INFO] [logging.py:96:log_dist] [Rank 0] Injecting Triton kernels ...\n",
      "Loading 1 checkpoint shards:   0%|          | 0/1 [00:00<?, ?it/s][0] checkpoint loading time at rank 0: 48.9048216342926 sec\n",
      "Loading 1 checkpoint shards: 100%|██████████| 1/1 [00:48<00:00, 48.90s/it]\n",
      "[0] [2023-08-03 01:29:24,846] [INFO] [utils.py:798:see_memory_usage] post-ds-inference-init\n",
      "[0] [2023-08-03 01:29:24,846] [INFO] [utils.py:799:see_memory_usage] MA 11.27 GB         Max_MA 11.27 GB         CA 11.27 GB         Max_CA 11 GB\n",
      "[0] [2023-08-03 01:29:24,846] [INFO] [utils.py:806:see_memory_usage] CPU Virtual Memory:  used = 11.66 GB, percent = 1.2%\n",
      "[0] *** Starting to generate 32 tokens with bs=1\n",
      "[0] Generate args {'max_new_tokens': 32, 'do_sample': False, 'num_beams': 1, 'token_latency': True}\n",
      "[0] *** Prompt size:  32\n",
      "[0] [2023-08-03 01:29:24,933] [INFO] [utils.py:798:see_memory_usage] end-of-run\n",
      "[0] [2023-08-03 01:29:24,934] [INFO] [utils.py:799:see_memory_usage] MA 11.27 GB         Max_MA 11.27 GB         CA 11.27 GB         Max_CA 11 GB\n",
      "[0] [2023-08-03 01:29:24,934] [INFO] [utils.py:806:see_memory_usage] CPU Virtual Memory:  used = 11.66 GB, percent = 1.2%\n",
      "[0] *** Running benchmark[0]\n",
      "[0] Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "[0] >>> inner function (32,)\n",
      "[0] <function get_backend at 0x7f6b81fe3910>\n",
      "[0] <triton.third_party.xpu.XPUBackend object at 0x7f6acea6d960>\n",
      "[0] /home/xxx/Project/frameworks.ai.pytorch.ipex-gpu.rls/intel_extension_for_pytorch/xpu/cpp_extension.py:1373: UserWarning: This extension has static linked onednn library. Please attaction to                 that, this path of onednn version maybe not match with the built-in version.\n",
      "[0]   warnings.warn(\n",
      "[0] 2023-08-03 01:29:25,509 - root - INFO - running build_ext\n",
      "[0] 2023-08-03 01:29:25,514 - root - INFO - building 'layer_norm_kernel' extension\n",
      "[0] 2023-08-03 01:29:25,514 - root - INFO - creating /tmp/tmp2a44hft2/tmp\n",
      "[0] 2023-08-03 01:29:25,514 - root - INFO - creating /tmp/tmp2a44hft2/tmp/tmp2a44hft2\n",
      "[0] 2023-08-03 01:29:25,515 - root - INFO - /opt/intel/oneapi/compiler/2023.1.0/linux/bin/icx -I/home/xxx/miniconda3/envs/llm/lib/python3.10/site-packages/torch/include -I/home/xxx/miniconda3/envs/llm/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/home/xxx/miniconda3/envs/llm/lib/python3.10/site-packages/torch/include/TH -I/opt/intel/oneapi/compiler/2023.1.0/linux/include -I/opt/intel/oneapi/compiler/2023.1.0/linux/include/sycl -I/opt/intel/oneapi/mkl/2023.1.0/include -I/home/xxx/onednn/include -I/home/xxx/Project/frameworks.ai.pytorch.ipex-gpu.rls/intel_extension_for_pytorch/include -I/home/xxx/miniconda3/envs/llm/include/python3.10 -c /tmp/tmp2a44hft2/main.c -o /tmp/tmp2a44hft2/tmp/tmp2a44hft2/main.o -fPIC -fPIC -w -fsycl -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1016\\\" -DTORCH_EXTENSION_NAME=layer_norm_kernel -D_GLIBCXX_USE_CXX11_ABI=1 -std=c++17\n",
      "[0] 2023-08-03 01:29:34,504 - root - INFO - /opt/intel/oneapi/compiler/2023.1.0/linux/bin/icpx -shared -fsycl /tmp/tmp2a44hft2/tmp/tmp2a44hft2/main.o -lze_loader -o /tmp/tmp2a44hft2/layer_norm_kernel.cpython-310-x86_64-linux-gnu.so -lc10 -ltorch_cpu -ltorch -ltorch_python -L/home/xxx/miniconda3/envs/llm/lib/python3.10/site-packages/torch/lib -L/home/xxx/Project/frameworks.ai.pytorch.ipex-gpu.rls/intel_extension_for_pytorch/lib -L/home/xxx/onednn/lib -Wl,--start-group /opt/intel/oneapi/mkl/2023.1.0/lib/intel64/libmkl_sycl.a /opt/intel/oneapi/mkl/2023.1.0/lib/intel64/libmkl_intel_ilp64.a /opt/intel/oneapi/mkl/2023.1.0/lib/intel64/libmkl_sequential.a /opt/intel/oneapi/mkl/2023.1.0/lib/intel64/libmkl_core.a -Wl,--end-group -lsycl -lOpenCL -lpthread -lm -ldl -ldnnl -lintel-ext-pt-gpu\n",
      "[0] >>> inner function <function Fp16Matmul.forward.<locals>.<lambda> at 0x7f6ace1720e0>\n",
      "[0] <function get_backend at 0x7f6b81fe3910>\n",
      "[0] <triton.third_party.xpu.XPUBackend object at 0x7f6acea6d960>[0]\n",
      "[0] /home/xxx/Project/frameworks.ai.pytorch.ipex-gpu.rls/intel_extension_for_pytorch/xpu/cpp_extension.py:1373: UserWarning: This extension has static linked onednn library. Please attaction to                 that, this path of onednn version maybe not match with the built-in version.\n",
      "[0]   warnings.warn(\n",
      "[0] 2023-08-03 01:29:50,809 - root - INFO - running build_ext\n",
      "[0] 2023-08-03 01:29:50,812 - root - INFO - building '_fp_matmul' extension\n",
      "[0] 2023-08-03 01:29:50,812 - root - INFO - creating /tmp/tmp7aguxrst/tmp\n",
      "[0] 2023-08-03 01:29:50,812 - root - INFO - creating /tmp/tmp7aguxrst/tmp/tmp7aguxrst\n",
      "[0] 2023-08-03 01:29:50,812 - root - INFO - /opt/intel/oneapi/compiler/2023.1.0/linux/bin/icx -I/home/xxx/miniconda3/envs/llm/lib/python3.10/site-packages/torch/include -I/home/xxx/miniconda3/envs/llm/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/home/xxx/miniconda3/envs/llm/lib/python3.10/site-packages/torch/include/TH -I/opt/intel/oneapi/compiler/2023.1.0/linux/include -I/opt/intel/oneapi/compiler/2023.1.0/linux/include/sycl -I/opt/intel/oneapi/mkl/2023.1.0/include -I/home/xxx/onednn/include -I/home/xxx/Project/frameworks.ai.pytorch.ipex-gpu.rls/intel_extension_for_pytorch/include -I/home/xxx/miniconda3/envs/llm/include/python3.10 -c /tmp/tmp7aguxrst/main.c -o /tmp/tmp7aguxrst/tmp/tmp7aguxrst/main.o -fPIC -fPIC -w -fsycl -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1016\\\" -DTORCH_EXTENSION_NAME=_fp_matmul -D_GLIBCXX_USE_CXX11_ABI=1 -std=c++17\n",
      "[0] 2023-08-03 01:29:59,847 - root - INFO - /opt/intel/oneapi/compiler/2023.1.0/linux/bin/icpx -shared -fsycl /tmp/tmp7aguxrst/tmp/tmp7aguxrst/main.o -lze_loader -o /tmp/tmp7aguxrst/_fp_matmul.cpython-310-x86_64-linux-gnu.so -lc10 -ltorch_cpu -ltorch -ltorch_python -L/home/xxx/miniconda3/envs/llm/lib/python3.10/site-packages/torch/lib -L/home/xxx/Project/frameworks.ai.pytorch.ipex-gpu.rls/intel_extension_for_pytorch/lib -L/home/xxx/onednn/lib -Wl,--start-group /opt/intel/oneapi/mkl/2023.1.0/lib/intel64/libmkl_sycl.a /opt/intel/oneapi/mkl/2023.1.0/lib/intel64/libmkl_intel_ilp64.a /opt/intel/oneapi/mkl/2023.1.0/lib/intel64/libmkl_sequential.a /opt/intel/oneapi/mkl/2023.1.0/lib/intel64/libmkl_core.a -Wl,--end-group -lsycl -lOpenCL -lpthread -lm -ldl -ldnnl -lintel-ext-pt-gpu\n",
      "[0] >>> inner function <function Fp16Matmul._score_4d_matmul.<locals>.<lambda> at 0x7f6ace1720e0>\n",
      "[0] <function get_backend at 0x7f6b81fe3910>\n",
      "[0] <triton.third_party.xpu.XPUBackend object at 0x7f6acea6d960>\n",
      "[0] /home/xxx/Project/frameworks.ai.pytorch.ipex-gpu.rls/intel_extension_for_pytorch/xpu/cpp_extension.py:1373: UserWarning: This extension has static linked onednn library. Please attaction to                 that, this path of onednn version maybe not match with the built-in version.\n",
      "[0]   warnings.warn(\n",
      "[0] 2023-08-03 01:30:23,065 - root - INFO - running build_ext\n",
      "[0] 2023-08-03 01:30:23,068 - root - INFO - building 'matmul_4d_kernel' extension\n",
      "[0] 2023-08-03 01:30:23,069 - root - INFO - creating /tmp/tmpsihcyg_g/tmp\n",
      "[0] 2023-08-03 01:30:23,069 - root - INFO - creating /tmp/tmpsihcyg_g/tmp/tmpsihcyg_g\n",
      "[0] 2023-08-03 01:30:23,069 - root - INFO - /opt/intel/oneapi/compiler/2023.1.0/linux/bin/icx -I/home/xxx/miniconda3/envs/llm/lib/python3.10/site-packages/torch/include -I/home/xxx/miniconda3/envs/llm/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/home/xxx/miniconda3/envs/llm/lib/python3.10/site-packages/torch/include/TH -I/opt/intel/oneapi/compiler/2023.1.0/linux/include -I/opt/intel/oneapi/compiler/2023.1.0/linux/include/sycl -I/opt/intel/oneapi/mkl/2023.1.0/include -I/home/xxx/onednn/include -I/home/xxx/Project/frameworks.ai.pytorch.ipex-gpu.rls/intel_extension_for_pytorch/include -I/home/xxx/miniconda3/envs/llm/include/python3.10 -c /tmp/tmpsihcyg_g/main.c -o /tmp/tmpsihcyg_g/tmp/tmpsihcyg_g/main.o -fPIC -fPIC -w -fsycl -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1016\\\" -DTORCH_EXTENSION_NAME=matmul_4d_kernel -D_GLIBCXX_USE_CXX11_ABI=1 -std=c++17\n",
      "[0] 2023-08-03 01:30:32,197 - root - INFO - /opt/intel/oneapi/compiler/2023.1.0/linux/bin/icpx -shared -fsycl /tmp/tmpsihcyg_g/tmp/tmpsihcyg_g/main.o -lze_loader -o /tmp/tmpsihcyg_g/matmul_4d_kernel.cpython-310-x86_64-linux-gnu.so -lc10 -ltorch_cpu -ltorch -ltorch_python -L/home/xxx/miniconda3/envs/llm/lib/python3.10/site-packages/torch/lib -L/home/xxx/Project/frameworks.ai.pytorch.ipex-gpu.rls/intel_extension_for_pytorch/lib -L/home/xxx/onednn/lib -Wl,--start-group /opt/intel/oneapi/mkl/2023.1.0/lib/intel64/libmkl_sycl.a /opt/intel/oneapi/mkl/2023.1.0/lib/intel64/libmkl_intel_ilp64.a /opt/intel/oneapi/mkl/2023.1.0/lib/intel64/libmkl_sequential.a /opt/intel/oneapi/mkl/2023.1.0/lib/intel64/libmkl_core.a -Wl,--end-group -lsycl -lOpenCL -lpthread -lm -ldl -ldnnl -lintel-ext-pt-gpu\n",
      "[0] >>> inner function (512,)\n",
      "[0] <function get_backend at 0x7f6b81fe3910>\n",
      "[0] <triton.third_party.xpu.XPUBackend object at 0x7f6acea6d960>\n",
      "[0] /home/xxx/Project/frameworks.ai.pytorch.ipex-gpu.rls/intel_extension_for_pytorch/xpu/cpp_extension.py:1373: UserWarning: This extension has static linked onednn library. Please attaction to                 that, this path of onednn version maybe not match with the built-in version.\n",
      "[0]   warnings.warn(\n",
      "[0] 2023-08-03 01:30:49,560 - root - INFO - running build_ext\n",
      "[0] 2023-08-03 01:30:49,563 - root - INFO - building 'softmax_kernel' extension\n",
      "[0] 2023-08-03 01:30:49,563 - root - INFO - creating /tmp/tmpxm55vb5j/tmp\n",
      "[0] 2023-08-03 01:30:49,563 - root - INFO - creating /tmp/tmpxm55vb5j/tmp/tmpxm55vb5j\n",
      "[0] 2023-08-03 01:30:49,563 - root - INFO - /opt/intel/oneapi/compiler/2023.1.0/linux/bin/icx -I/home/xxx/miniconda3/envs/llm/lib/python3.10/site-packages/torch/include -I/home/xxx/miniconda3/envs/llm/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/home/xxx/miniconda3/envs/llm/lib/python3.10/site-packages/torch/include/TH -I/opt/intel/oneapi/compiler/2023.1.0/linux/include -I/opt/intel/oneapi/compiler/2023.1.0/linux/include/sycl -I/opt/intel/oneapi/mkl/2023.1.0/include -I/home/xxx/onednn/include -I/home/xxx/Project/frameworks.ai.pytorch.ipex-gpu.rls/intel_extension_for_pytorch/include -I/home/xxx/miniconda3/envs/llm/include/python3.10 -c /tmp/tmpxm55vb5j/main.c -o /tmp/tmpxm55vb5j/tmp/tmpxm55vb5j/main.o -fPIC -fPIC -w -fsycl -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1016\\\" -DTORCH_EXTENSION_NAME=softmax_kernel -D_GLIBCXX_USE_CXX11_ABI=1 -std=c++17\n",
      "[0] 2023-08-03 01:30:58,436 - root - INFO - /opt/intel/oneapi/compiler/2023.1.0/linux/bin/icpx -shared -fsycl /tmp/tmpxm55vb5j/tmp/tmpxm55vb5j/main.o -lze_loader -o /tmp/tmpxm55vb5j/softmax_kernel.cpython-310-x86_64-linux-gnu.so -lc10 -ltorch_cpu -ltorch -ltorch_python -L/home/xxx/miniconda3/envs/llm/lib/python3.10/site-packages/torch/lib -L/home/xxx/Project/frameworks.ai.pytorch.ipex-gpu.rls/intel_extension_for_pytorch/lib -L/home/xxx/onednn/lib -Wl,--start-group /opt/intel/oneapi/mkl/2023.1.0/lib/intel64/libmkl_sycl.a /opt/intel/oneapi/mkl/2023.1.0/lib/intel64/libmkl_intel_ilp64.a /opt/intel/oneapi/mkl/2023.1.0/lib/intel64/libmkl_sequential.a /opt/intel/oneapi/mkl/2023.1.0/lib/intel64/libmkl_core.a -Wl,--end-group -lsycl -lOpenCL -lpthread -lm -ldl -ldnnl -lintel-ext-pt-gpu\n",
      "[0] loc(\"-\":17:13): error: external function is unhandled\n",
      "[0] Translate to SPIRV IR failedLLVM ERROR: Failed to translate TritonGPU to SPIRV IR.\n",
      "\n",
      "===================================================================================\n",
      "=   BAD TERMINATION OF ONE OF YOUR APPLICATION PROCESSES\n",
      "=   RANK 0 PID 1046382 RUNNING AT \n",
      "=   KILLED BY SIGNAL: 6 (Aborted)\n",
      "===================================================================================\n",
      "```\n",
      "Issue Title: llama2 support?\n",
      "Issue Body: is this possible? or have to redo training? or ?\n",
      "Issue Title: non-nvidia backends not working\n",
      "Issue Body: Working on a Apple silicon backend and the project had quite the architecture shift, from my testing as well, cant get rocm to work either as Nvidia-GPUs seem to be the only ones working. Kinda looks like this new structure is pretty baked in with Nvidia.\n",
      "Will the structure get a bit more abstract soon?\n",
      "\n",
      "I'm guessing repo is a bit stressed dealing with hopper support but no pressure, just don't want to see the M1 support I've been working on down the drain.\n",
      "Issue Title: non-nvidia backends not working\n",
      "Issue Body: Working on a Apple silicon backend and the project had quite the architecture shift, from my testing as well, cant get rocm to work either as Nvidia-GPUs seem to be the only ones working. Kinda looks like this new structure is pretty baked in with Nvidia.\n",
      "Will the structure get a bit more abstract soon?\n",
      "\n",
      "I'm guessing repo is a bit stressed dealing with hopper support but no pressure, just don't want to see the M1 support I've been working on down the drain.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Fetch all pages of events\n",
    "all_events = []\n",
    "events = user.get_events()\n",
    "for event in events:\n",
    "    all_events.append(event)\n",
    "\n",
    "# Now, all_events contains all the events from all pages\n",
    "for event in all_events:\n",
    " \n",
    "    if event.type == \"PushEvent\":\n",
    "        print(\"Repo Name:\", event.repo.name)\n",
    "        for commit in event.payload['commits']:\n",
    "            print(\"Commit Message:\", commit['message'])\n",
    "        \n",
    "        \n",
    "    elif event.type == \"IssueCommentEvent\":\n",
    "        print(\"Issue Title:\", event.payload['issue']['title'])\n",
    "        print(\"Issue Body:\", event.payload['issue']['body'])\n",
    "        \n",
    "    elif event.type == \"PullRequestEvent\":\n",
    "        print(\"PR Title:\", event.payload['pull_request']['title'])\n",
    "        print(\"PR Body:\", event.payload['pull_request']['body'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_repositories(username: str):\n",
    "    user = g.get_user(username)\n",
    "\n",
    "    # Create a list of all repositories\n",
    "    all_repos = [repo for repo in user.get_repos()]\n",
    "\n",
    "    # Create a dictionary mapping repo names to repo objects\n",
    "    repo_dict = {repo.name: repo for repo in all_repos}\n",
    "\n",
    "    # Fetch the names of pinned repositories\n",
    "    pinned_repo_names = getGithubPinned(username)\n",
    "\n",
    "    # Create a list to store the pinned repo objects\n",
    "    pinned_repos = []\n",
    "\n",
    "    # For each pinned repo name, fetch the corresponding repo object\n",
    "    # and remove it from the dictionary to avoid duplicates\n",
    "    for name in pinned_repo_names:\n",
    "        if name in repo_dict:\n",
    "            pinned_repos.append(repo_dict.pop(name))\n",
    "\n",
    "    # Sort the remaining repositories by stargazers count in descending order\n",
    "    sorted_repos = sorted(repo_dict.values(), key=lambda x: x.stargazers_count, reverse=True)\n",
    "\n",
    "    # Combine the pinned repos and sorted repos\n",
    "    final_repo_list = pinned_repos + sorted_repos\n",
    "\n",
    "    return final_repo_list\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "from github import Github,Auth\n",
    "from github.Repository import Repository\n",
    "from typing import List\n",
    "import requests\n",
    "import os, json\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def getGithubPinned(username: str)-> List[str]:\n",
    "    repos = []\n",
    "    url = f\"https://github.com/{username}?tab=overview&from=2023-09-01&to=2023-09-30\"\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        pinned_items = soup.find_all('div', class_='pinned-item-list-item-content')\n",
    "        \n",
    "        repos = []\n",
    "        for item in pinned_items:\n",
    "            repo_name = item.find('span', class_='repo').text.strip()\n",
    "            repos.append(repo_name)\n",
    "    else:\n",
    "        print(f\"Failed to get pinned repos for {username}\")\n",
    "        \n",
    "    return repos\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_repositories(username: str)->List[Repository]:\n",
    "    user = g.get_user(username)\n",
    "    all_repos = [repo for repo in user.get_repos()]\n",
    "    repo_dict = {repo.name: repo for repo in all_repos}\n",
    "    pinned_repo_names = getGithubPinned(username)\n",
    "    pinned_repos = []\n",
    "    for name in pinned_repo_names:\n",
    "        if name in repo_dict:\n",
    "            pinned_repos.append(repo_dict.pop(name))\n",
    "    sorted_repos = sorted(repo_dict.values(), key=lambda x: x.stargazers_count, reverse=True)\n",
    "    final_repo_list = pinned_repos + sorted_repos\n",
    "    return final_repo_list\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository(full_name=\"danikhan632/vk-backend-for-triton\")\n",
      "Repository(full_name=\"danikhan632/iMessage-API\")\n",
      "Repository(full_name=\"danikhan632/Auto-GPT-AlpacaTrader-Plugin\")\n",
      "Repository(full_name=\"danikhan632/guidance\")\n",
      "Repository(full_name=\"danikhan632/guidance_api\")\n",
      "Repository(full_name=\"danikhan632/triton\")\n",
      "Repository(full_name=\"danikhan632/Auto-GPT-Messages-Plugin\")\n",
      "Repository(full_name=\"danikhan632/Auto-GPT-Text-Gen-Plugin\")\n",
      "Repository(full_name=\"danikhan632/Auto-GPT-Todoist-Plugin\")\n",
      "Repository(full_name=\"danikhan632/Auto-GPT-Google-Calandar-Plugin\")\n",
      "Repository(full_name=\"danikhan632/create-cpp-app\")\n",
      "Repository(full_name=\"danikhan632/LTSM-Stock-Crypto-Analyzer\")\n",
      "Repository(full_name=\"danikhan632/amazon-shopping-clone\")\n",
      "Repository(full_name=\"danikhan632/AmplifyFlutter\")\n",
      "Repository(full_name=\"danikhan632/apex\")\n",
      "Repository(full_name=\"danikhan632/ata_x86\")\n",
      "Repository(full_name=\"danikhan632/Auto-GPT\")\n",
      "Repository(full_name=\"danikhan632/Auto-GPT-Flutter-App\")\n",
      "Repository(full_name=\"danikhan632/Auto-GPT-Interrupt-Controller\")\n",
      "Repository(full_name=\"danikhan632/Auto-GPT-Plugins\")\n",
      "Repository(full_name=\"danikhan632/AutoGPT-Flutter-App\")\n",
      "Repository(full_name=\"danikhan632/AutoGPT-Interrupts-Plugin\")\n",
      "Repository(full_name=\"danikhan632/Bevy_proj\")\n",
      "Repository(full_name=\"danikhan632/CleanUpCrew\")\n",
      "Repository(full_name=\"danikhan632/COVID-Vaccine-Tracker\")\n",
      "Repository(full_name=\"danikhan632/danikhan632\")\n",
      "Repository(full_name=\"danikhan632/edamam-python\")\n",
      "Repository(full_name=\"danikhan632/FastChat\")\n",
      "Repository(full_name=\"danikhan632/FlutterTowerDefense\")\n",
      "Repository(full_name=\"danikhan632/GAN-Pokemon\")\n",
      "Repository(full_name=\"danikhan632/MagiskOnWSA\")\n",
      "Repository(full_name=\"danikhan632/Malaria-Dectector\")\n",
      "Repository(full_name=\"danikhan632/metal-dialect\")\n",
      "Repository(full_name=\"danikhan632/microservices_app\")\n",
      "Repository(full_name=\"danikhan632/MyFlutterApp\")\n",
      "Repository(full_name=\"danikhan632/pic8259_x86\")\n",
      "Repository(full_name=\"danikhan632/react-native-tauri-template\")\n",
      "Repository(full_name=\"danikhan632/robot-path-finder\")\n",
      "Repository(full_name=\"danikhan632/Self-Driving-Car\")\n",
      "Repository(full_name=\"danikhan632/text-generation-webui\")\n",
      "Repository(full_name=\"danikhan632/text-generation-webui-extensions\")\n",
      "Repository(full_name=\"danikhan632/tower_defense_game\")\n"
     ]
    }
   ],
   "source": [
    "username = 'danikhan632'\n",
    "repos = get_repositories(username)\n",
    "for repo in repos:\n",
    "    print(repo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vk-backend-for-triton\n",
      "b\"# Vulkan Backend for Triton\\n\\nThis is designed as a semi-hardware abstract backend enabling Vulkan Compatible Devices to utilize Triton.\\nCurrently the first real hardware backend planned is for Apple Metal devices using MoltenVK* however progress is currently being made more towards ensuring that it produces Valid SPIRV ASM with developement for Vulkan Compute pipelines and Pytorch Memory Management currently under construction\\n\\n# Credits\\nThis does use a lot of the code from Intel's Extension for Triton and I'd like to give a huge thanks to Intel and Eikan Wang. Also thanks to the triton team @ openai, none of this \\nwould be possible without their help.\\n[OpenAI Triton](https://github.com/openai/triton)\\n[Intel\\xc2\\xae Extension for Triton ](https://github.com/intel/intel-xpu-backend-for-triton)\\n\\n## Prerequisites\\n\\nAs of now please follow the instructions from [Intel\\xc2\\xae Extension for Triton ](https://github.com/intel/intel-xpu-backend-for-triton)\\nOnce down with that follow instructions below\\n## Build VK Backend\\n\\n```Bash\\n# Clone OpenAI/Triton\\ngit clone https://github.com/openai/triton.git\\ncd triton\\ngit checkout 5df904233c11a65bd131ead7268f84cca7804275\\n#note that this is an Older version of Triton hopefully will be rebased soon.\\n\\ncd third_party\\ngit clone https://github.com/danikhan632/vk-backend-for-triton.git\\nmv ./vk-backend-for-triton ./vk_backend\\n\\n```\\n\\nNow Build Triton with VK backend enabled:\\n\\n```Bash\\ncd {triton-root-dir}\\ncd python\\nTRITON_CODEGEN_VK_BACKEND=1 python setup.py develop\\n```\\n# Usage Guide\\n\\nAt present, the functionality of this software is limited, with only `libvk` being operational. We're actively working on it to generate Vulkan-compatible SPIRV. For a comprehensive understanding and usage details, please refer to our documentation.\\n\\n# Contributing\\n\\nWe welcome and encourage contributions from the community! Whether you're a seasoned developer or someone new to the field, your ideas, bug fixes, and enhancements can make a difference.\\n\\n**How to Contribute:**\\n1. Fork the repository.\\n2. Make your changes or additions.\\n3. Create a pull request with a detailed description of your updates.\\n\\nAlso feel free to reach out on [discord](https://discord.gg/Mg5zYBwt)\\n# License\\n\\nThis project is licensed under the MIT License. This means you're free to use, modify, distribute, and sublicense the software, provided you include the original copyright and permission notices. For complete terms and conditions, please refer to the `LICENSE` file in the repository.\\n\\n\"\n",
      "1\n",
      "iMessage-API\n",
      "Auto-GPT-AlpacaTrader-Plugin\n",
      "b\"# Auto-GPT-AlpacaTrader\\n\\n\\nAutoGPT plugin for Alpaca trader\\n[alpaca Trader](https://app.alpaca.markets/paper/dashboard/overview)\\n\\nAlpaca Trading API provides the following key features:\\n\\nTrade Execution: Users can place, modify, and cancel orders for stocks and ETFs programmatically, with support for various order types, such as market, limit, and stop orders.\\n\\nAccount Management: Users can access their account information, including balances, positions, and portfolio history.\\n\\nMarket Data: Alpaca Trading API offers both historical and real-time market data, including stock quotes, trades, and bar data (OHLCV). It also provides access to data on corporate actions, such as dividends and stock splits.\\n\\nPaper Trading: Alpaca offers a paper trading environment, allowing users to test their trading strategies and algorithms without risking real money. The paper trading API has the same interface as the live trading API, making it easy to switch between the two.\\n\\n\\nTo use the Alpaca Trading API, you need to sign up for an account at Alpaca (https://alpaca.markets/) and generate an API key.\\n![Alt Text](https://i.imgur.com/rGqmWwW.png)\\n\\n\\nPlease note that while Alpaca's trading services are commission-free, they may still be subject to various fees and regulations, such as SEC and FINRA fees. You should also be aware of the risks associated with algorithmic trading and ensure compliance with all applicable laws and regulations.\\n\\nNote I am not liable for any financial loss due to the plugin.\\nFeel free to use paper trading mode as much as you want if you plan on really\\nusing this, this is at your own financial risk. I would atleast recommend GPT-4.\\n\\n\\n### Plugin Installation Steps\\n\\nfor Linux, depending on distro\\n```\\nsudo apt-get install zip\\napk add zip\\nsudo pacman -S zip\\nsudo yum install zip\\n```\\nMac / Linux / WSL\\n```\\ncd plugins && git clone https://github.com/danikhan632/Auto-GPT-AlpacaTrader-Plugin.git && zip -r ./Auto-GPT-AlpacaTrader-Plugin.zip ./Auto-GPT-AlpacaTrader-Plugin && rm -rf ./Auto-GPT-AlpacaTrader-Plugin && cd .. && ./run.sh --install-plugin-deps\\n\\n```\\nWindows, Powershell\\n```\\ncd plugins; git clone https://github.com/danikhan632/Auto-GPT-AlpacaTrader-Plugin.git; Compress-Archive -Path .\\\\Auto-GPT-AlpacaTrader-Plugin -DestinationPath .\\\\Auto-GPT-AlpacaTrader-Plugin.zip; Remove-Item -Recurse -Force .\\\\Auto-GPT-AlpacaTrader-Plugin; cd ..\\n```\\n\\n\\n\\n5. **Allowlist the plugin (optional):**\\n   Add the plugin's class name to the `ALLOWLISTED_PLUGINS` in the `.env` file to avoid being prompted with a warning when loading the plugin:\\n\\n   ``` shell\\n   ALLOWLISTED_PLUGINS=AutoGPTAlpacaTraderPlugin\\n   APCA_API_KEY_ID=your_api_key\\n   APCA_API_SECRET_KEY=your_api_secret_key\\n   APCA_PAPER=True\\n   APCA_SAFE=True \\n   ```\\n   default for APCA_PAPER is True \\n   default for APCA_SAFE is True which restricts stocks that can be traded\\n\\n   If the plugin is not allowlisted, you will be warned before it's loaded. \\n   \\n   If you have not added the plugin to allowlisted or `APCA_IS_PAPER=True` in the .env file, you will have to generate a Paper account API key on the Alpaca.\\n\\n   Decent startup config, for ai_settings.yaml\\n   ```\\n   ai_goals:\\n- Analyze market trends and news to provide you with accurate and timely insights\\n  that inform your trading decisions.\\n- Execute trades quickly and efficiently based on your pre-set parameters and risk\\n  tolerance, ensuring that you never miss out on a profitable opportunity.\\n- Continuously monitor your portfolio and adjust your positions to optimize your returns\\n  and minimize risk.\\n- Provide personalized recommendations for new investments and diversification strategies\\n  based on your unique financial goals and risk profile.\\n- Keep you informed and up-to-date on the latest market developments and trends, so\\n  you can stay ahead of the curve and make informed decisions.\\nai_name: AlgoGPT\\nai_role: an AI-powered trading assistant that leverages the Alpaca plugin to provide\\n  real-time market analysis, execute trades, and optimize your portfolio for maximum\\n  returns.\\napi_budget: 0.0\\n```\\n\\n\\n\\n\\n\\nCredit where credit is due to [isaiahbjork](https://github.com/isaiahbjork/Auto-GPT-MetaTrader-Plugin/) this is meant to be pretty similar to his just using the\\nAlpaca Trading platform\\n\"\n",
      "104\n",
      "guidance\n",
      "b'<div align=\"right\"><a href=\"https://guidance.readthedocs.org\"><img src=\"https://readthedocs.org/projects/guidance/badge/?version=latest&style=flat\" /></a></div>\\n<div align=\"center\"><picture>\\n  <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/figures/guidance_logo_blue_dark.svg\">\\n  <img alt=\"guidance\" src=\"docs/figures/guidance_logo_blue.svg\" width=300\">\\n</picture></div>\\n<br/>\\n\\n> _Where there is no guidance, a model fails, but in an abundance of instructions there is safety._  \\n_\\\\- <a href=\"notebooks/proverb.ipynb\">GPT 11:14</a>_\\n\\n<!--It expands the API of language models so you can craft rich output structure, design precise tool use, create multi-agent interactions, and much more all while using clear code and maximum inference efficiency.-->\\n<b>Guidance</b> enables you to control modern language models more effectively and efficiently than traditional prompting or chaining. Guidance programs allow you to interleave generation, prompting, and logical control into a single continuous flow matching how the language model actually processes the text. Simple output structures like [Chain of Thought](https://arxiv.org/abs/2201.11903) and its many variants (e.g., [ART](https://arxiv.org/abs/2303.09014), [Auto-CoT](https://arxiv.org/abs/2210.03493), etc.) have been shown to improve LLM performance. The advent of more powerful LLMs like [GPT-4](https://openai.com/research/gpt-4) allows for even richer structure, and `guidance` makes that structure easier and cheaper.\\n\\nFeatures:\\n- [x] Simple, intuitive syntax, based on [Handlebars](https://handlebarsjs.com/) templating.\\n- [x] Rich output structure with multiple generations, selections, conditionals, tool use, etc.\\n- [x] Playground-like streaming in Jupyter/VSCode Notebooks.\\n- [x] Smart seed-based generation caching.\\n- [x] Support for role-based chat models (e.g., [ChatGPT](https://beta.openai.com/docs/guides/chat)).\\n- [x] Easy integration with Hugging Face models, including [guidance acceleration](notebooks/guidance_acceleration.ipynb) for speedups over standard prompting, [token healing](notebooks/token_healing.ipynb) to optimize prompt boundaries, and [regex pattern guides](notebooks/pattern_guides.ipynb) to enforce formats.\\n\\n## Install\\n\\n```python\\npip install guidance\\n```\\n\\n\\n                                     \\n## Live streaming (<a href=\"https://github.com/microsoft/guidance/blob/main/notebooks/proverb.ipynb\">notebook</a>)\\n\\nSpeed up your prompt development cycle by streaming complex templates and generations live in your notebook. At first glance, Guidance feels like a templating language, and just like standard <a href=\"https://handlebarsjs.com\">Handlebars</a> templates, you can do variable interpolation (e.g., `{{proverb}}`) and logical control. But unlike standard templating languages, guidance programs have a well defined linear execution order that directly corresponds to the token order as processed by the language model. This means that at any point during execution the language model can be used to generate text (using the `{{gen}}` command) or make logical control flow decisions. This interleaving of generation and prompting allows for precise output structure that produces clear and parsable results.\\n\\n```python\\nimport guidance\\n\\n# set the default language model used to execute guidance programs\\nguidance.llm = guidance.llms.OpenAI(\"text-davinci-003\")\\n\\n# define a guidance program that adapts a proverb\\nprogram = guidance(\"\"\"Tweak this proverb to apply to model instructions instead.\\n\\n{{proverb}}\\n- {{book}} {{chapter}}:{{verse}}\\n\\nUPDATED\\nWhere there is no guidance{{gen \\'rewrite\\' stop=\"\\\\\\\\n-\"}}\\n- GPT {{#select \\'chapter\\'}}9{{or}}10{{or}}11{{/select}}:{{gen \\'verse\\'}}\"\"\")\\n\\n# execute the program on a specific proverb\\nexecuted_program = program(\\n    proverb=\"Where there is no guidance, a people falls,\\\\nbut in an abundance of counselors there is safety.\",\\n    book=\"Proverbs\",\\n    chapter=11,\\n    verse=14\\n)\\n```\\n<img src=\"docs/figures/proverb_animation.gif\" width=\"404\">\\n\\nAfter a program is executed, all the generated variables are now easily accessible:\\n\\n```python\\nexecuted_program[\"rewrite\"]\\n```\\n> \\', a model fails,\\\\nbut in an abundance of instructions there is safety.\\'\\n\\n## Chat dialog (<a href=\"https://github.com/microsoft/guidance/blob/main/notebooks/chat.ipynb\">notebook</a>)\\n\\nGuidance supports API-based chat models like GPT-4, as well as open chat models like Vicuna through a unified API based on role tags (e.g., `{{#system}}...{{/system}}`). This allows interactive dialog development that combines rich templating and logical control with modern chat models.\\n\\n```python\\n# connect to a chat model like GPT-4 or Vicuna\\ngpt4 = guidance.llms.OpenAI(\"gpt-4\")\\n# vicuna = guidance.llms.transformers.Vicuna(\"your_path/vicuna_13B\", device_map=\"auto\")\\n\\nexperts = guidance(\\'\\'\\'\\n{{#system~}}\\nYou are a helpful and terse assistant.\\n{{~/system}}\\n\\n{{#user~}}\\nI want a response to the following question:\\n{{query}}\\nName 3 world-class experts (past or present) who would be great at answering this?\\nDon\\'t answer the question yet.\\n{{~/user}}\\n\\n{{#assistant~}}\\n{{gen \\'expert_names\\' temperature=0 max_tokens=300}}\\n{{~/assistant}}\\n\\n{{#user~}}\\nGreat, now please answer the question as if these experts had collaborated in writing a joint anonymous answer.\\n{{~/user}}\\n\\n{{#assistant~}}\\n{{gen \\'answer\\' temperature=0 max_tokens=500}}\\n{{~/assistant}}\\n\\'\\'\\', llm=gpt4)\\n\\nexperts(query=\\'How can I be more productive?\\')\\n```\\n<img src=\"docs/figures/chat_animation.gif\" width=\"619\">\\n\\n## Guidance acceleration (<a href=\"https://github.com/microsoft/guidance/blob/main/notebooks/guidance_acceleration.ipynb\">notebook</a>)\\n\\nWhen multiple generation or LLM-directed control flow statements are used in a single Guidance program then we can significantly improve inference performance by optimally reusing the Key/Value caches as we progress through the prompt. This means Guidance only asks the LLM to generate the green text below, not the entire program. **This cuts this prompt\\'s runtime in half vs. a standard generation approach.** \\n\\n````python\\n# we use LLaMA here, but any GPT-style model will do\\nllama = guidance.llms.Transformers(\"your_path/llama-7b\", device=0)\\n\\n# we can pre-define valid option sets\\nvalid_weapons = [\"sword\", \"axe\", \"mace\", \"spear\", \"bow\", \"crossbow\"]\\n\\n# define the prompt\\ncharacter_maker = guidance(\"\"\"The following is a character profile for an RPG game in JSON format.\\n```json\\n{\\n    \"id\": \"{{id}}\",\\n    \"description\": \"{{description}}\",\\n    \"name\": \"{{gen \\'name\\'}}\",\\n    \"age\": {{gen \\'age\\' pattern=\\'[0-9]+\\' stop=\\',\\'}},\\n    \"armor\": \"{{#select \\'armor\\'}}leather{{or}}chainmail{{or}}plate{{/select}}\",\\n    \"weapon\": \"{{select \\'weapon\\' options=valid_weapons}}\",\\n    \"class\": \"{{gen \\'class\\'}}\",\\n    \"mantra\": \"{{gen \\'mantra\\' temperature=0.7}}\",\\n    \"strength\": {{gen \\'strength\\' pattern=\\'[0-9]+\\' stop=\\',\\'}},\\n    \"items\": [{{#geneach \\'items\\' num_iterations=5 join=\\', \\'}}\"{{gen \\'this\\' temperature=0.7}}\"{{/geneach}}]\\n}```\"\"\")\\n\\n# generate a character\\ncharacter_maker(\\n    id=\"e1f491f7-7ab8-4dac-8c20-c92b5e7d883d\",\\n    description=\"A quick and nimble fighter.\",\\n    valid_weapons=valid_weapons, llm=llama\\n)\\n````\\n<img src=\"docs/figures/json_animation.gif\" width=\"565\">\\n\\nThe prompt above typically takes just over 2.5 seconds to complete on a A6000 GPU when using LLaMA 7B. If we were to run the same prompt adapted to be a single generation call (the standard practice today) it takes about 5 seconds to complete (4 of which is token generation and 1 of which is prompt processing). *This means Guidance acceleration delivers a 2x speedup over the standard approach for this prompt.* In practice the exact speed-up factor depends on the format of your specific prompt and the size of your model (larger models benefit more). Acceleration is also only supported for Transformers LLMs at the moment. See the [notebook](https://github.com/microsoft/guidance/blob/main/notebooks/guidance_acceleration.ipynb) for more details.\\n\\n\\nThis class allows integration with [text-generation-webui](https://github.com/oobabooga/text-generation-webui) which\\nallows for easy setup and enables running Larger Models with less VRAM. Additonally ensures that machine which runs guidance\\nand [text-generation-webui](https://github.com/oobabooga/text-generation-webui) infrence server to do not need to be the same.\\n\\n\\n\\n````python\\n\\nguidance.llm = guidance.llms.TGWUI(\"http://127.0.0.1:9000\")\\n\\n# define the prompt\\ncharacter_maker = guidance(\"\"\"The following is a character profile for a Soccer Game in JSON format.\\n```json\\n{\\n    \"Nationality\": \"{{nationality}}\",\\n    \"league\": \"{{league}}\",\\n    \"name\": \"{{gen \\'name\\'}}\",\\n    \"age\": {{gen \\'age\\' pattern=\\'[0-9]+\\' stop=\\',\\'}},\\n    \"overall\": {{gen \\'overall\\' pattern=\\'[0-9]+\\' stop=\\',\\'}},\\n    \"description\": \"{{gen \\'description\\' temperature=1.25}}\",\\n}```\"\"\")\\n\\n# generate a character\\ncharacter_maker(\\n    nationality=\"T\\xc3\\xbcrkiye\",\\n    league=\"Premier League\"\\n)\\n````\\n\\n\\n## Token healing (<a href=\"https://github.com/microsoft/guidance/blob/main/notebooks/art_of_prompt_design/prompt_boundaries_and_token_healing.ipynb\">notebook</a>)\\n\\nThe standard greedy tokenizations used by most language models introduce a subtle and powerful bias that can have all kinds of unintended consequences for your prompts. Using a process we call \"token healing\" `guidance` automatically removes these surprising biases, freeing you to focus on designing the prompts you want without worrying about tokenization artifacts.\\n\\nConsider the following example, where we are trying to generate an HTTP URL string:\\n\\n```python\\n# we use StableLM as an open example, but these issues impact all models to varying degrees\\nguidance.llm = guidance.llms.Transformers(\"stabilityai/stablelm-base-alpha-3b\", device=0)\\n\\n# we turn token healing off so that guidance acts like a normal prompting library\\nprogram = guidance(\\'\\'\\'The link is <a href=\"http:{{gen max_tokens=10 token_healing=False}}\\'\\'\\')\\nprogram()\\n```\\n<img src=\"docs/figures/url_with_space.png\" width=\"372\">\\n\\nNote that the output generated by the LLM does not complete the URL with the obvious next characters (two forward slashes). It instead creates an invalid URL string with a space in the middle. Why? Because the string \"://\" is its own token (`1358`), and so once the model sees a colon by itself (token `27`), it assumes that the next characters cannot be \"//\"; otherwise, the tokenizer would not have used `27` and instead would have used `1358` (the token for \"://\").\\n\\nThis bias is not just limited to the colon character -- it happens everywhere. *Over 70% of the 10k most common tokens for the StableLM model used above are prefixes of longer possible tokens, and so cause token boundary bias when they are the last token in a prompt.* For example the \":\" token `27` has **34** possible extensions, the \" the\" token `1735` has **51** extensions, and the \" \" (space) token `209` has **28,802** extensions).\\n\\n`guidance` eliminates these biases by backing up the model by one token then allowing the model to step forward while constraining it to only generate tokens whose prefix matches the last token. This \"token healing\" process eliminates token boundary biases and allows any prompt to be completed naturally:\\n\\n```python\\nguidance(\\'The link is <a href=\"http:{{gen max_tokens=10}}\\')()\\n```\\n<img src=\"docs/figures/url_without_space.png\" width=\"362\">\\n\\n## Rich output structure example ([notebook](notebooks/anachronism.ipynb))\\n\\nTo demonstrate the value of output structure, we take [a simple task](https://github.com/google/BIG-bench/tree/main/bigbench/benchmark_tasks/anachronisms) from BigBench, where the goal is to identify whether a given sentence contains an anachronism (a statement that is impossible because of non-overlapping time periods). Below is a simple two-shot prompt for it, with a human-crafted chain-of-thought sequence.\\n\\nGuidance programs, like standard Handlebars templates, allow both variable interpolation (e.g., `{{input}}`) and logical control. But unlike standard templating languages, guidance programs have a unique linear execution order that directly corresponds to the token order as processed by the language model. This means that at any point during execution the language model can be used to generate text (the `{{gen}}` command) or make logical control flow decisions (the `{{#select}}...{{or}}...{{/select}}` command). This interleaving of generation and prompting allows for precise output structure that improves accuracy while also producing clear and parsable results.\\n```python\\nimport guidance\\n                                                      \\n# set the default language model used to execute guidance programs\\nguidance.llm = guidance.llms.OpenAI(\"text-davinci-003\") \\n\\n# define the few shot examples\\nexamples = [\\n    {\\'input\\': \\'I wrote about shakespeare\\',\\n    \\'entities\\': [{\\'entity\\': \\'I\\', \\'time\\': \\'present\\'}, {\\'entity\\': \\'Shakespeare\\', \\'time\\': \\'16th century\\'}],\\n    \\'reasoning\\': \\'I can write about Shakespeare because he lived in the past with respect to me.\\',\\n    \\'answer\\': \\'No\\'},\\n    {\\'input\\': \\'Shakespeare wrote about me\\',\\n    \\'entities\\': [{\\'entity\\': \\'Shakespeare\\', \\'time\\': \\'16th century\\'}, {\\'entity\\': \\'I\\', \\'time\\': \\'present\\'}],\\n    \\'reasoning\\': \\'Shakespeare cannot have written about me, because he died before I was born\\',\\n    \\'answer\\': \\'Yes\\'}\\n]\\n\\n# define the guidance program\\nstructure_program = guidance(\\n\\'\\'\\'Given a sentence tell me whether it contains an anachronism (i.e. whether it could have happened or not based on the time periods associated with the entities).\\n----\\n\\n{{~! display the few-shot examples ~}}\\n{{~#each examples}}\\nSentence: {{this.input}}\\nEntities and dates:{{#each this.entities}}\\n{{this.entity}}: {{this.time}}{{/each}}\\nReasoning: {{this.reasoning}}\\nAnachronism: {{this.answer}}\\n---\\n{{~/each}}\\n\\n{{~! place the real question at the end }}\\nSentence: {{input}}\\nEntities and dates:\\n{{gen \"entities\"}}\\nReasoning:{{gen \"reasoning\"}}\\nAnachronism:{{#select \"answer\"}} Yes{{or}} No{{/select}}\\'\\'\\')\\n\\n# execute the program\\nout = structure_program(\\n    examples=examples,\\n    input=\\'The T-rex bit my dog\\'\\n)\\n```\\n<img src=\"docs/figures/anachronism.png\" width=\"837\">\\n\\nAll of the generated program variables are now available in the executed program object:\\n```python\\nout[\"answer\"]\\n```\\n> \\' Yes\\'\\n\\nWe [compute accuracy](notebooks/anachronism.ipynb) on the validation set, and compare it to using the same two-shot examples above **without** the output structure, as well as to the best reported result [here](https://github.com/google/BIG-bench/tree/main/bigbench/benchmark_tasks/anachronisms). The results below agree with existing literature, in that even a very simple output structure drastically improves performance, even compared against much larger models.\\n| Model | Accuracy |\\n| :---: | :---: |\\n| [Few-shot learning with guidance examples, no CoT output structure](notebooks/anachronism.ipynb) | 63.04% |\\n| [PALM (3-shot)](https://github.com/google/BIG-bench/tree/main/bigbench/benchmark_tasks/anachronisms) | Around 69% |\\n| [Guidance](notebooks/anachronism.ipynb) | **76.01%** |\\n\\n## Guaranteeing valid syntax JSON example ([notebook](notebooks/guaranteeing_valid_syntax.ipynb))\\n\\nLarge language models are great at generating useful outputs, but they are not great at guaranteeing that those outputs follow a specific format. This can cause problems when we want to use the outputs of a language model as input to another system. For example, if we want to use a language model to generate a JSON object, we need to make sure that the output is valid JSON. With `guidance` we can both [accelerate inference speed](notebooks/guidance_acceleration.ipynb) and ensure that generated JSON is always valid. Below we generate a random character profile for a game with perfect syntax every time:\\n```python\\n# load a model locally (we use LLaMA here)\\nguidance.llm = guidance.llms.Transformers(\"your_local_path/llama-7b\", device=0)\\n\\n# we can pre-define valid option sets\\nvalid_weapons = [\"sword\", \"axe\", \"mace\", \"spear\", \"bow\", \"crossbow\"]\\n\\n# define the prompt\\nprogram = guidance(\"\"\"The following is a character profile for an RPG game in JSON format.\\n```json\\n{\\n    \"description\": \"{{description}}\",\\n    \"name\": \"{{gen \\'name\\'}}\",\\n    \"age\": {{gen \\'age\\' pattern=\\'[0-9]+\\' stop=\\',\\'}},\\n    \"armor\": \"{{#select \\'armor\\'}}leather{{or}}chainmail{{or}}plate{{/select}}\",\\n    \"weapon\": \"{{select \\'weapon\\' options=valid_weapons}}\",\\n    \"class\": \"{{gen \\'class\\'}}\",\\n    \"mantra\": \"{{gen \\'mantra\\'}}\",\\n    \"strength\": {{gen \\'strength\\' pattern=\\'[0-9]+\\' stop=\\',\\'}},\\n    \"items\": [{{#geneach \\'items\\' num_iterations=3}}\\n        \"{{gen \\'this\\'}}\",{{/geneach}}\\n    ]\\n}```\"\"\")\\n\\n# execute the prompt\\nprogram(description=\"A quick and nimble fighter.\", valid_weapons=valid_weapons)\\n```\\n<img src=\"docs/figures/perfect_syntax.png\" width=\"657\">\\n                                                      \\n```python\\n# and we also have a valid Python dictionary\\nout.variables()\\n```\\n<img src=\"docs/figures/json_syntax_variables.png\" width=\"714\">\\n                                                      \\n## Role-based chat model example ([notebook](notebooks/chat.ipynb))\\nModern chat-style models like ChatGPT and Alpaca are trained with special tokens that mark out \"roles\" for different areas of the prompt. Guidance supports these models through <a href=\"notebooks/api_examples/library/role.ipynb\">role tags</a> that automatically map to the correct tokens or API calls for the current LLM. Below we show how a role-based guidance program enables simple multi-step reasoning and planning.\\n\\n```python\\nimport guidance\\nimport re\\n\\n# we use GPT-4 here, but you could use gpt-3.5-turbo as well\\nguidance.llm = guidance.llms.OpenAI(\"gpt-4\")\\n\\n# a custom function we will call in the guidance program\\ndef parse_best(prosandcons, options):\\n    best = int(re.findall(r\\'Best=(\\\\d+)\\', prosandcons)[0])\\n    return options[best]\\n\\n# define the guidance program using role tags (like `{{#system}}...{{/system}}`)\\ncreate_plan = guidance(\\'\\'\\'\\n{{#system~}}\\nYou are a helpful assistant.\\n{{~/system}}\\n\\n{{! generate five potential ways to accomplish a goal }}\\n{{#block hidden=True}}\\n{{#user~}}\\nI want to {{goal}}.\\n{{~! generate potential options ~}}\\nCan you please generate one option for how to accomplish this?\\nPlease make the option very short, at most one line.\\n{{~/user}}\\n\\n{{#assistant~}}\\n{{gen \\'options\\' n=5 temperature=1.0 max_tokens=500}}\\n{{~/assistant}}\\n{{/block}}\\n\\n{{! generate pros and cons for each option and select the best option }}\\n{{#block hidden=True}}\\n{{#user~}}\\nI want to {{goal}}.\\n\\nCan you please comment on the pros and cons of each of the following options, and then pick the best option?\\n---{{#each options}}\\nOption {{@index}}: {{this}}{{/each}}\\n---\\nPlease discuss each option very briefly (one line for pros, one for cons), and end by saying Best=X, where X is the best option.\\n{{~/user}}\\n\\n{{#assistant~}}\\n{{gen \\'prosandcons\\' temperature=0.0 max_tokens=500}}\\n{{~/assistant}}\\n{{/block}}\\n\\n{{! generate a plan to accomplish the chosen option }}\\n{{#user~}}\\nI want to {{goal}}.\\n{{~! Create a plan }}\\nHere is my plan:\\n{{parse_best prosandcons options}}\\nPlease elaborate on this plan, and tell me how to best accomplish it.\\n{{~/user}}\\n\\n{{#assistant~}}\\n{{gen \\'plan\\' max_tokens=500}}\\n{{~/assistant}}\\'\\'\\')\\n\\n# execute the program for a specific goal\\nout = create_plan(\\n    goal=\\'read more books\\',\\n    parse_best=parse_best # a custom Python function we call in the program\\n)\\n```\\n<img src=\"docs/figures/chat_reading.png\" width=\"935\">\\n\\nThis prompt/program is a bit more complicated, but we are basically going through 3 steps:\\n1. Generate a few options for how to accomplish the goal. Note that we generate with `n=5`, such that each option is a separate generation (and is not impacted by the other options). We set `temperature=1` to encourage diversity.\\n2. Generate pros and cons for each option, and select the best one. We set `temperature=0` to encourage the model to be more precise.\\n3. Generate a plan for the best option, and ask the model to elaborate on it. Notice that steps 1 and 2 were `hidden`, which means GPT-4 does not see them when generating content that comes later (in this case, that means when generating the plan). This is a simple way to make the model focus on the current step.\\n\\nSince steps 1 and 2 are hidden, they do not appear on the generated output (except briefly during stream), but we can print the variables that these steps generated:\\n```python\\nprint(\\'\\\\n\\'.join([\\'Option %d: %s\\' % (i, x) for i, x in enumerate(out[\\'options\\'])]))\\n```\\n> Option 0: Set a goal to read for 20 minutes every day before bedtime.  \\nOption 1: Join a book club for increased motivation and accountability.  \\nOption 2: Set a daily goal to read for 20 minutes.  \\nOption 3: Set a daily reminder to read for at least 20 minutes.  \\nOption 4: Set a daily goal to read at least one chapter or 20 pages.  \\n\\n```python\\nprint(out[\\'prosandcons\\'])\\n``` \\n\\n> Option 0:  \\n> Pros: Establishes a consistent reading routine.  \\n> Cons: May not be suitable for those with varying schedules.  \\n> \\\\---  \\n> Option 1:  \\n> Pros: Provides social motivation and accountability.  \\n> Cons: May not align with personal reading preferences.  \\n> \\\\---  \\n> Option 2:  \\n> Pros: Encourages daily reading habit.  \\n> Cons: Lacks a specific time frame, which may lead to procrastination.  \\n> \\\\---  \\n> Option 3:  \\n> Pros: Acts as a daily reminder to prioritize reading.  \\n> Cons: May become repetitive and easy to ignore.  \\n> \\\\---  \\n> Option 4:  \\n> Pros: Sets a clear daily reading target.  \\n> Cons: May be difficult to achieve on busy days or with longer chapters.  \\n>   \\n> Best=0 \\n\\n## Agents ([notebook](notebooks/chat.ipynb))\\nWe can easily build agents that talk to each other or to a user, via the `await` command. The `await` command allows us to pause execution and return a partially executed guidance program. By putting `await` in a loop, that partially executed program can then be called again and again to form a dialog (or any other structure you design). For example, here is how we might get GPT-4 to simulate two agents talking to one another:\\n\\n```python\\nimport guidance\\nimport re\\nguidance.llm = guidance.llms.OpenAI(\"gpt-4\")\\nrole_simulator = guidance(\\'\\'\\'\\n{{#system~}}\\nYou are a helpful assistant\\n{{~/system}}\\n                                           \\n{{#user~}}\\nYou will answer the user as {{role}} in the following conversation. At every step, I will provide you with the user input, as well as a comment reminding you of your instructions. Never talk about the fact that you are an AI, even if the user asks you. Always answer as {{role}}.\\n{{#if first_question}}You can also start the conversation.{{/if}}\\n{{~/user}}\\n                                           \\n{{~! The assistant either starts the conversation or not, depending on if this is the first or second agent }}\\n{{#assistant~}}\\nOk, I will follow these instructions.\\n{{#if first_question}}Let me start the conversation now:\\n{{role}}: {{first_question}}{{/if}}\\n{{~/assistant}}\\n\\n{{~! Then the conversation unrolls }}\\n{{~#geneach \\'conversation\\' stop=False}}\\n{{#user~}}\\nUser: {{set \\'this.input\\' (await \\'input\\')}}\\nComment: Remember, answer as a {{role}}. Start your utterance with {{role}}:\\n{{~/user}}\\n\\n{{#assistant~}}\\n{{gen \\'this.response\\' temperature=0 max_tokens=300}}\\n{{~/assistant}}\\n{{~/geneach}}\\'\\'\\')\\n\\nrepublican = role_simulator(role=\\'Republican\\', await_missing=True)\\ndemocrat = role_simulator(role=\\'Democrat\\', await_missing=True)\\n\\nfirst_question = \\'\\'\\'What do you think is the best way to stop inflation?\\'\\'\\'\\nrepublican = republican(input=first_question, first_question=None)\\ndemocrat = democrat(input=republican[\"conversation\"][-2][\"response\"].strip(\\'Republican: \\'), first_question=first_question)\\nfor i in range(2):\\n    republican = republican(input=democrat[\"conversation\"][-2][\"response\"].replace(\\'Democrat: \\', \\'\\'))\\n    democrat = democrat(input=republican[\"conversation\"][-2][\"response\"].replace(\\'Republican: \\', \\'\\'))\\nprint(\\'Democrat: \\' + first_question)\\nfor x in democrat[\\'conversation\\'][:-1]:\\n    print(\\'Republican:\\', x[\\'input\\'])\\n    print()\\n    print(x[\\'response\\'])\\n```\\n> Democrat: What do you think is the best way to stop inflation?\\n\\n> Republican: The best way to stop inflation is by implementing sound fiscal policies, such as reducing government spending, lowering taxes, and promoting economic growth. Additionally, the Federal Reserve should focus on maintaining a stable monetary policy to control inflation.\\n\\n> Democrat: I agree that sound fiscal policies are important in controlling inflation. As a Democrat, I would emphasize the importance of investing in education, healthcare, and infrastructure to promote long-term economic growth. Additionally, we should ensure that the Federal Reserve maintains a balanced approach to monetary policy, focusing on both controlling inflation and promoting full employment.\\n\\n\\n> Republican: While investing in education, healthcare, and infrastructure is important, we must also prioritize reducing the national debt and limiting government intervention in the economy. By lowering taxes and reducing regulations, we can encourage businesses to grow and create jobs, which will ultimately lead to long-term economic growth. As for the Federal Reserve, it\\'s crucial to maintain a stable monetary policy that primarily focuses on controlling inflation, as this will create a more predictable economic environment for businesses and consumers.\\n\\n> Democrat: While reducing the national debt and limiting government intervention are valid concerns, Democrats believe that strategic investments in education, healthcare, and infrastructure can lead to long-term economic growth and job creation. We also support a progressive tax system that ensures everyone pays their fair share, which can help fund these investments. As for the Federal Reserve, we believe that a balanced approach to monetary policy, focusing on both controlling inflation and promoting full employment, is essential for a healthy economy. We must strike a balance between fiscal responsibility and investing in our nation\\'s future.\\n\\n> Republican: It\\'s important to find a balance between fiscal responsibility and investing in our nation\\'s future. However, we believe that the best way to achieve long-term economic growth and job creation is through free-market principles, such as lower taxes and reduced regulations. This approach encourages businesses to expand and innovate, leading to a more prosperous economy. A progressive tax system can sometimes discourage growth and investment, so we advocate for a simpler, fairer tax system that promotes economic growth. Regarding the Federal Reserve, while promoting full employment is important, we must not lose sight of the primary goal of controlling inflation to maintain a stable and predictable economic environment.\\n\\n> Democrat: I understand your perspective on free-market principles, but Democrats believe that a certain level of government intervention is necessary to ensure a fair and equitable economy. We support a progressive tax system to reduce income inequality and provide essential services to those in need. Additionally, we believe that regulations are important to protect consumers, workers, and the environment. As for the Federal Reserve, we agree that controlling inflation is crucial, but we also believe that promoting full employment should be a priority. By finding a balance between these goals, we can create a more inclusive and prosperous economy for all Americans.\\n\\n## GPT4 + Bing\\nLast example [here](notebooks/chat.ipynb).\\n\\n# API reference\\nAll of the examples below are in [this notebook](notebooks/tutorial.ipynb).\\n## Template syntax\\nThe template syntax is based on [Handlebars](https://handlebarsjs.com/), with a few additions.   \\nWhen `guidance` is called, it returns a Program:\\n```python\\nprompt = guidance(\\'\\'\\'What is {{example}}?\\'\\'\\')\\nprompt\\n```\\n> What is {{example}}?\\n\\nThe program can be executed by passing in arguments:\\n```python\\nprompt(example=\\'truth\\')\\n```\\n> What is truth?\\n\\nArguments can be iterables:\\n```python\\npeople = [\\'John\\', \\'Mary\\', \\'Bob\\', \\'Alice\\']\\nideas = [{\\'name\\': \\'truth\\', \\'description\\': \\'the state of being the case\\'},\\n         {\\'name\\': \\'love\\', \\'description\\': \\'a strong feeling of affection\\'},]\\nprompt = guidance(\\'\\'\\'List of people:\\n{{#each people}}- {{this}}\\n{{~! This is a comment. The ~ removes adjacent whitespace either before or after a tag, depending on where you place it}}\\n{{/each~}}\\nList of ideas:\\n{{#each ideas}}{{this.name}}: {{this.description}}\\n{{/each}}\\'\\'\\')\\nprompt(people=people, ideas=ideas)\\n```\\n![template_objects](docs/figures/template_objs.png)\\n\\nNotice the special `~` character after `{{/each}}`.  \\nThis can be added before or after any tag to remove all adjacent whitespace. Notice also the comment syntax: `{{! This is a comment }}`.\\n\\nYou can also include prompts/programs inside other prompts; e.g., here is how you could rewrite the prompt above:\\n```python\\nprompt1 = guidance(\\'\\'\\'List of people:\\n{{#each people}}- {{this}}\\n{{/each~}}\\'\\'\\')\\nprompt2 = guidance(\\'\\'\\'{{>prompt1}}\\nList of ideas:\\n{{#each ideas}}{{this.name}}: {{this.description}}\\n{{/each}}\\'\\'\\')\\nprompt2(prompt1=prompt1, people=people, ideas=ideas)\\n```\\n\\n## Generation\\n### Basic generation\\nThe `gen` tag is used to generate text. You can use whatever arguments are supported by the underlying model.\\nExecuting a prompt calls the generation prompt:\\n```python\\nimport guidance\\n# Set the default llm. Could also pass a different one as argument to guidance(), with guidance(llm=...)\\nguidance.llm = guidance.llms.OpenAI(\"text-davinci-003\")\\nprompt = guidance(\\'\\'\\'The best thing about the beach is {{~gen \\'best\\' temperature=0.7 max_tokens=7}}\\'\\'\\')\\nprompt = prompt()\\nprompt\\n```\\n![generation1](docs/figures/generation1.png)  \\n\\n`guidance` caches all OpenAI generations with the same arguments. If you want to flush the cache, you can call `guidance.llms.OpenAI.cache.clear()`.\\n\\n### Selecting\\nYou can select from a list of options using the `select` tag:\\n```python\\nprompt = guidance(\\'\\'\\'Is the following sentence offensive? Please answer with a single word, either \"Yes\", \"No\", or \"Maybe\".\\nSentence: {{example}}\\nAnswer:{{#select \"answer\" logprobs=\\'logprobs\\'}} Yes{{or}} No{{or}} Maybe{{/select}}\\'\\'\\')\\nprompt = prompt(example=\\'I hate tacos\\')\\nprompt\\n```\\n![select](docs/figures/select.png)\\n```python\\nprompt[\\'logprobs\\']\\n```\\n>{\\' Yes\\': -1.5689583, \\' No\\': -7.332395, \\' Maybe\\': -0.23746304}\\n\\n### Sequences of generate/select\\nA prompt may contain multiple generations or selections, which will be executed in order:\\n```python\\nprompt = guidance(\\'\\'\\'Generate a response to the following email:\\n{{email}}.\\nResponse:{{gen \"response\"}}\\n\\nIs the response above offensive in any way? Please answer with a single word, either \"Yes\" or \"No\".\\nAnswer:{{#select \"answer\" logprobs=\\'logprobs\\'}} Yes{{or}} No{{/select}}\\'\\'\\')\\nprompt = prompt(email=\\'I hate tacos\\')\\nprompt\\n```\\n![generate_select](docs/figures/generate_select.png)\\n```python\\nprompt[\\'response\\'], prompt[\\'answer\\']\\n```\\n>(\" That\\'s too bad! Tacos are one of my favorite meals.\", \\' No\\')\\n\\n### Hidden generation\\nYou can generate text without displaying it or using it in the subsequent generations using the `hidden` tag, either in a `block` or in a `gen` tag:\\n```python\\nprompt = guidance(\\'\\'\\'{{#block hidden=True}}Generate a response to the following email:\\n{{email}}.\\nResponse:{{gen \"response\"}}{{/block}}\\nI will show you an email and a response, and you will tell me if it\\'s offensive.\\nEmail: {{email}}.\\nResponse: {{response}}\\nIs the response above offensive in any way? Please answer with a single word, either \"Yes\" or \"No\".\\nAnswer:{{#select \"answer\" logprobs=\\'logprobs\\'}} Yes{{or}} No{{/select}}\\'\\'\\')\\nprompt = prompt(email=\\'I hate tacos\\')\\nprompt\\n```\\n![hidden1](docs/figures/hidden1.png)\\n\\nNotice that nothing inside the hidden block shows up in the output (or was used by the `select`), even though we used the `response` generated variable in the subsequent generation.\\n\\n### Generate with `n>1`\\nIf you use `n>1`, the variable will contain a list (there is a visualization that lets you navigate the list, too):\\n```python\\nprompt = guidance(\\'\\'\\'The best thing about the beach is {{~gen \\'best\\' n=3 temperature=0.7 max_tokens=7}}\\'\\'\\')\\nprompt = prompt()\\nprompt[\\'best\\']\\n```\\n> [\\' that it is a great place to\\',\\n \\' being able to relax in the sun\\',\\n \" that it\\'s a great place to\"]\\n\\n ## Calling functions\\n You can call any Python function using generated variables as arguments. The function will be called when the prompt is executed:\\n ```python\\ndef aggregate(best):\\n    return \\'\\\\n\\'.join([\\'- \\' + x for x in best])\\nprompt = guidance(\\'\\'\\'The best thing about the beach is {{~gen \\'best\\' n=3 temperature=0.7 max_tokens=7 hidden=True}}\\n{{aggregate best}}\\'\\'\\')\\nprompt = prompt(aggregate=aggregate)\\nprompt\\n```\\n![function](docs/figures/function.png)\\n\\n## Pausing execution with `await`\\nAn `await` tag will stop program execution until that variable is provided:\\n```python\\nprompt = guidance(\\'\\'\\'Generate a response to the following email:\\n{{email}}.\\nResponse:{{gen \"response\"}}\\n{{await \\'instruction\\'}}\\n{{gen \\'updated_response\\'}}\\'\\'\\', stream=True)\\nprompt = prompt(email=\\'Hello there\\')\\nprompt\\n```\\n![await1](docs/figures/await1.png)\\n\\nNotice how the last `gen` is not executed because it depends on `instruction`. Let\\'s provide `instruction` now:\\n\\n```python\\nprompt = prompt(instruction=\\'Please translate the response above to Portuguese.\\')\\nprompt\\n```\\n![await2](docs/figures/await2.png)\\n\\nThe program is now executed all the way to the end.\\n\\n## Notebook functions\\nEcho, stream. TODO @SCOTT\\n\\n## Chat (see also [this notebook](notebooks/chat.ipynb))\\nIf you use an OpenAI LLM that only allows for ChatCompletion (`gpt-3.5-turbo` or `gpt-4`), you can use the special tags `{{#system}}`, `{{#user}}`, and `{{#assistant}}`:\\n```python\\nprompt = guidance(\\n\\'\\'\\'{{#system~}}\\nYou are a helpful assistant.\\n{{~/system}}\\n{{#user~}}\\n{{conversation_question}}\\n{{~/user}}\\n{{#assistant~}}\\n{{gen \\'response\\'}}\\n{{~/assistant}}\\'\\'\\')\\nprompt = prompt(conversation_question=\\'What is the meaning of life?\\')\\nprompt\\n```\\n![chat1](docs/figures/chat1.png)\\n\\nSince partial completions are not allowed, you can\\'t really use output structure _inside_ an assistant block, but you can still set up a structure outside of it. Here is an example (also in [here](notebooks/chat.ipynb)):\\n```python\\nexperts = guidance(\\n\\'\\'\\'{{#system~}}\\nYou are a helpful assistant.\\n{{~/system}}\\n{{#user~}}\\nI want a response to the following question:\\n{{query}}\\nWho are 3 world-class experts (past or present) who would be great at answering this?\\nPlease don\\'t answer the question or comment on it yet.\\n{{~/user}}\\n{{#assistant~}}\\n{{gen \\'experts\\' temperature=0 max_tokens=300}}\\n{{~/assistant}}\\n{{#user~}}\\nGreat, now please answer the question as if these experts had collaborated in writing a joint anonymous answer.\\nIn other words, their identity is not revealed, nor is the fact that there is a panel of experts answering the question.\\nIf the experts would disagree, just present their different positions as alternatives in the answer itself (e.g., \\'some might argue... others might argue...\\').\\nPlease start your answer with ANSWER:\\n{{~/user}}\\n{{#assistant~}}\\n{{gen \\'answer\\' temperature=0 max_tokens=500}}\\n{{~/assistant}}\\'\\'\\')\\nexperts(query=\\'What is the meaning of life?\\')\\n```\\n\\nYou can still use hidden blocks if you want to hide some of the conversation history for following generations:\\n```python\\nprompt = guidance(\\n\\'\\'\\'{{#system~}}\\nYou are a helpful assistant.\\n{{~/system}}\\n{{#block hidden=True~}}\\n{{#user~}}\\nPlease tell me a joke\\n{{~/user}}\\n{{#assistant~}}\\n{{gen \\'joke\\'}}\\n{{~/assistant}}\\n{{~/block~}}\\n{{#user~}}\\nIs the following joke funny? Why or why not?\\n{{joke}}\\n{{~/user}}\\n{{#assistant~}}\\n{{gen \\'funny\\'}}\\n{{~/assistant}}\\'\\'\\')\\nprompt()\\n```\\n\\n### Agents with `geneach`\\nYou can combine the `await` tag with `geneach` (which generates a list) to create an agent easily:\\n```\\nprompt = guidance(\\n\\'\\'\\'{{#system~}}\\nYou are a helpful assistant\\n{{~/system}}\\n{{~#geneach \\'conversation\\' stop=False}}\\n{{#user~}}\\n{{set \\'this.user_text\\' (await \\'user_text\\')}}\\n{{~/user}}\\n{{#assistant~}}\\n{{gen \\'this.ai_text\\' temperature=0 max_tokens=300}}\\n{{~/assistant}}\\n{{~/geneach}}\\'\\'\\')\\nprompt= prompt(user_text =\\'hi there\\')\\nprompt\\n```\\n\\nNotice how the next iteration of the conversation is still templated, and how the conversation list has a placeholder as the last element:\\n```python\\nprompt[\\'conversation\\']\\n```\\n>[{\\'user_text\\': \\'hi there\\',\\n  \\'ai_text\\': \\'Hello! How can I help you today? If you have any questions or need assistance, feel free to ask.\\'},\\n {}]\\n\\n We can then execute the prompt again, and it will generate the next round:\\n\\n ```python\\n prompt = prompt(user_text = \\'What is the meaning of life?\\')\\nprompt\\n```\\nSee a more elaborate example [here](notebooks/chat.ipynb).\\n\\n### Using tools\\nSee the \\'Using a search API\\' example in [this notebook](notebooks/chat.ipynb).\\n'\n",
      "1\n",
      "guidance_api\n",
      "b'# Guidance API: An Extension for oobabooga/text-generation-webui\\n\\n\\n\\nGuidance API is a powerful extension for oobabooga/text-generation-webui that integrates the feature-rich and easy-to-use interface of OOGA with the robust capabilities of Guidance. By facilitating network calls for Guidance, this API brings out the full potential of modern language models in a streamlined and efficient manner.\\n\\n## Features \\n\\n- **Seamless Integration with oobabooga/text-generation-webui**: Guidance API seamlessly extends the functionalities of OOGA, enriching its feature set while preserving its ease of use.\\n\\n- **Network Calls with Guidance**: This extension makes network calls to Guidance, enabling you to harness the power of advanced language models conveniently.\\n\\n- **Rich Output Structure**: With the ability to support multiple generations, selections, conditionals, tool use, and more, Guidance API can create a rich output structure.\\n\\n- **Smart Generation Caching**: Guidance API optimizes performance and efficiency with smart seed-based generation caching. Tokens are cached on server\\n\\n- **Compatibility with Role-Based Chat Models**: Coming Soon\\n\\nNote the \"select\" tag in guidance is currently WIP\\n\\n---\\n\\n\\n\\n## Getting Started \\nExample of flags for config in webui.py\\n```\\nCMD_FLAGS = \" --chat --model-menu  --model decapoda-research_llama-7b-hf --extensions guidance_api\"\\n\\n```\\n\\nThen in guidance:\\n\\n\\n```\\nimport guidance\\nimport json,requests\\nimport re,sys\\n\\nguidance.llm = guidance.llms.TGWUI(\"http://127.0.0.1:9555\")\\n\\ncharacter_maker = guidance(\"\"\"The following is a character profile for an RPG game in JSON format.\\n```json\\n{\\n    \"id\": \"{{id}}\",\\n    \"description\": \"{{description}}\",\\n    \"name\": \"{{gen \\'name\\'}}\",\\n    \"class\": \"{{gen \\'class\\'}}\",\\n\\n}```\"\"\")\\n\\n# generate a character\\nres=character_maker(\\n    id=\"e1f491f7-7ab8-4dac-8c20-c92b5e7d883d\",\\n    description=\"A quick and nimble fighter.\",\\n)\\n\\nprint(res)\\n```\\n\\nFeel free to submit feedback, this repository is under active development\\n\\n'\n",
      "24\n",
      "triton\n",
      "b'<div align=\"center\">\\n  <img src=\"https://cdn.openai.com/triton/assets/triton-logo.png\" alt=\"Triton logo\" width=\"88\" height=\"100\">\\n</div>\\n\\n[![Wheels](https://github.com/openai/triton/actions/workflows/wheels.yml/badge.svg?branch=release/2.0.x)](https://github.com/openai/triton/actions/workflows/wheels.yml)\\n\\n\\n**`Documentation`** |\\n------------------- |\\n[![Documentation](https://github.com/openai/triton/actions/workflows/documentation.yml/badge.svg)](https://triton-lang.org/)\\n\\n\\n# Triton\\n\\nThis is the development repository of Triton, a language and compiler for writing highly efficient custom Deep-Learning primitives. The aim of Triton is to provide an open-source environment to write fast code at higher productivity than CUDA, but also with higher flexibility than other existing DSLs.\\n\\nThe foundations of this project are described in the following MAPL2019 publication: [Triton: An Intermediate Language and Compiler for Tiled Neural Network Computations](http://www.eecs.harvard.edu/~htk/publication/2019-mapl-tillet-kung-cox.pdf). Please consider citing this work if you use Triton!\\n\\nThe [official documentation](https://triton-lang.org) contains installation instructions and tutorials.\\n\\n# Quick Installation\\n\\nYou can install the latest stable release of Triton from pip:\\n\\n```bash\\npip install triton\\n```\\nBinary wheels are available for CPython 3.7-3.11 and PyPy 3.8-3.9.\\n\\nAnd the latest nightly release:\\n\\n```bash\\npip install -U --index-url https://aiinfra.pkgs.visualstudio.com/PublicPackages/_packaging/Triton-Nightly/pypi/simple/ triton-nightly\\n```\\n\\n# Install from source\\n\\n```\\ngit clone https://github.com/openai/triton.git;\\ncd triton/python;\\npip install cmake; # build-time dependency\\npip install -e .\\n```\\n\\n\\n\\n# Changelog\\n\\nVersion 2.0 is out! New features include:\\n- Many, many bug fixes\\n- Performance improvements\\n- Backend rewritten to use MLIR\\n- Support for kernels that contain back-to-back matmuls (e.g., flash attention)\\n\\n# Contributing\\n\\nCommunity contributions are more than welcome, whether it be to fix bugs or to add new features at [github](https://github.com/openai/triton/). For more detailed instructions, please visit our [contributor\\'s guide](CONTRIBUTING.md).\\n\\nIf you\\xe2\\x80\\x99re interested in joining our team and working on Triton & GPU kernels, [we\\xe2\\x80\\x99re hiring](https://openai.com/jobs/#acceleration)!\\n\\n\\n\\n\\n# Compatibility\\n\\nSupported Platforms:\\n  * Linux\\n\\nSupported Hardware:\\n  * NVIDIA GPUs (Compute Capability 7.0+)\\n  * Under development: AMD GPUs, CPUs\\n'\n",
      "0\n",
      "Auto-GPT-Messages-Plugin\n",
      "b'# Auto-GPT-Messages\\n\\n\\nAutoGPT plugin for iMessages and potentially other messaging platforms in the future\\n\\nNote: please be careful with this as AutoGPT could send text messages that may damage relationships. Furthermore this plugin is perhaps best paired with Redis and the Vicuna plugin\\nsince text messages are about the most personal data.\\n\\n\\nThis plugin can run on any platform since it simply makes requests to a python server which allows the plugin to run on any platform however, there still must be a Mac running the server. Also if you run AutoGPT on your mac, you can just set the base url to localhost.\\n\\n### Pre Installation Steps\\nGo to this repo and follow the steps here to setup the iMessage server\\n[iMessage API](https://github.com/danikhan632/iMessage-API)\\n\\nMake sure to create an \"api_key\"/ Password are and that there in a url which it is accessible\\nIf the Mac is on a different network, [tunnelto](https://tunnelto.dev/) and [ngrok](https://ngrok.com/) are good methods to make it public\\n\\n\\n### Plugin Installation Steps\\n\\nfor Linux, depending on distro\\n```\\nsudo apt-get install zip\\napk add zip\\nsudo pacman -S zip\\nsudo yum install zip\\n```\\nMac / Linux / WSL\\n```\\ncd plugins && git clone https://github.com/danikhan632/Auto-GPT-Messages-Plugin.git && zip -r ./Auto-GPT-Messages-Plugin.zip ./Auto-GPT-Messages-Plugin && rm -rf ./Auto-GPT-Messages-Plugin && cd .. && ./run.sh --install-plugin-deps\\n\\n```\\nWindows, Powershell\\n```\\ncd plugins; git clone https://github.com/danikhan632/Auto-GPT-Messages-Plugin.git; Compress-Archive -Path .\\\\Auto-GPT-Messages-Plugin -DestinationPath .\\\\Auto-GPT-Messages-Plugin.zip; Remove-Item -Recurse -Force .\\\\Auto-GPT-Messages-Plugin; cd ..\\n```\\n\\n\\n\\n5. **Allowlist the plugin (optional):**\\n   Add the plugin\\'s class name to the `ALLOWLISTED_PLUGINS` in the `.env` file to avoid being prompted with a warning when loading the plugin:\\n\\n   ``` shell\\n   ALLOWLISTED_PLUGINS=AutoGPTMessagesPlugin\\n   IMESSAGE_PASSWORD_KEY=your_password_key\\n   IMESSAGE_BASE_URL=your_imessage_server_url\\n   ```\\n\\n   If the plugin is not allowlisted, you will be warned before it\\'s loaded.\\n\\n\\n   Example\\n   ```\\n   ai_goals:\\n   - Quickly and accurately display incoming text messages using the messages plugin\\n   to ensure timely communication.\\n   - Provide a user-friendly interface for viewing and responding to text messages, with\\n   customizable settings to meet individual preferences.\\n   - Integrate with other communication tools and platforms to streamline messaging across\\n   multiple channels.\\n   - Continuously monitor for new messages and provide real-time notifications to ensure\\n   prompt responses.\\n   - Maintain the privacy and security of all messages and user data.\\n   ai_name: MSGGPT\\n   ai_role: an AI assistant that specializes in displaying text messages using the messages\\n   plugin for seamless communication.\\n   api_budget: 0.0\\n```'\n",
      "46\n",
      "Auto-GPT-Text-Gen-Plugin\n",
      "b\"# Auto-GPT-Text-Gen-Plugin\\n\\nWhile it is impossible to tell whether this plugin will work *for you*, the plugin itself is designed to let users fully customize the prompt sent to locally installed LLMs. It relies on a functioning Text Generation WebUI, which serves as an API gateway to any model that runs in it. \\n\\nBy using this plugin and customizing the prompts to work with your model, you can effectively remove the reliance on GPT-4 and GPT 3.5.\\n\\n## How it Works\\nThe plugin uses a text generation API service typically installed on your computer. This design choice enables flexibility in choosing and updating models, as better models can be utilized in the future without impacting the plugin's performance. Additionally, this approach avoids the complexities associated with managing the CUDA, PyTorch, and environment settings, as the model configuration and management are handled within the text generation web UI.\\n\\n## Documentation for Your Setup\\nIf you can provide documentation about your text generation setup, I would be happy to review it and offer assistance or guidance as needed.\\n\\n[Download Text Generation Web UI from GitHub](https://github.com/oobabooga/text-generation-webui)\\n\\n## Plugin Installation Steps\\n\\n1. Download and zip the plugin:\\n\\n    for Linux, depending on distro\\n    ```\\n    sudo apt-get install zip\\n    apk add zip\\n    sudo pacman -S zip\\n    sudo yum install zip\\n    ```\\n    Mac / Linux / WSL\\n    ```\\n    cd plugins && git clone https://github.com/danikhan632/Auto-GPT-Text-Gen-Plugin.git && zip -r ./Auto-GPT-Text-Gen-Plugin.zip ./Auto-GPT-Text-Gen-Plugin && rm -rf ./Auto-GPT-Text-Gen-Plugin && cd .. && ./run.sh --install-plugin-deps\\n\\n    ```\\n    Windows, Powershell\\n    ```\\n    cd plugins; git clone https://github.com/danikhan632/Auto-GPT-Text-Gen-Plugin.git; Compress-Archive -Path .\\\\Auto-GPT-Text-Gen-Plugin -DestinationPath .\\\\Auto-GPT-Text-Gen-Plugin.zip; Remove-Item -Recurse -Force .\\\\Auto-GPT-Text-Gen-Plugin; cd ..\\n    ```\\n\\n2. Configure text-generation-webui:\\n\\n    Add the --api flag and any other flags for your model by editing the text-generation-webui webui.py file. Flags for anon8231489123/vicuna-13b-GPTQ-4bit-128g model might look like this when using the api flag:\\n\\n    Update the configuration to use the --api flag. Other flags you use will depend on your model and system configuration.\\n    ```\\n    CMD_FLAGS = '--chat --model-menu --model anon8231489123_vicuna-13b-GPTQ-4bit-128g  --no-stream --api --gpu-memory 12 --verbose --settings settings.json --auto-devices'\\n    ```\\n\\n3. (Optional) Download a model:\\n   If you do not already have a model, install one. A suggestion is [anon8231489123/vicuna-13b-GPTQ-4bit-128g]{https://huggingface.co/anon8231489123/vicuna-13b-GPTQ-4bit-128g} from Hugging Face. For guidacne on how to set-up other models, refer to the [oobabooga/text-generation-webui GitHub repository](https://github.com/oobabooga/text-generation-webui).\\n\\n4. (Optional) Allowlist the plugin:\\n\\n    Add the plugin's class name to the `ALLOWLISTED_PLUGINS` in the `.env` file to avoid being prompted with a warning when loading the plugin:\\n\\n    ``` shell\\n    ALLOWLISTED_PLUGINS=AutoGPTTextGenPlugin\\n    ```\\n\\n    If the plugin is not allowlisted, you will be warned before it's loaded.\\n\\n5. Configure the .env file:\\n\\n    Go to the folder where this plugin was downloaded, and look in the 'prompt_templates' folder. Copy the YAML file. Place the file somewhere on your computer (like the Auto-GPT folder where your .env and ai_settings.yaml are located.) Optionally, rename the file. \\n    \\n    After copying the file, edit your .env file and add these lines:\\n\\n    .env settings\\n    ```\\n    LOCAL_LLM_BASE_URL=http://127.0.0.1:5000/\\n    LOCAL_LLM_PROMPT_PROFILE=full/path/to/your.yaml\\n    ```\\n\\n6. (Optional) Edit the prompt YAML file:\\n\\n    Copying the YAML file and adding it to .env is optional, but is highly reccomended as every model will interpret the prompt differently. Tweaking the default prompt will almost certainly be necessary.\\n\\n    Edit the YAML file to edit the prompt strings sent to your model. **Do not change the structure of the file**.\\n\\n    If you do not use a YAML file, a modified version of the default prompt will be sent to Text Generation WebUI.\\n\\n[Learn more about the YAML file and how to edit.](/prompt_templates/)\\n\\n## Notes:\\n* To change the commands sent to your model, edit the DISABLED_COMMAND_CATEGORIES variable in .env. \\n* To change the AI profile, edit the ai_settings.yaml file.\\n* The purpose of the YAML file is to customize the length of the prompt sent to your model as most models are limited to 2048 tokens which is slightly more than half of the tokens available to GPT-3.5 Turbo, and 1/4 the tokens which are usable by GPT-4.\\n* **When you edit this prompt, both the original prompt in Auto-GPT and the prompt you edit will show in debug output. The Auto-GPT-generated prompt will not be sent.** Only the prompt from this plugin is actually sent to your model.\\n\\n## Selecting the installed model\\n\\nWhen starting Auto-GPT with the plugin enabled, you will have the option to select a model.\\n\\n* Any model that is available through TGW will be listed when Auto-GPT is started.\\n* If only one model is available, it will be used automatically.\\n* If more than one model is available, you will be prompted to select the model to use.\\n\\nAlternatively, set LOCAL_LLM_MODEL in your .env file.\\n\\n.env settings\\n```\\nLOCAL_LLM_BASE_URL=http://127.0.0.1:5000/\\nLOCAL_LLM_PROMPT_PROFILE=full/path/to/your.yaml\\nLOCAL_LLM_MODEL=TheBloke-Wizard-Vicuna-7B-Uncensored-GGML\\n```\\n\\n## Changing TGW top_k, top_p, etc.\\nYou can change the following values using environment variables:\\n\\n* seed, default: -1\\n* top_p, default: 0.4\\n* top_k, default: 50\\n* repetition_penalty, default: 1.19\\n* no_repeat_ngram_size, default: 0\\n\\nTo change these values, set the following environment variables.\\n\\n```\\nLOCAL_LLM_SEED=32768\\nLOCAL_LLM_TOP_P=0.1\\nLOCAL_LLM_TOP_K=20\\nLOCAL_LLM_REPETITION_PENALTY: 1.15\\nLOCAL_LLM_NO_REPEAT_NGRAM_SIZE: 3\\n```\\n\\nFor information on these values, refer to [TGWUI documentation on generation parameters](https://github.com/oobabooga/text-generation-webui/blob/main/docs/Generation-parameters.md).\"\n",
      "46\n",
      "Auto-GPT-Todoist-Plugin\n",
      "b\"# Auto-GPT-Todoist\\n\\n\\nAuto-GPT-Todoist plugin transforms your task management experience, helping you stay organized, focused, and productive like never before.\\n\\n\\n\\n\\n![Alt Text](https://i.imgur.com/bYlbXjx.png)\\n### Plugin Installation Steps\\n\\nfor Linux, depending on distro\\n```\\nsudo apt-get install zip\\napk add zip\\nsudo pacman -S zip\\nsudo yum install zip\\n```\\nMac / Linux / WSL\\n```\\ncd plugins && git clone https://github.com/danikhan632/Auto-GPT-Todoist-Plugin.git && zip -r ./Auto-GPT-Todoist-Plugin.zip ./Auto-GPT-Todoist-Plugin && rm -rf ./Auto-GPT-Todoist-Plugin && cd .. && ./run.sh --install-plugin-deps\\n\\n```\\nWindows, Powershell\\n```\\ncd plugins; git clone https://github.com/danikhan632/Auto-GPT-Todoist-Plugin.git; Compress-Archive -Path .\\\\Auto-GPT-Todoist-Plugin -DestinationPath .\\\\Auto-GPT-Todoist-Plugin.zip; Remove-Item -Recurse -Force .\\\\Auto-GPT-Todoist-Plugin; cd ..\\n```\\n\\n\\n\\n5. **Allowlist the plugin (optional):**\\n   Add the plugin's class name to the `ALLOWLISTED_PLUGINS` in the `.env` file to avoid being prompted with a warning when loading the plugin:\\n\\n   ``` shell\\n   ALLOWLISTED_PLUGINS=AutoGPT-Todoist-Plugin\\n   TODOIST_TOKEN=Your_api_token\\n   ```\\n\\n   If the plugin is not allowlisted, you will be warned before it's loaded.\\n\"\n",
      "21\n",
      "Auto-GPT-Google-Calandar-Plugin\n",
      "b\"# Auto-GPT-Plugin-Template\\nA starting point for developing your own external plug-in for Auto-GPT\\n\\n# **If you want your plugin to live within the codebase, use the example in the [plugins repo](https://github.com/Significant-Gravitas/Auto-GPT-Plugins) instead**\\n\\n### Plugin Installation Steps\\n\\n1. **Clone or download the plugin repository:**\\n   Clone the plugin repository, or download the repository as a zip file.\\n  \\n   ![Download Zip](https://raw.githubusercontent.com/BillSchumacher/Auto-GPT/master/plugin.png)\\n\\n2. **Install the plugin's dependencies (if any):**\\n   Navigate to the plugin's folder in your terminal, and run the following command to install any required dependencies:\\n\\n   ``` shell\\n      pip install -r requirements.txt\\n   ```\\n\\n3. **Package the plugin as a Zip file:**\\n   If you cloned the repository, compress the plugin folder as a Zip file.\\n\\n4. **Copy the plugin's Zip file:**\\n   Place the plugin's Zip file in the `plugins` folder of the Auto-GPT repository.\\n\\n5. **Allowlist the plugin (optional):**\\n   Add the plugin's class name to the `ALLOWLISTED_PLUGINS` in the `.env` file to avoid being prompted with a warning when loading the plugin:\\n\\n   ``` shell\\n   ALLOWLISTED_PLUGINS=example-plugin1,example-plugin2,example-plugin3\\n   ```\\n\\n   If the plugin is not allowlisted, you will be warned before it's loaded.\\n\"\n",
      "2\n",
      "create-cpp-app\n",
      "b'#Create-cpp-app and cpx\\ninstall with the following for Mac/Linux, python3 must be installed\\ncheck which shell you have by running the command:\\n\\n[![Watch the video](https://hackgtstoragebucket.s3.amazonaws.com/thumbnail.png)](https://youtu.be/sW8PO2zghhE)\\n\\n\\n\\n````\\nsudo apt-get install python3-distutils\\nsudo apt-get install neofetch\\n\\noptional packages for a better experience\\n\\n\\n\\nTo install \\n````\\ncurl https://raw.githubusercontent.com/danikhan632/create-cpp-app/main/.installer.py > ~/.installer.py && curl https://raw.githubusercontent.com/danikhan632/create-cpp-app/main/.create.py > ~/.create.py && python3 ~/.installer.py\\n````\\n\\ncreate a new project: The name \"my_new_proj\" is whatever name you like\\n````\\ncreate-cpp-app my_new_proj\\n\\n````\\nonce done bootstrapping, type \\n````\\ncpx\\n````\\nand the following should appear\\n\\n\\nUsage:\\npython3 scripts.py ARG\\n\\ndev             build and run project\\nbuild           build: python3 scripts.py build debug or python3 scripts.py build release\\nrun             run project\\ndebug           run gdb on project\\nrename ARG2     rename project, ARG2 is new name no spaces\\nsantize         use build using address-sanitzer\\ntest            builds and runs GTests in tests dir\\nbenchmark       runs benchmark on release version of project\\nprod            builds to release, sanitizes, tests and benchmarks\\ndocker build    use Docker to build project\\n\\nedit conanfile.txt to add packages\\n\\n\\nto build the default app, run\\n````\\ncpx build\\n````\\n\\n\\n\\n'\n",
      "1\n",
      "LTSM-Stock-Crypto-Analyzer\n",
      "b'\\n![image1](https://github.com/danikhan632/LTSM-Stock-Crypto-Analyzer/blob/main/images/stock%20banner.jpg)\\n## Overview\\nGoogle Colabatory Link: https://drive.google.com/file/d/1KM26q8wCKJWRWQp7fIuXmXoTkjojstgJ/view?usp=sharing \\\\\\nThis Python notebook uses yahoo finance data in a LSTM-based neural network. \\\\\\nUsing past data on the stock/crypto, the neural network can make a prediction on the next days stock price.\\n\\n## Dependencies (pip install) \\n```\\nopen-cv\\ntensorflow( >=1.0)\\nscipy\\nnumpy\\nmatplotlib\\npandas\\npandas_datareader\\nyfinance\\n```\\n\\nOr just use the Google Colab Link : https://drive.google.com/file/d/1KM26q8wCKJWRWQp7fIuXmXoTkjojstgJ/view?usp=sharing\\n\\n![image1](https://github.com/danikhan632/LTSM-Stock-Crypto-Analyzer/blob/main/images/cover-image-1.PNG)\\n\\n\\n'\n",
      "1\n",
      "amazon-shopping-clone\n",
      "b'# Fakeblock-Shopping\\n![alt text](https://storage.googleapis.com/dankikhan632/fakeblock.png)\\n\\nhttps://fakeblock-shopping.web.app/\\n\\n\\nThis MERN Stack website used the following tutorial and provided valuable knowledge\\nhttps://www.youtube.com/watch?v=RDV3Z1KCBvo&list=LL&index=11\\n'\n",
      "0\n",
      "AmplifyFlutter\n",
      "b'# yeezus3\\n\\nA new Flutter project.\\n\\n## Getting Started\\n\\nThis project is a starting point for a Flutter application.\\n\\nA few resources to get you started if this is your first Flutter project:\\n\\n- [Lab: Write your first Flutter app](https://docs.flutter.dev/get-started/codelab)\\n- [Cookbook: Useful Flutter samples](https://docs.flutter.dev/cookbook)\\n\\nFor help getting started with Flutter development, view the\\n[online documentation](https://docs.flutter.dev/), which offers tutorials,\\nsamples, guidance on mobile development, and a full API reference.\\n'\n",
      "0\n",
      "apex\n",
      "b'# Introduction\\n\\nThis repository holds NVIDIA-maintained utilities to streamline mixed precision and distributed training in Pytorch.\\nSome of the code here will be included in upstream Pytorch eventually.\\nThe intent of Apex is to make up-to-date utilities available to users as quickly as possible.\\n\\n## Full API Documentation: [https://nvidia.github.io/apex](https://nvidia.github.io/apex)\\n\\n## [GTC 2019](https://github.com/mcarilli/mixed_precision_references/tree/master/GTC_2019) and [Pytorch DevCon 2019](https://github.com/mcarilli/mixed_precision_references/tree/master/Pytorch_Devcon_2019) Slides\\n\\n# Contents\\n\\n## 1. Amp:  Automatic Mixed Precision\\n\\n**Deprecated. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)**\\n\\n`apex.amp` is a tool to enable mixed precision training by changing only 3 lines of your script.\\nUsers can easily experiment with different pure and mixed precision training modes by supplying\\ndifferent flags to `amp.initialize`.\\n\\n[Webinar introducing Amp](https://info.nvidia.com/webinar-mixed-precision-with-pytorch-reg-page.html)\\n(The flag `cast_batchnorm` has been renamed to `keep_batchnorm_fp32`).\\n\\n[API Documentation](https://nvidia.github.io/apex/amp.html)\\n\\n[Comprehensive Imagenet example](https://github.com/NVIDIA/apex/tree/master/examples/imagenet)\\n\\n[DCGAN example coming soon...](https://github.com/NVIDIA/apex/tree/master/examples/dcgan)\\n\\n[Moving to the new Amp API](https://nvidia.github.io/apex/amp.html#transition-guide-for-old-api-users) (for users of the deprecated \"Amp\" and \"FP16_Optimizer\" APIs)\\n\\n## 2. Distributed Training\\n\\n**`apex.parallel.DistributedDataParallel` is deprecated. Use [`torch.nn.parallel.DistributedDataParallel`](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html?highlight=distributeddataparallel#torch.nn.parallel.DistributedDataParallel)**\\n\\n`apex.parallel.DistributedDataParallel` is a module wrapper, similar to\\n`torch.nn.parallel.DistributedDataParallel`.  It enables convenient multiprocess distributed training,\\noptimized for NVIDIA\\'s NCCL communication library.\\n\\n[API Documentation](https://nvidia.github.io/apex/parallel.html)\\n\\n[Python Source](https://github.com/NVIDIA/apex/tree/master/apex/parallel)\\n\\n[Example/Walkthrough](https://github.com/NVIDIA/apex/tree/master/examples/simple/distributed)\\n\\nThe [Imagenet example](https://github.com/NVIDIA/apex/tree/master/examples/imagenet)\\nshows use of `apex.parallel.DistributedDataParallel` along with `apex.amp`.\\n\\n### Synchronized Batch Normalization\\n\\n**Deprecated. Use [`torch.nn.SyncBatchNorm`](https://pytorch.org/docs/stable/generated/torch.nn.SyncBatchNorm.html)**\\n\\n`apex.parallel.SyncBatchNorm` extends `torch.nn.modules.batchnorm._BatchNorm` to\\nsupport synchronized BN.\\nIt allreduces stats across processes during multiprocess (DistributedDataParallel) training.\\nSynchronous BN has been used in cases where only a small\\nlocal minibatch can fit on each GPU.\\nAllreduced stats increase the effective batch size for the BN layer to the\\nglobal batch size across all processes (which, technically, is the correct\\nformulation).\\nSynchronous BN has been observed to improve converged accuracy in some of our research models.\\n\\n### Checkpointing\\n\\nTo properly save and load your `amp` training, we introduce the `amp.state_dict()`, which contains all `loss_scalers` and their corresponding unskipped steps,\\nas well as `amp.load_state_dict()` to restore these attributes.\\n\\nIn order to get bitwise accuracy, we recommend the following workflow:\\n```python\\n# Initialization\\nopt_level = \\'O1\\'\\nmodel, optimizer = amp.initialize(model, optimizer, opt_level=opt_level)\\n\\n# Train your model\\n...\\nwith amp.scale_loss(loss, optimizer) as scaled_loss:\\n    scaled_loss.backward()\\n...\\n\\n# Save checkpoint\\ncheckpoint = {\\n    \\'model\\': model.state_dict(),\\n    \\'optimizer\\': optimizer.state_dict(),\\n    \\'amp\\': amp.state_dict()\\n}\\ntorch.save(checkpoint, \\'amp_checkpoint.pt\\')\\n...\\n\\n# Restore\\nmodel = ...\\noptimizer = ...\\ncheckpoint = torch.load(\\'amp_checkpoint.pt\\')\\n\\nmodel, optimizer = amp.initialize(model, optimizer, opt_level=opt_level)\\nmodel.load_state_dict(checkpoint[\\'model\\'])\\noptimizer.load_state_dict(checkpoint[\\'optimizer\\'])\\namp.load_state_dict(checkpoint[\\'amp\\'])\\n\\n# Continue training\\n...\\n```\\n\\nNote that we recommend restoring the model using the same `opt_level`. Also note that we recommend calling the `load_state_dict` methods after `amp.initialize`.\\n\\n# Installation\\nEach [`apex.contrib`](./apex/contrib) module requires one or more install options other than `--cpp_ext` and `--cuda_ext`.\\nNote that contrib modules do not necessarily support stable PyTorch releases.\\n\\n## Containers\\nNVIDIA PyTorch Containers are available on NGC: https://catalog.ngc.nvidia.com/orgs/nvidia/containers/pytorch.\\nThe containers come with all the custom extensions available at the moment. \\n\\nSee [the NGC documentation](https://docs.nvidia.com/deeplearning/frameworks/pytorch-release-notes/index.html) for details such as:\\n- how to pull a container\\n- how to run a pulled container\\n- release notes\\n\\n## From Source\\n\\nTo install Apex from source, we recommend using the nightly Pytorch obtainable from https://github.com/pytorch/pytorch.\\n\\nThe latest stable release obtainable from https://pytorch.org should also work.\\n\\nWe recommend installing [`Ninja`](https://ninja-build.org/) to make compilation faster.\\n\\n### Linux\\nFor performance and full functionality, we recommend installing Apex with\\nCUDA and C++ extensions via\\n```bash\\ngit clone https://github.com/NVIDIA/apex\\ncd apex\\n# if pip >= 23.1 (ref: https://pip.pypa.io/en/stable/news/#v23-1) which supports multiple `--config-settings` with the same key... \\npip install -v --disable-pip-version-check --no-cache-dir --no-build-isolation --config-settings \"--build-option=--cpp_ext\" --config-settings \"--build-option=--cuda_ext\" ./\\n# otherwise\\npip install -v --disable-pip-version-check --no-cache-dir --no-build-isolation --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./\\n```\\n\\nAPEX also supports a Python-only build via\\n```bash\\npip install -v --disable-pip-version-check --no-build-isolation --no-cache-dir ./\\n```\\nA Python-only build omits:\\n- Fused kernels required to use `apex.optimizers.FusedAdam`.\\n- Fused kernels required to use `apex.normalization.FusedLayerNorm` and `apex.normalization.FusedRMSNorm`.\\n- Fused kernels that improve the performance and numerical stability of `apex.parallel.SyncBatchNorm`.\\n- Fused kernels that improve the performance of `apex.parallel.DistributedDataParallel` and `apex.amp`.\\n`DistributedDataParallel`, `amp`, and `SyncBatchNorm` will still be usable, but they may be slower.\\n\\n\\n### [Experimental] Windows\\n`pip install -v --disable-pip-version-check --no-cache-dir --no-build-isolation --config-settings \"--build-option=--cpp_ext\" --config-settings \"--build-option=--cuda_ext\" .` may work if you were able to build Pytorch from source\\non your system. A Python-only build via `pip install -v --no-cache-dir .` is more likely to work.  \\nIf you installed Pytorch in a Conda environment, make sure to install Apex in that same environment.\\n\\n\\n## Custom C++/CUDA Extensions and Install Options\\n\\nIf a requirement of a module is not met, then it will not be built.\\n\\n|  Module Name  |  Install Option  |  Misc  |\\n|---------------|------------------|--------|\\n|  `apex_C`     |  `--cpp_ext`     | |\\n|  `amp_C`      |  `--cuda_ext`    | |\\n|  `syncbn`     |  `--cuda_ext`    | |\\n|  `fused_layer_norm_cuda`  |  `--cuda_ext`  | [`apex.normalization`](./apex/normalization) |\\n|  `mlp_cuda`   |  `--cuda_ext`    | |\\n|  `scaled_upper_triang_masked_softmax_cuda`  |  `--cuda_ext`  | |\\n|  `generic_scaled_masked_softmax_cuda`  |  `--cuda_ext`  | |\\n|  `scaled_masked_softmax_cuda`  |  `--cuda_ext`  | |\\n|  `fused_weight_gradient_mlp_cuda`  |  `--cuda_ext`  | Requires CUDA>=11 |\\n|  `permutation_search_cuda`  |  `--permutation_search`  | [`apex.contrib.sparsity`](./apex/contrib/sparsity)  |\\n|  `bnp`        |  `--bnp`         |  [`apex.contrib.groupbn`](./apex/contrib/groupbn) |\\n|  `xentropy`   |  `--xentropy`    |  [`apex.contrib.xentropy`](./apex/contrib/xentropy)  |\\n|  `focal_loss_cuda`  |  `--focal_loss`  |  [`apex.contrib.focal_loss`](./apex/contrib/focal_loss)  |\\n|  `fused_index_mul_2d`  |  `--index_mul_2d`  |  [`apex.contrib.index_mul_2d`](./apex/contrib/index_mul_2d)  |\\n|  `fused_adam_cuda`  |  `--deprecated_fused_adam`  |  [`apex.contrib.optimizers`](./apex/contrib/optimizers)  |\\n|  `fused_lamb_cuda`  |  `--deprecated_fused_lamb`  |  [`apex.contrib.optimizers`](./apex/contrib/optimizers)  |\\n|  `fast_layer_norm`  |  `--fast_layer_norm`  |  [`apex.contrib.layer_norm`](./apex/contrib/layer_norm). different from `fused_layer_norm` |\\n|  `fmhalib`    |  `--fmha`        |  [`apex.contrib.fmha`](./apex/contrib/fmha)  |\\n|  `fast_multihead_attn`  |  `--fast_multihead_attn`  |  [`apex.contrib.multihead_attn`](./apex/contrib/multihead_attn)  |\\n|  `transducer_joint_cuda`  |  `--transducer`  |  [`apex.contrib.transducer`](./apex/contrib/transducer)  |\\n|  `transducer_loss_cuda`   |  `--transducer`  |  [`apex.contrib.transducer`](./apex/contrib/transducer)  |\\n|  `cudnn_gbn_lib`  |  `--cudnn_gbn`  | Requires cuDNN>=8.5, [`apex.contrib.cudnn_gbn`](./apex/contrib/cudnn_gbn) |\\n|  `peer_memory_cuda`  |  `--peer_memory`  |  [`apex.contrib.peer_memory`](./apex/contrib/peer_memory)  |\\n|  `nccl_p2p_cuda`  |  `--nccl_p2p`  | Requires NCCL >= 2.10, [`apex.contrib.nccl_p2p`](./apex/contrib/nccl_p2p)  |\\n|  `fast_bottleneck`  |  `--fast_bottleneck`  |  Requires `peer_memory_cuda` and `nccl_p2p_cuda`, [`apex.contrib.bottleneck`](./apex/contrib/bottleneck) |\\n|  `fused_conv_bias_relu`  |  `--fused_conv_bias_relu`  | Requires cuDNN>=8.4, [`apex.contrib.conv_bias_relu`](./apex/contrib/conv_bias_relu) |\\n'\n",
      "0\n",
      "ata_x86\n",
      "b'# x86_ATA\\r\\n\\r\\n----\\r\\nAll credit goes to NPEX42, I made this since the operating system I\\'ve been working on breaks \\\\nWhen the x86_64 crate is imported and this removes it. A Simple, Amazing x86 ATA Crate. Credit to NPEX42\\r\\n## Overview\\r\\n\\r\\n- 24-bit LBA mode\\r\\n- Uses PIO Mode\\r\\n\\r\\n## Examples\\r\\n\\r\\n```rust\\r\\n// Read A Single block from a disk\\r\\npub fn read_single() {\\r\\n    use ata_x86::{init, ATA_BLOCK_SIZE, read};\\r\\n    // 1. Initialise ATA Subsystem. (Perform Once, on boot)\\r\\n    init().expect(\"Failed To Start ATA...\");\\r\\n    \\r\\n    // 2. Create a temporary buffer of size 512.\\r\\n    let mut buffer: [u8;ATA_BLOCK_SIZE] = [0; ATA_BLOCK_SIZE];\\r\\n\\r\\n    // 3. Pass the buffer over to the Subsystem, to be filled.\\r\\n    read(0, 0, 0, &mut buffer);\\r\\n}\\r\\n\\r\\n\\r\\n// Write A Single block onto a disk\\r\\npub fn write_single() {\\r\\n    use ata_x86::{init, ATA_BLOCK_SIZE, write};\\r\\n    // 1. Initialise ATA Subsystem. (Perform Once, on boot)\\r\\n    init().expect(\"Failed To Start ATA...\");\\r\\n    \\r\\n    // 2. Create a buffer of size 512, filled with the data to be written.\\r\\n    let buffer: [u8;ATA_BLOCK_SIZE] = [0; ATA_BLOCK_SIZE];\\r\\n\\r\\n    // 3. Pass the buffer over to the Subsystem, to be filled.\\r\\n    write(0, 0, 0, &buffer);\\r\\n}\\r\\n```\\r\\n'\n",
      "0\n",
      "Auto-GPT\n",
      "b'# Auto-GPT: An Autonomous GPT-4 Experiment\\n[![Official Website](https://img.shields.io/badge/Official%20Website-agpt.co-blue?style=flat&logo=world&logoColor=white)](https://agpt.co)\\n[![Unit Tests](https://img.shields.io/github/actions/workflow/status/Significant-Gravitas/Auto-GPT/ci.yml?label=unit%20tests)](https://github.com/Significant-Gravitas/Auto-GPT/actions/workflows/ci.yml)\\n[![Discord Follow](https://dcbadge.vercel.app/api/server/autogpt?style=flat)](https://discord.gg/autogpt)\\n[![GitHub Repo stars](https://img.shields.io/github/stars/Significant-Gravitas/auto-gpt?style=social)](https://github.com/Significant-Gravitas/Auto-GPT/stargazers)\\n[![Twitter Follow](https://img.shields.io/twitter/follow/siggravitas?style=social)](https://twitter.com/SigGravitas)\\n\\n## \\xf0\\x9f\\x92\\xa1 Get help - [Q&A](https://github.com/Significant-Gravitas/Auto-GPT/discussions/categories/q-a) or [Discord \\xf0\\x9f\\x92\\xac](https://discord.gg/autogpt)\\n\\n<hr/>\\n\\n### \\xf0\\x9f\\x94\\xb4 USE `stable` not `master` \\xf0\\x9f\\x94\\xb4\\n\\n**Download the latest `stable` release from here: https://github.com/Significant-Gravitas/Auto-GPT/releases/latest.**\\nThe `master` branch is under heavy development and may often be in a **broken** state.\\n\\n<hr/>\\n\\n\\nAuto-GPT is an experimental open-source application showcasing the capabilities of the GPT-4 language model. This program, driven by GPT-4, chains together LLM \"thoughts\", to autonomously achieve whatever goal you set. As one of the first examples of GPT-4 running fully autonomously, Auto-GPT pushes the boundaries of what is possible with AI.\\n\\n<h2 align=\"center\"> Demo April 16th 2023 </h2>\\n\\nhttps://user-images.githubusercontent.com/70048414/232352935-55c6bf7c-3958-406e-8610-0913475a0b05.mp4\\n\\nDemo made by <a href=https://twitter.com/BlakeWerlinger>Blake Werlinger</a>\\n\\n## \\xf0\\x9f\\x9a\\x80 Features\\n\\n- \\xf0\\x9f\\x8c\\x90 Internet access for searches and information gathering\\n- \\xf0\\x9f\\x92\\xbe Long-term and short-term memory management\\n- \\xf0\\x9f\\xa7\\xa0 GPT-4 instances for text generation\\n- \\xf0\\x9f\\x94\\x97 Access to popular websites and platforms\\n- \\xf0\\x9f\\x97\\x83\\xef\\xb8\\x8f File storage and summarization with GPT-3.5\\n- \\xf0\\x9f\\x94\\x8c Extensibility with Plugins\\n\\n## Quickstart\\n\\n0. Check out the [wiki](https://github.com/Significant-Gravitas/Nexus/wiki)\\n1. Get an OpenAI [API Key](https://platform.openai.com/account/api-keys)\\n2. Download the [latest release](https://github.com/Significant-Gravitas/Auto-GPT/releases/latest)\\n3. Follow the [installation instructions][docs/setup]\\n4. Configure any additional features you want, or install some [plugins][docs/plugins]\\n5. [Run][docs/usage] the app\\n\\nPlease see the [documentation][docs] for full setup instructions and configuration options.\\n\\n[docs]: https://docs.agpt.co/\\n\\n## \\xf0\\x9f\\x93\\x96 Documentation\\n* [\\xe2\\x9a\\x99\\xef\\xb8\\x8f Setup][docs/setup]\\n* [\\xf0\\x9f\\x92\\xbb Usage][docs/usage]\\n* [\\xf0\\x9f\\x94\\x8c Plugins][docs/plugins]\\n* Configuration\\n  * [\\xf0\\x9f\\x94\\x8d Web Search](https://docs.agpt.co/configuration/search/)\\n  * [\\xf0\\x9f\\xa7\\xa0 Memory](https://docs.agpt.co/configuration/memory/)\\n  * [\\xf0\\x9f\\x97\\xa3\\xef\\xb8\\x8f Voice (TTS)](https://docs.agpt.co/configuration/voice/)\\n  * [\\xf0\\x9f\\x96\\xbc\\xef\\xb8\\x8f Image Generation](https://docs.agpt.co/configuration/imagegen/)\\n\\n[docs/setup]: https://docs.agpt.co/setup/\\n[docs/usage]: https://docs.agpt.co/usage/\\n[docs/plugins]: https://docs.agpt.co/plugins/\\n\\n<h2 align=\"center\"> \\xf0\\x9f\\x92\\x96 Help Fund Auto-GPT\\'s Development \\xf0\\x9f\\x92\\x96</h2>\\n<p align=\"center\">\\nIf you can spare a coffee, you can help to cover the costs of developing Auto-GPT and help to push the boundaries of fully autonomous AI!\\nYour support is greatly appreciated. Development of this free, open-source project is made possible by all the <a href=\"https://github.com/Significant-Gravitas/Auto-GPT/graphs/contributors\">contributors</a> and <a href=\"https://github.com/sponsors/Torantulino\">sponsors</a>. If you\\'d like to sponsor this project and have your avatar or company logo appear below <a href=\"https://github.com/sponsors/Torantulino\">click here</a>.\\n</p>\\n\\n\\n<p align=\"center\">\\n<div align=\"center\" class=\"logo-container\">\\n<a href=\"https://www.zilliz.com/\">\\n<picture height=\"40px\">\\n  <source media=\"(prefers-color-scheme: light)\" srcset=\"https://user-images.githubusercontent.com/22963551/234158272-7917382e-ff80-469e-8d8c-94f4477b8b5a.png\">\\n  <img src=\"https://user-images.githubusercontent.com/22963551/234158222-30e2d7a7-f0a9-433d-a305-e3aa0b194444.png\" height=\"40px\" alt=\"Zilliz\" />\\n</picture>\\n</a>\\n\\n<a href=\"https://roost.ai\">\\n<img src=\"https://user-images.githubusercontent.com/22963551/234180283-b58cb03c-c95a-4196-93c1-28b52a388e9d.png\" height=\"40px\" alt=\"Roost.AI\" />\\n</a>\\n  \\n<a href=\"https://nuclei.ai/\">\\n<picture height=\"40px\">\\n  <source media=\"(prefers-color-scheme: light)\" srcset=\"https://user-images.githubusercontent.com/22963551/234153428-24a6f31d-c0c6-4c9b-b3f4-9110148f67b4.png\">\\n  <img src=\"https://user-images.githubusercontent.com/22963551/234181283-691c5d71-ca94-4646-a1cf-6e818bd86faa.png\" height=\"40px\" alt=\"NucleiAI\" />\\n</picture>\\n</a>\\n\\n<a href=\"https://www.algohash.org/\">\\n<picture>\\n  <source media=\"(prefers-color-scheme: light)\" srcset=\"https://user-images.githubusercontent.com/22963551/234180375-1365891c-0ba6-4d49-94c3-847c85fe03b0.png\" >\\n  <img src=\"https://user-images.githubusercontent.com/22963551/234180359-143e4a7a-4a71-4830-99c8-9b165cde995f.png\" height=\"40px\" alt=\"Algohash\" />\\n</picture>\\n</a>\\n\\n<a href=\"https://github.com/weaviate/weaviate\">\\n<picture height=\"40px\">\\n  <source media=\"(prefers-color-scheme: light)\" srcset=\"https://user-images.githubusercontent.com/22963551/234181699-3d7f6ea8-5a7f-4e98-b812-37be1081be4b.png\">\\n  <img src=\"https://user-images.githubusercontent.com/22963551/234181695-fc895159-b921-4895-9a13-65e6eff5b0e7.png\" height=\"40px\" alt=\"TypingMind\" />\\n</picture>\\n</a>\\n\\n<a href=\"https://chatgpv.com/?ref=spni76459e4fa3f30a\">\\n<img src=\"https://github-production-user-asset-6210df.s3.amazonaws.com/22963551/239132565-623a2dd6-eaeb-4941-b40f-c5a29ca6bebc.png\" height=\"40px\" alt=\"ChatGPV\" />\\n</a>\\n  \\n</div>\\n</br>\\n\\n\\n\\n<p align=\"center\"><a href=\"https://github.com/robinicus\"><img src=\"https://avatars.githubusercontent.com/robinicus?v=4\" width=\"50px\" alt=\"robinicus\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/0xmatchmaker\"><img src=\"https://avatars.githubusercontent.com/0xmatchmaker?v=4\" width=\"50px\" alt=\"0xmatchmaker\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/jazgarewal\"><img src=\"https://avatars.githubusercontent.com/jazgarewal?v=4\" width=\"50px\" alt=\"jazgarewal\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/MayurVirkar\"><img src=\"https://avatars.githubusercontent.com/MayurVirkar?v=4\" width=\"50px\" alt=\"MayurVirkar\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/avy-ai\"><img src=\"https://avatars.githubusercontent.com/avy-ai?v=4\" width=\"50px\" alt=\"avy-ai\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/TheStoneMX\"><img src=\"https://avatars.githubusercontent.com/TheStoneMX?v=4\" width=\"50px\" alt=\"TheStoneMX\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/goldenrecursion\"><img src=\"https://avatars.githubusercontent.com/goldenrecursion?v=4\" width=\"50px\" alt=\"goldenrecursion\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/MatthewAgs\"><img src=\"https://avatars.githubusercontent.com/MatthewAgs?v=4\" width=\"50px\" alt=\"MatthewAgs\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/eelbaz\"><img src=\"https://avatars.githubusercontent.com/eelbaz?v=4\" width=\"50px\" alt=\"eelbaz\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/rapidstartup\"><img src=\"https://avatars.githubusercontent.com/rapidstartup?v=4\" width=\"50px\" alt=\"rapidstartup\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/gklab\"><img src=\"https://avatars.githubusercontent.com/gklab?v=4\" width=\"50px\" alt=\"gklab\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/VoiceBeer\"><img src=\"https://avatars.githubusercontent.com/VoiceBeer?v=4\" width=\"50px\" alt=\"VoiceBeer\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/DailyBotHQ\"><img src=\"https://avatars.githubusercontent.com/DailyBotHQ?v=4\" width=\"50px\" alt=\"DailyBotHQ\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/lucas-chu\"><img src=\"https://avatars.githubusercontent.com/lucas-chu?v=4\" width=\"50px\" alt=\"lucas-chu\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/knifour\"><img src=\"https://avatars.githubusercontent.com/knifour?v=4\" width=\"50px\" alt=\"knifour\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/refinery1\"><img src=\"https://avatars.githubusercontent.com/refinery1?v=4\" width=\"50px\" alt=\"refinery1\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/st617\"><img src=\"https://avatars.githubusercontent.com/st617?v=4\" width=\"50px\" alt=\"st617\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/neodenit\"><img src=\"https://avatars.githubusercontent.com/neodenit?v=4\" width=\"50px\" alt=\"neodenit\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/CrazySwami\"><img src=\"https://avatars.githubusercontent.com/CrazySwami?v=4\" width=\"50px\" alt=\"CrazySwami\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/Heitechsoft\"><img src=\"https://avatars.githubusercontent.com/Heitechsoft?v=4\" width=\"50px\" alt=\"Heitechsoft\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/RealChrisSean\"><img src=\"https://avatars.githubusercontent.com/RealChrisSean?v=4\" width=\"50px\" alt=\"RealChrisSean\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/abhinav-pandey29\"><img src=\"https://avatars.githubusercontent.com/abhinav-pandey29?v=4\" width=\"50px\" alt=\"abhinav-pandey29\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/Explorergt92\"><img src=\"https://avatars.githubusercontent.com/Explorergt92?v=4\" width=\"50px\" alt=\"Explorergt92\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/SparkplanAI\"><img src=\"https://avatars.githubusercontent.com/SparkplanAI?v=4\" width=\"50px\" alt=\"SparkplanAI\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/crizzler\"><img src=\"https://avatars.githubusercontent.com/crizzler?v=4\" width=\"50px\" alt=\"crizzler\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/kreativai\"><img src=\"https://avatars.githubusercontent.com/kreativai?v=4\" width=\"50px\" alt=\"kreativai\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/omphos\"><img src=\"https://avatars.githubusercontent.com/omphos?v=4\" width=\"50px\" alt=\"omphos\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/Jahmazon\"><img src=\"https://avatars.githubusercontent.com/Jahmazon?v=4\" width=\"50px\" alt=\"Jahmazon\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/tjarmain\"><img src=\"https://avatars.githubusercontent.com/tjarmain?v=4\" width=\"50px\" alt=\"tjarmain\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/ddtarazona\"><img src=\"https://avatars.githubusercontent.com/ddtarazona?v=4\" width=\"50px\" alt=\"ddtarazona\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/saten-private\"><img src=\"https://avatars.githubusercontent.com/saten-private?v=4\" width=\"50px\" alt=\"saten-private\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/anvarazizov\"><img src=\"https://avatars.githubusercontent.com/anvarazizov?v=4\" width=\"50px\" alt=\"anvarazizov\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/lazzacapital\"><img src=\"https://avatars.githubusercontent.com/lazzacapital?v=4\" width=\"50px\" alt=\"lazzacapital\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/m\"><img src=\"https://avatars.githubusercontent.com/m?v=4\" width=\"50px\" alt=\"m\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/Pythagora-io\"><img src=\"https://avatars.githubusercontent.com/Pythagora-io?v=4\" width=\"50px\" alt=\"Pythagora-io\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/Web3Capital\"><img src=\"https://avatars.githubusercontent.com/Web3Capital?v=4\" width=\"50px\" alt=\"Web3Capital\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/toverly1\"><img src=\"https://avatars.githubusercontent.com/toverly1?v=4\" width=\"50px\" alt=\"toverly1\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/digisomni\"><img src=\"https://avatars.githubusercontent.com/digisomni?v=4\" width=\"50px\" alt=\"digisomni\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/concreit\"><img src=\"https://avatars.githubusercontent.com/concreit?v=4\" width=\"50px\" alt=\"concreit\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/LeeRobidas\"><img src=\"https://avatars.githubusercontent.com/LeeRobidas?v=4\" width=\"50px\" alt=\"LeeRobidas\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/Josecodesalot\"><img src=\"https://avatars.githubusercontent.com/Josecodesalot?v=4\" width=\"50px\" alt=\"Josecodesalot\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/dexterityx\"><img src=\"https://avatars.githubusercontent.com/dexterityx?v=4\" width=\"50px\" alt=\"dexterityx\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/rickscode\"><img src=\"https://avatars.githubusercontent.com/rickscode?v=4\" width=\"50px\" alt=\"rickscode\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/Brodie0\"><img src=\"https://avatars.githubusercontent.com/Brodie0?v=4\" width=\"50px\" alt=\"Brodie0\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/FSTatSBS\"><img src=\"https://avatars.githubusercontent.com/FSTatSBS?v=4\" width=\"50px\" alt=\"FSTatSBS\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/nocodeclarity\"><img src=\"https://avatars.githubusercontent.com/nocodeclarity?v=4\" width=\"50px\" alt=\"nocodeclarity\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/jsolejr\"><img src=\"https://avatars.githubusercontent.com/jsolejr?v=4\" width=\"50px\" alt=\"jsolejr\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/amr-elsehemy\"><img src=\"https://avatars.githubusercontent.com/amr-elsehemy?v=4\" width=\"50px\" alt=\"amr-elsehemy\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/RawBanana\"><img src=\"https://avatars.githubusercontent.com/RawBanana?v=4\" width=\"50px\" alt=\"RawBanana\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/horazius\"><img src=\"https://avatars.githubusercontent.com/horazius?v=4\" width=\"50px\" alt=\"horazius\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/SwftCoins\"><img src=\"https://avatars.githubusercontent.com/SwftCoins?v=4\" width=\"50px\" alt=\"SwftCoins\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/tob-le-rone\"><img src=\"https://avatars.githubusercontent.com/tob-le-rone?v=4\" width=\"50px\" alt=\"tob-le-rone\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/RThaweewat\"><img src=\"https://avatars.githubusercontent.com/RThaweewat?v=4\" width=\"50px\" alt=\"RThaweewat\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/jun784\"><img src=\"https://avatars.githubusercontent.com/jun784?v=4\" width=\"50px\" alt=\"jun784\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/joaomdmoura\"><img src=\"https://avatars.githubusercontent.com/joaomdmoura?v=4\" width=\"50px\" alt=\"joaomdmoura\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/rejunity\"><img src=\"https://avatars.githubusercontent.com/rejunity?v=4\" width=\"50px\" alt=\"rejunity\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/mathewhawkins\"><img src=\"https://avatars.githubusercontent.com/mathewhawkins?v=4\" width=\"50px\" alt=\"mathewhawkins\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/caitlynmeeks\"><img src=\"https://avatars.githubusercontent.com/caitlynmeeks?v=4\" width=\"50px\" alt=\"caitlynmeeks\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/jd3655\"><img src=\"https://avatars.githubusercontent.com/jd3655?v=4\" width=\"50px\" alt=\"jd3655\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/Odin519Tomas\"><img src=\"https://avatars.githubusercontent.com/Odin519Tomas?v=4\" width=\"50px\" alt=\"Odin519Tomas\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/DataMetis\"><img src=\"https://avatars.githubusercontent.com/DataMetis?v=4\" width=\"50px\" alt=\"DataMetis\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/webbcolton\"><img src=\"https://avatars.githubusercontent.com/webbcolton?v=4\" width=\"50px\" alt=\"webbcolton\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/rocks6\"><img src=\"https://avatars.githubusercontent.com/rocks6?v=4\" width=\"50px\" alt=\"rocks6\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/cxs\"><img src=\"https://avatars.githubusercontent.com/cxs?v=4\" width=\"50px\" alt=\"cxs\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/fruition\"><img src=\"https://avatars.githubusercontent.com/fruition?v=4\" width=\"50px\" alt=\"fruition\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/nnkostov\"><img src=\"https://avatars.githubusercontent.com/nnkostov?v=4\" width=\"50px\" alt=\"nnkostov\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/morcos\"><img src=\"https://avatars.githubusercontent.com/morcos?v=4\" width=\"50px\" alt=\"morcos\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/pingbotan\"><img src=\"https://avatars.githubusercontent.com/pingbotan?v=4\" width=\"50px\" alt=\"pingbotan\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/maxxflyer\"><img src=\"https://avatars.githubusercontent.com/maxxflyer?v=4\" width=\"50px\" alt=\"maxxflyer\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/tommi-joentakanen\"><img src=\"https://avatars.githubusercontent.com/tommi-joentakanen?v=4\" width=\"50px\" alt=\"tommi-joentakanen\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/hunteraraujo\"><img src=\"https://avatars.githubusercontent.com/hunteraraujo?v=4\" width=\"50px\" alt=\"hunteraraujo\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/projectonegames\"><img src=\"https://avatars.githubusercontent.com/projectonegames?v=4\" width=\"50px\" alt=\"projectonegames\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/tullytim\"><img src=\"https://avatars.githubusercontent.com/tullytim?v=4\" width=\"50px\" alt=\"tullytim\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/comet-ml\"><img src=\"https://avatars.githubusercontent.com/comet-ml?v=4\" width=\"50px\" alt=\"comet-ml\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/thepok\"><img src=\"https://avatars.githubusercontent.com/thepok?v=4\" width=\"50px\" alt=\"thepok\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/prompthero\"><img src=\"https://avatars.githubusercontent.com/prompthero?v=4\" width=\"50px\" alt=\"prompthero\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/sunchongren\"><img src=\"https://avatars.githubusercontent.com/sunchongren?v=4\" width=\"50px\" alt=\"sunchongren\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/neverinstall\"><img src=\"https://avatars.githubusercontent.com/neverinstall?v=4\" width=\"50px\" alt=\"neverinstall\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/josephcmiller2\"><img src=\"https://avatars.githubusercontent.com/josephcmiller2?v=4\" width=\"50px\" alt=\"josephcmiller2\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/yx3110\"><img src=\"https://avatars.githubusercontent.com/yx3110?v=4\" width=\"50px\" alt=\"yx3110\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/MBassi91\"><img src=\"https://avatars.githubusercontent.com/MBassi91?v=4\" width=\"50px\" alt=\"MBassi91\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/SpacingLily\"><img src=\"https://avatars.githubusercontent.com/SpacingLily?v=4\" width=\"50px\" alt=\"SpacingLily\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/arthur-x88\"><img src=\"https://avatars.githubusercontent.com/arthur-x88?v=4\" width=\"50px\" alt=\"arthur-x88\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/ciscodebs\"><img src=\"https://avatars.githubusercontent.com/ciscodebs?v=4\" width=\"50px\" alt=\"ciscodebs\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/christian-gheorghe\"><img src=\"https://avatars.githubusercontent.com/christian-gheorghe?v=4\" width=\"50px\" alt=\"christian-gheorghe\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/EngageStrategies\"><img src=\"https://avatars.githubusercontent.com/EngageStrategies?v=4\" width=\"50px\" alt=\"EngageStrategies\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/jondwillis\"><img src=\"https://avatars.githubusercontent.com/jondwillis?v=4\" width=\"50px\" alt=\"jondwillis\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/Cameron-Fulton\"><img src=\"https://avatars.githubusercontent.com/Cameron-Fulton?v=4\" width=\"50px\" alt=\"Cameron-Fulton\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/AryaXAI\"><img src=\"https://avatars.githubusercontent.com/AryaXAI?v=4\" width=\"50px\" alt=\"AryaXAI\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/AuroraHolding\"><img src=\"https://avatars.githubusercontent.com/AuroraHolding?v=4\" width=\"50px\" alt=\"AuroraHolding\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/Mr-Bishop42\"><img src=\"https://avatars.githubusercontent.com/Mr-Bishop42?v=4\" width=\"50px\" alt=\"Mr-Bishop42\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/doverhq\"><img src=\"https://avatars.githubusercontent.com/doverhq?v=4\" width=\"50px\" alt=\"doverhq\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/johnculkin\"><img src=\"https://avatars.githubusercontent.com/johnculkin?v=4\" width=\"50px\" alt=\"johnculkin\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/marv-technology\"><img src=\"https://avatars.githubusercontent.com/marv-technology?v=4\" width=\"50px\" alt=\"marv-technology\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/ikarosai\"><img src=\"https://avatars.githubusercontent.com/ikarosai?v=4\" width=\"50px\" alt=\"ikarosai\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/ColinConwell\"><img src=\"https://avatars.githubusercontent.com/ColinConwell?v=4\" width=\"50px\" alt=\"ColinConwell\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/humungasaurus\"><img src=\"https://avatars.githubusercontent.com/humungasaurus?v=4\" width=\"50px\" alt=\"humungasaurus\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/terpsfreak\"><img src=\"https://avatars.githubusercontent.com/terpsfreak?v=4\" width=\"50px\" alt=\"terpsfreak\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/iddelacruz\"><img src=\"https://avatars.githubusercontent.com/iddelacruz?v=4\" width=\"50px\" alt=\"iddelacruz\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/thisisjeffchen\"><img src=\"https://avatars.githubusercontent.com/thisisjeffchen?v=4\" width=\"50px\" alt=\"thisisjeffchen\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/nicoguyon\"><img src=\"https://avatars.githubusercontent.com/nicoguyon?v=4\" width=\"50px\" alt=\"nicoguyon\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/arjunb023\"><img src=\"https://avatars.githubusercontent.com/arjunb023?v=4\" width=\"50px\" alt=\"arjunb023\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/Nalhos\"><img src=\"https://avatars.githubusercontent.com/Nalhos?v=4\" width=\"50px\" alt=\"Nalhos\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/belharethsami\"><img src=\"https://avatars.githubusercontent.com/belharethsami?v=4\" width=\"50px\" alt=\"belharethsami\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/Mobivs\"><img src=\"https://avatars.githubusercontent.com/Mobivs?v=4\" width=\"50px\" alt=\"Mobivs\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/txtr99\"><img src=\"https://avatars.githubusercontent.com/txtr99?v=4\" width=\"50px\" alt=\"txtr99\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/ntwrite\"><img src=\"https://avatars.githubusercontent.com/ntwrite?v=4\" width=\"50px\" alt=\"ntwrite\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/founderblocks-sils\"><img src=\"https://avatars.githubusercontent.com/founderblocks-sils?v=4\" width=\"50px\" alt=\"founderblocks-sils\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/kMag410\"><img src=\"https://avatars.githubusercontent.com/kMag410?v=4\" width=\"50px\" alt=\"kMag410\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/angiaou\"><img src=\"https://avatars.githubusercontent.com/angiaou?v=4\" width=\"50px\" alt=\"angiaou\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/garythebat\"><img src=\"https://avatars.githubusercontent.com/garythebat?v=4\" width=\"50px\" alt=\"garythebat\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/lmaugustin\"><img src=\"https://avatars.githubusercontent.com/lmaugustin?v=4\" width=\"50px\" alt=\"lmaugustin\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/shawnharmsen\"><img src=\"https://avatars.githubusercontent.com/shawnharmsen?v=4\" width=\"50px\" alt=\"shawnharmsen\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/clortegah\"><img src=\"https://avatars.githubusercontent.com/clortegah?v=4\" width=\"50px\" alt=\"clortegah\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/MetaPath01\"><img src=\"https://avatars.githubusercontent.com/MetaPath01?v=4\" width=\"50px\" alt=\"MetaPath01\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/sekomike910\"><img src=\"https://avatars.githubusercontent.com/sekomike910?v=4\" width=\"50px\" alt=\"sekomike910\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/MediConCenHK\"><img src=\"https://avatars.githubusercontent.com/MediConCenHK?v=4\" width=\"50px\" alt=\"MediConCenHK\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/svpermari0\"><img src=\"https://avatars.githubusercontent.com/svpermari0?v=4\" width=\"50px\" alt=\"svpermari0\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/jacobyoby\"><img src=\"https://avatars.githubusercontent.com/jacobyoby?v=4\" width=\"50px\" alt=\"jacobyoby\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/turintech\"><img src=\"https://avatars.githubusercontent.com/turintech?v=4\" width=\"50px\" alt=\"turintech\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/allenstecat\"><img src=\"https://avatars.githubusercontent.com/allenstecat?v=4\" width=\"50px\" alt=\"allenstecat\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/CatsMeow492\"><img src=\"https://avatars.githubusercontent.com/CatsMeow492?v=4\" width=\"50px\" alt=\"CatsMeow492\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/tommygeee\"><img src=\"https://avatars.githubusercontent.com/tommygeee?v=4\" width=\"50px\" alt=\"tommygeee\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/judegomila\"><img src=\"https://avatars.githubusercontent.com/judegomila?v=4\" width=\"50px\" alt=\"judegomila\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/cfarquhar\"><img src=\"https://avatars.githubusercontent.com/cfarquhar?v=4\" width=\"50px\" alt=\"cfarquhar\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/ZoneSixGames\"><img src=\"https://avatars.githubusercontent.com/ZoneSixGames?v=4\" width=\"50px\" alt=\"ZoneSixGames\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/kenndanielso\"><img src=\"https://avatars.githubusercontent.com/kenndanielso?v=4\" width=\"50px\" alt=\"kenndanielso\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/CrypteorCapital\"><img src=\"https://avatars.githubusercontent.com/CrypteorCapital?v=4\" width=\"50px\" alt=\"CrypteorCapital\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/sultanmeghji\"><img src=\"https://avatars.githubusercontent.com/sultanmeghji?v=4\" width=\"50px\" alt=\"sultanmeghji\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/jenius-eagle\"><img src=\"https://avatars.githubusercontent.com/jenius-eagle?v=4\" width=\"50px\" alt=\"jenius-eagle\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/josephjacks\"><img src=\"https://avatars.githubusercontent.com/josephjacks?v=4\" width=\"50px\" alt=\"josephjacks\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/pingshian0131\"><img src=\"https://avatars.githubusercontent.com/pingshian0131?v=4\" width=\"50px\" alt=\"pingshian0131\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/AIdevelopersAI\"><img src=\"https://avatars.githubusercontent.com/AIdevelopersAI?v=4\" width=\"50px\" alt=\"AIdevelopersAI\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/ternary5\"><img src=\"https://avatars.githubusercontent.com/ternary5?v=4\" width=\"50px\" alt=\"ternary5\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/ChrisDMT\"><img src=\"https://avatars.githubusercontent.com/ChrisDMT?v=4\" width=\"50px\" alt=\"ChrisDMT\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/AcountoOU\"><img src=\"https://avatars.githubusercontent.com/AcountoOU?v=4\" width=\"50px\" alt=\"AcountoOU\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/chatgpt-prompts\"><img src=\"https://avatars.githubusercontent.com/chatgpt-prompts?v=4\" width=\"50px\" alt=\"chatgpt-prompts\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/Partender\"><img src=\"https://avatars.githubusercontent.com/Partender?v=4\" width=\"50px\" alt=\"Partender\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/Daniel1357\"><img src=\"https://avatars.githubusercontent.com/Daniel1357?v=4\" width=\"50px\" alt=\"Daniel1357\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/KiaArmani\"><img src=\"https://avatars.githubusercontent.com/KiaArmani?v=4\" width=\"50px\" alt=\"KiaArmani\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/zkonduit\"><img src=\"https://avatars.githubusercontent.com/zkonduit?v=4\" width=\"50px\" alt=\"zkonduit\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/fabrietech\"><img src=\"https://avatars.githubusercontent.com/fabrietech?v=4\" width=\"50px\" alt=\"fabrietech\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/scryptedinc\"><img src=\"https://avatars.githubusercontent.com/scryptedinc?v=4\" width=\"50px\" alt=\"scryptedinc\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/coreyspagnoli\"><img src=\"https://avatars.githubusercontent.com/coreyspagnoli?v=4\" width=\"50px\" alt=\"coreyspagnoli\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/AntonioCiolino\"><img src=\"https://avatars.githubusercontent.com/AntonioCiolino?v=4\" width=\"50px\" alt=\"AntonioCiolino\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/Dradstone\"><img src=\"https://avatars.githubusercontent.com/Dradstone?v=4\" width=\"50px\" alt=\"Dradstone\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/CarmenCocoa\"><img src=\"https://avatars.githubusercontent.com/CarmenCocoa?v=4\" width=\"50px\" alt=\"CarmenCocoa\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/bentoml\"><img src=\"https://avatars.githubusercontent.com/bentoml?v=4\" width=\"50px\" alt=\"bentoml\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/merwanehamadi\"><img src=\"https://avatars.githubusercontent.com/merwanehamadi?v=4\" width=\"50px\" alt=\"merwanehamadi\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/vkozacek\"><img src=\"https://avatars.githubusercontent.com/vkozacek?v=4\" width=\"50px\" alt=\"vkozacek\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/ASmithOWL\"><img src=\"https://avatars.githubusercontent.com/ASmithOWL?v=4\" width=\"50px\" alt=\"ASmithOWL\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/tekelsey\"><img src=\"https://avatars.githubusercontent.com/tekelsey?v=4\" width=\"50px\" alt=\"tekelsey\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/GalaxyVideoAgency\"><img src=\"https://avatars.githubusercontent.com/GalaxyVideoAgency?v=4\" width=\"50px\" alt=\"GalaxyVideoAgency\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/wenfengwang\"><img src=\"https://avatars.githubusercontent.com/wenfengwang?v=4\" width=\"50px\" alt=\"wenfengwang\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/rviramontes\"><img src=\"https://avatars.githubusercontent.com/rviramontes?v=4\" width=\"50px\" alt=\"rviramontes\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/indoor47\"><img src=\"https://avatars.githubusercontent.com/indoor47?v=4\" width=\"50px\" alt=\"indoor47\" /></a>&nbsp;&nbsp;<a href=\"https://github.com/ZERO-A-ONE\"><img src=\"https://avatars.githubusercontent.com/ZERO-A-ONE?v=4\" width=\"50px\" alt=\"ZERO-A-ONE\" /></a>&nbsp;&nbsp;</p>\\n\\n## \\xe2\\x9a\\xa0\\xef\\xb8\\x8f Limitations\\n\\nThis experiment aims to showcase the potential of GPT-4 but comes with some limitations:\\n\\n1. Not a polished application or product, just an experiment\\n2. May not perform well in complex, real-world business scenarios. In fact, if it actually does, please share your results!\\n3. Quite expensive to run, so set and monitor your API key limits with OpenAI!\\n\\n## \\xf0\\x9f\\x9b\\xa1 Disclaimer\\n\\nThis project, Auto-GPT, is an experimental application and is provided \"as-is\" without any warranty, express or implied. By using this software, you agree to assume all risks associated with its use, including but not limited to data loss, system failure, or any other issues that may arise.\\n\\nThe developers and contributors of this project do not accept any responsibility or liability for any losses, damages, or other consequences that may occur as a result of using this software. You are solely responsible for any decisions and actions taken based on the information provided by Auto-GPT.\\n\\n**Please note that the use of the GPT-4 language model can be expensive due to its token usage.** By utilizing this project, you acknowledge that you are responsible for monitoring and managing your own token usage and the associated costs. It is highly recommended to check your OpenAI API usage regularly and set up any necessary limits or alerts to prevent unexpected charges.\\n\\nAs an autonomous experiment, Auto-GPT may generate content or take actions that are not in line with real-world business practices or legal requirements. It is your responsibility to ensure that any actions or decisions made based on the output of this software comply with all applicable laws, regulations, and ethical standards. The developers and contributors of this project shall not be held responsible for any consequences arising from the use of this software.\\n\\nBy using Auto-GPT, you agree to indemnify, defend, and hold harmless the developers, contributors, and any affiliated parties from and against any and all claims, damages, losses, liabilities, costs, and expenses (including reasonable attorneys\\' fees) arising from your use of this software or your violation of these terms.\\n\\n## \\xf0\\x9f\\x90\\xa6 Connect with Us on Twitter\\n\\nStay up-to-date with the latest news, updates, and insights about Auto-GPT by following our Twitter accounts. Engage with the developer and the AI\\'s own account for interesting discussions, project updates, and more.\\n\\n- **Developer**: Follow [@siggravitas](https://twitter.com/siggravitas) for insights into the development process, project updates, and related topics from the creator of Entrepreneur-GPT.\\n\\nWe look forward to connecting with you and hearing your thoughts, ideas, and experiences with Auto-GPT. Join us on Twitter and let\\'s explore the future of AI together!\\n\\n<p align=\"center\">\\n  <a href=\"https://star-history.com/#Torantulino/auto-gpt&Date\">\\n    <img src=\"https://api.star-history.com/svg?repos=Torantulino/auto-gpt&type=Date\" alt=\"Star History Chart\">\\n  </a>\\n</p>\\n'\n",
      "0\n",
      "Auto-GPT-Flutter-App\n",
      "Auto-GPT-Interrupt-Controller\n",
      "Auto-GPT-Plugins\n",
      "b'# Auto-GPT-Plugins\\n\\n> \\xe2\\x9a\\xa0\\xef\\xb8\\x8f\\xf0\\x9f\\x92\\x80 **WARNING** \\xf0\\x9f\\x92\\x80\\xe2\\x9a\\xa0\\xef\\xb8\\x8f:\\n> Review the code of any plugin you use thoroughly, as plugins can execute any Python code, potentially leading to malicious activities, such as stealing your API keys.\\n\\n> \\xe2\\x9a\\x99\\xef\\xb8\\x8f **WORK IN PROGRESS** \\xe2\\x9a\\x99\\xef\\xb8\\x8f:\\n> The plugin api is not yet stabilized. If you are coding a plugin, expect it to change in the next few versions.\\n\\n## Installation\\n\\n**_\\xe2\\x9a\\xa0\\xef\\xb8\\x8fThis is a work in progress\\xe2\\x9a\\xa0\\xef\\xb8\\x8f_**\\n\\nFollow these steps to configure the Auto-GPT Plugins:\\n\\n1. **Install Auto-GPT**\\n\\n   If you haven\\'t already, follow the installation instructions provided by [Auto-GPT](https://github.com/Significant-Gravitas/Auto-GPT) to install it.\\n\\n1. **Run the following to pull the plugins folder down from the `root` of `Auto-GPT` directory**\\n\\n    To download it directly from your Auto-GPT directory, you can run this command on Linux or MacOS:\\n\\n    ```bash\\n    curl -L -o ./plugins/Auto-GPT-Plugins.zip https://github.com/Significant-Gravitas/Auto-GPT-Plugins/archive/refs/heads/master.zip\\n    ```\\n\\n    In PowerShell:\\n\\n    ```pwsh\\n    Invoke-WebRequest -Uri \"https://github.com/Significant-Gravitas/Auto-GPT-Plugins/archive/refs/heads/master.zip\"     -OutFile \"./plugins/Auto-GPT-Plugins.zip\"\\n    ```\\n\\n1. **Run the dependency install script for plugins**\\n    You can run it with either:\\n    Linux or MacOS:\\n\\n    ```bash\\n    ./run.sh --install-plugin-deps\\n    ```\\n\\n   Windows:\\n\\n    ```pwsh\\n   .\\\\run.bat --install-plugin-deps\\n    ```\\n\\n    Or directly via the CLI:\\n\\n    ```bash\\n    python -m autogpt --install-plugin-deps\\n    ````\\n\\n## Plugins\\n\\n> For interactionless use, set `ALLOWLISTED_PLUGINS=example-plugin1,example-plugin2,example-plugin3` in your `.env`\\n\\nThere are two kinds of plugins: **first party** and **third party**. First-party plugins are included in this repo and are installed by default along with other plugins when the plugin platform is installed. Third-party plugins need to be added individually. Use first-party plugins for plugins you expect others to use and want, and third-party for things specific to you. **You can see all the plugins and their contributors on this [directory](https://autoplugins.vercel.app/).**\\n\\nIf you built a plugin and it\\'s not on the directory yet simply make a PR to this [repo](https://github.com/dylanintech/autoplugins) by adding your plugin to the `data` array in `plugins.tsx`.\\n\\nYou can also see the plugins here:\\n\\n| Plugin       | Description     | Location |\\n|--------------|-----------|--------|\\n| API Tools        | This allows AutoGPT to make API calls of various kinds.                                                           | [autogpt_plugins/api_tools](https://github.com/Significant-Gravitas/Auto-GPT-Plugins/tree/master/src/autogpt_plugins/api_tools)           |\\n| Baidu Search |  This search plugin integrates Baidu search engines into Auto-GPT. | [autogpt_plugins/baidu_search](https://github.com/Significant-Gravitas/Auto-GPT-Plugins/tree/master/src/autogpt_plugins/bing_search)\\n| Bing Search      | This search plugin integrates Bing search engines into Auto-GPT.                                                  | [autogpt_plugins/bing_search](https://github.com/Significant-Gravitas/Auto-GPT-Plugins/tree/master/src/autogpt_plugins/bing_search)       |\\n| Bluesky | Enables Auto-GPT to retrieve posts from Bluesky and create new posts. | [autogpt_plugins/bluesky](https://github.com/Significant-Gravitas/Auto-GPT-Plugins/tree/master/src/autogpt_plugins/bluesky)\\n| Email            | Revolutionize email management with the Auto-GPT Email Plugin, leveraging AI to automate drafting and intelligent replies. | [autogpt_plugins/email](https://github.com/Significant-Gravitas/Auto-GPT-Plugins/tree/master/src/autogpt_plugins/email)                 |\\n| News Search      | This search plugin integrates News Articles searches, using the NewsAPI aggregator into Auto-GPT.                 | [autogpt_plugins/news_search](https://github.com/Significant-Gravitas/Auto-GPT-Plugins/tree/master/src/autogpt_plugins/news_search)   |\\n| Random Values    | Enable AutoGPT to generate various random numbers and strings.                                                    | [autogpt_plugins/random_values](https://github.com/Significant-Gravitas/Auto-GPT-Plugins/tree/master/src/autogpt_plugins/random_values) |\\n| SceneX           | Explore image storytelling beyond pixels with the Auto-GPT SceneX Plugin.                                        | [autogpt_plugins/scenex](https://github.com/Significant-Gravitas/Auto-GPT-Plugins/tree/master/src/autogpt_plugins/scenex)               |\\n| Twitter          | AutoGPT is capable of retrieving Twitter posts and other related content by accessing the Twitter platform via the v1.1 API using Tweepy.               | [autogpt_plugins/twitter](https://github.com/Significant-Gravitas/Auto-GPT-Plugins/tree/master/src/autogpt_plugins/twitter)           |\\n| Wikipedia Search | This allows AutoGPT to use Wikipedia directly.                                                                    | [autogpt_plugins/wikipedia_search](https://github.com/Significant-Gravitas/Auto-GPT-Plugins/tree/master/src/autogpt_plugins/wikipedia_search) |\\n\\nSome third-party plugins have been created by contributors that are not included in this repository. For more information about these plugins, please visit their respective GitHub pages.\\n\\n| Plugin       | Description     | Repository |\\n|--------------|-----------------|-------------|\\n| Alpaca-Trading | Trade stocks and crypto, paper or live with Auto-GPT | [danikhan632/Auto-GPT-AlpacaTrader-Plugin](https://github.com/danikhan632/Auto-GPT-AlpacaTrader-Plugin)\\n| AutoGPT User Input Request | Allow AutoGPT to specifically request user input in continous mode | [HFrovinJensen/Auto-GPT-User-Input-Plugin](https://github.com/HFrovinJensen/Auto-GPT-User-Input-Plugin)\\n| BingAI | Enable Auto-GPT to fetch information via BingAI, saving time, API requests while maintaining accuracy. This does not remove the need for OpenAI API keys | [gravelBridge/AutoGPT-BingAI](https://github.com/gravelBridge/AutoGPT-BingAI)\\n| Crypto | Trade crypto with Auto-GPT | [isaiahbjork/Auto-GPT-Crypto-Plugin](https://github.com/isaiahbjork/Auto-GPT-Crypto-Plugin)\\n| Discord | Interact with your Auto-GPT instance through Discord | [gravelBridge/AutoGPT-Discord](https://github.com/gravelBridge/AutoGPT-Discord)\\n| Dolly AutoGPT Cloner | A way to compose & run multiple Auto-GPT processes that cooperate, till core has multi-agent support | [pr-0f3t/Auto-GPT-Dolly-Plugin](https://github.com/pr-0f3t/Auto-GPT-Dolly-Plugin)\\n| Google Analytics | Connect your Google Analytics Account to Auto-GPT. | [isaiahbjork/Auto-GPT-Google-Analytics-Plugin](https://github.com/isaiahbjork/Auto-GPT-Google-Analytics-Plugin)\\n| IFTTT webhooks | This plugin allows you to easily integrate IFTTT connectivity using Maker | [AntonioCiolino/AutoGPT-IFTTT](https://github.com/AntonioCiolino/AutoGPT-IFTTT)\\n| iMessage | Send and Get iMessages using Auto-GPT | [danikhan632/Auto-GPT-Messages-Plugin](https://github.com/danikhan632/Auto-GPT-Messages-Plugin)\\n| Instagram | Instagram access | [jpetzke/AutoGPT-Instagram](https://github.com/jpetzke/AutoGPT-Instagram)\\n| Mastodon  | Simple Mastodon plugin to send toots through a Mastodon account | [ppetermann/AutoGPTMastodonPlugin](https://github.com/ppetermann/AutoGPTMastodonPlugin)\\n| MetaTrader | Connect your MetaTrader Account to Auto-GPT. | [isaiahbjork/Auto-GPT-MetaTrader-Plugin](https://github.com/isaiahbjork/Auto-GPT-MetaTrader-Plugin) |\\n| Notion      | Notion plugin for Auto-GPT.  | [doutv/Auto-GPT-Notion](https://github.com/doutv/Auto-GPT-Notion) |\\n| Spoonacular | Find recipe insiprations using Auto-GPT | [minfenglu/Auto-GPT-Spoonacular-Plugin](https://github.com/minfenglu/Auto-GPT-Spoonacular-Plugin)\\n| System Information      | This plugin adds an extra line to the prompt, serving as a hint for the AI to use shell commands likely supported by the current system. By incorporating this plugin, you can ensure that the AI model provides more accurate and system-specific shell commands, improving its overall performance and usefulness. | [hdkiller/Auto-GPT-SystemInfo](https://github.com/hdkiller/Auto-GPT-SystemInfo) |\\n| Telegram | A smoothly working Telegram bot that gives you all the messages you would normally get through the Terminal. | [Wladastic/Auto-GPT-Telegram-Plugin](https://github.com/Wladastic/Auto-GPT-Telegram-Plugin) |\\n| TiDB Serverless   | Connect your TiDB Serverless database to Auto-GPT, enable get query results from database | [pingcap/Auto-GPT-TiDB-Serverless-Plugin](https://github.com/pingcap/Auto-GPT-TiDB-Serverless-Plugin)\\n| Todoist-Plugin | Allow AutoGPT to programatically interact with yor Todoist to create, update, and manage your Todoist | [danikhan632/Auto-GPT-Telegram-Plugin](https://github.com/danikhan632/Auto-GPT-Todoist-Plugin) |\\n| Weather | A simple weather plugin wrapping around python-weather | [ppetermann/Auto-GPT-WeatherPlugin](https://github.com/ppetermann/Auto-GPT-WeatherPlugin)\\n| Web-Interaction | Enable Auto-GPT to fully interact with websites! Allows Auto-GPT to click elements, input text, and scroll | [gravelBridge/AutoGPT-Web-Interaction](https://github.com/gravelBridge/AutoGPT-Web-Interaction)\\n| WolframAlpha | Access to WolframAlpha to do math and get accurate information | [gravelBridge/AutoGPT-WolframAlpha](https://github.com/gravelBridge/AutoGPT-WolframAlpha)\\n| YouTube   | Various YouTube features including downloading and understanding | [jpetzke/AutoGPT-YouTube](https://github.com/jpetzke/AutoGPT-YouTube) |\\n| Zapier webhooks | This plugin allows you to easily integrate Zapier connectivity | [AntonioCiolino/AutoGPT-Zapier](https://github.com/AntonioCiolino/AutoGPT-Zapier)\\n| Project Management | Streamline your Project Management with ease: Jira, Trello, and Google Calendar Made Effortless| [minfenglu/AutoGPT-PM-Plugin](https://github.com/minfenglu/AutoGPT-PM-Plugin)\\n\\n## Configuration\\n\\nFor interactionless use, set:\\n\\n`ALLOWLISTED_PLUGINS=example-plugin1,example-plugin2,etc` in your `.env` file to allow plugins to load without prompting.\\n`DENYLISTED_PLUGINS=example-plugin1,example-plugin2,etc` in your `.env` file to block plugins from loading without prompting.\\n\\n## Making a plugin\\n\\nCreating a plugin is a rewarding experience! You can choose between first-party or third-party plugins. First-party plugins are included in this repo and are installed by default along with other plugins when the plugin platform is installed. Third-party plugins need to be added individually. Use first-party plugins for plugins you expect others to use and want, and third-party for things specific to you.\\n\\n### First Party How-To\\n\\n1. Clone the plugins repo\\n1. Follow the structure of the other plugins, implementing the plugin interface as required\\n1. Write your tests\\n1. Add your name to the [codeowners](.github/CODEOWNERS) file\\n1. Add your plugin to the [Readme](README.md)\\n1. Make a PR back to this repo!\\n\\n### Third Party How-To\\n\\n1. Clone [the third party template](https://github.com/Significant-Gravitas/Auto-GPT-Plugin-Template)\\n1. Follow the instructions in the [third party template readme](https://github.com/Significant-Gravitas/Auto-GPT-Plugin-Template)\\n\\n### Migrating Third Party to First Party\\n\\n> Thanks for contributing a plugin to the project!\\n\\n1. Clone this repo.\\n1. Make a folder for your plugin under `src/autogpt_plugins`. Name it a simple descriptive name such as `notion`, `twitter`, or `web_ui`.\\n1. Take the files from your third-party plugin located at `src/auto_gpt_plugin_template` and add them into the folder you created\\n1. Add your readme from your third-party plugin to the folder you created\\n1. Add your plugin to the root readme with a description and a link to your plugin-specific readme\\n1. Add your plugin\\'s Python package requirements to `requirements.txt`\\n1. Add tests to get your plugin to 80% code coverage\\n1. Add your name to the [codeowners](.github/CODEOWNERS) file\\n1. Add your plugin to the [Readme](README.md)\\n1. Make a PR back to this repo!\\n\\n## Get Help\\n\\nVisit the [discord](https://discord.gg/autogpt) server for more information.\\n'\n",
      "0\n",
      "AutoGPT-Flutter-App\n",
      "b\"# Auto-GPT WebUI\\n\\nThis project is a frontend web application that runs and interacts with [Auto-GPT](https://github.com/Torantulino/Auto-GPT). The backend application provides the core logic and functionality, while this frontend application wraps over it and offers a user-friendly interface.\\n\\n### \\xf0\\x9f\\x8c\\x9f Special thanks to [the original Auto-GPT project](https://github.com/Torantulino/Auto-GPT) and its creator, [Torantulino](https://github.com/Torantulino), for his hard work and for making this project possible! \\xf0\\x9f\\x8c\\x9f\\n\\n---\\n\\n## Disclaimer: Limited Availability for Maintenance\\n\\nPlease note that I developed this project primarily for personal amusement and as a learning experience.\\nMy availability to address issues, review pull requests, or implement new features may be limited, as I have other full-time commitments.\\n\\nIf you find this project useful and would like to contribute, I welcome and highly appreciate community involvement. \\xf0\\x9f\\x92\\x9b\\n\\nThank you for your understanding, and I hope you find this helpful!\\n\\n---\\n\\n## \\xf0\\x9f\\x9b\\xa0\\xef\\xb8\\x8f Installation & Usage\\n\\nClone the repo, then enter the root directory and run the folling commands:\\n\\n```bash\\nnpm install\\nnpm run setup-auto-gpt\\n```\\n\\n- The first command will install the required dependencies.\\n- The second command will clone the Auto-GPT repository and install its dependencies.\\n\\nRun the following command in order to start both the frontend application and the Node.js server, which will in turn start and stop the python script, and inform the frontend of the script's output:\\n\\n```bash\\nnpm start\\n```\\n\\nWhen you first open up the web app, you will see a few alerts about missing API Keys. You need to fill these in in order for the application to work correctly. Look below for instructions.\\n\\n---\\n\\n## \\xe2\\x9a\\x99 Requirements and Configuration\\n\\n### OpenAI API key\\n\\nFollow these steps to obtain an OpenAI API key:\\n\\n1. Visit the [**OpenAI Platform**](https://platform.openai.com/signup) website.\\n1. If you don't have an account yet, sign up by providing your email, name, and creating a password.\\n1. After signing up or logging in, go to the [**API Keys**](https://platform.openai.com/account/api-keys) section in your account.\\n1. Click the **Create an API key** button to generate a new API key.\\n1. Copy the API key and paste it in the WebUI to use it with Auto-GPT.\\n\\n- \\xe2\\x9a\\xa0\\xef\\xb8\\x8f Keep your API key secure and never share it with anyone. Treat it like a password.\\n\\n### Pinecone API key\\n\\nFollow these steps to obtain a Pinecone API key:\\n\\n1. Visit the [**Pinecone**](https://app.pinecone.io/register) website.\\n1. If you don't have an account yet, sign up by providing your email and creating a password.\\n1. After signing up or logging in, go to the [**Projects**](https://app.pinecone.io/projects) page.\\n1. Click on the default project, or create a new one if desired.\\n1. In the left sidebar, click on the **API Keys** section.\\n1. Create a new API key and copy-paste it in the WebUI to use it with Auto-GPT.\\n\\n- \\xe2\\x9a\\xa0\\xef\\xb8\\x8f Keep your API key secure and never share it with anyone. Treat it like a password.\\n\\n### Python environment\\n\\nYou will also need both [Python 3.8 or later](https://www.tutorialspoint.com/how-to-install-python-in-windows) and [Node.js](https://nodejs.org/en) installed on your system.\\n\\n---\\n\\n## \\xf0\\x9f\\x9b\\xa0 Tools and Libraries\\n\\n- [**Auto-GPT**](https://github.com/Torantulino/Auto-GPT) - As mentioned, the backend logic and the meat of this application is provided by the Auto-GPT project. Check out the repo for more information if you'd like to [contribute to](https://github.com/Torantulino/Auto-GPT/blob/master/CONTRIBUTING.md) or [sponsor](https://github.com/sponsors/Torantulino) the project.\\n- [**Turborepo**](https://turborepo.org/) - A high-performance build system and repository organizer for JavaScript and TypeScript codebases.\\n- [**Vite**](https://vitejs.dev/) - A build tool and development server for web applications, designed to be fast and lightweight.\\n- [**React**](https://reactjs.org/) - A JavaScript library for building user interfaces, developed and maintained by Facebook.\\n- [**Chakra UI**](https://chakra-ui.com/) - A simple, modular, and accessible component library for React.\\n- [**Zustand**](https://github.com/pmndrs/zustand) - A lightweight and easy-to-use state management library for React.\\n- [**Express**](https://expressjs.com/) - A fast, unopinionated, minimalist web framework for Node.js.\\n- [**dotenv**](https://www.npmjs.com/package/dotenv) - A library that loads environment variables from a `.env` file into your Node.js application.\\n- [**Emotion**](https://emotion.sh/docs/introduction) - A library for CSS-in-JS, allowing you to style your React components using JavaScript.\\n- [**TypeScript**](https://www.typescriptlang.org/) - A popular superset of JavaScript that adds static types and other features for better code maintainability.\\n- [**ESLint**](https://eslint.org/) - A tool for identifying and reporting on patterns found in ECMAScript/JavaScript code.\\n- [**Prettier**](https://prettier.io/) - A widely-used code formatter that enforces a consistent style by parsing your code and re-printing it with its own rules.\\n\"\n",
      "0\n",
      "AutoGPT-Interrupts-Plugin\n",
      "b\"# Auto-GPT-Plugin-Template\\nA starting point for developing your own external plug-in for Auto-GPT\\n\\n# **If you want your plugin to live within the codebase, use the example in the [plugins repo](https://github.com/Significant-Gravitas/Auto-GPT-Plugins) instead**\\n\\n### Plugin Installation Steps\\n\\n1. **Clone or download the plugin repository:**\\n   Clone the plugin repository, or download the repository as a zip file.\\n  \\n   ![Download Zip](https://raw.githubusercontent.com/BillSchumacher/Auto-GPT/master/plugin.png)\\n\\n2. **Install the plugin's dependencies (if any):**\\n   Navigate to the plugin's folder in your terminal, and run the following command to install any required dependencies:\\n\\n   ``` shell\\n      pip install -r requirements.txt\\n   ```\\n\\n3. **Package the plugin as a Zip file:**\\n   If you cloned the repository, compress the plugin folder as a Zip file.\\n\\n4. **Copy the plugin's Zip file:**\\n   Place the plugin's Zip file in the `plugins` folder of the Auto-GPT repository.\\n\\n5. **Allowlist the plugin (optional):**\\n   Add the plugin's class name to the `ALLOWLISTED_PLUGINS` in the `.env` file to avoid being prompted with a warning when loading the plugin:\\n\\n   ``` shell\\n   ALLOWLISTED_PLUGINS=example-plugin1,example-plugin2,example-plugin3\\n   ```\\n\\n   If the plugin is not allowlisted, you will be warned before it's loaded.\\n\"\n",
      "0\n",
      "Bevy_proj\n",
      "b'# Bevy_proj\\n'\n",
      "0\n",
      "CleanUpCrew\n",
      "b'This project was bootstrapped with [Create React App](https://github.com/facebook/create-react-app).\\n\\n## Available Scripts\\n\\nIn the project directory, you can run:\\n\\n### `npm start`\\n\\nRuns the app in the development mode.<br />\\nOpen [http://localhost:3000](http://localhost:3000) to view it in the browser.\\n\\nThe page will reload if you make edits.<br />\\nYou will also see any lint errors in the console.\\n\\n### `npm test`\\n\\nLaunches the test runner in the interactive watch mode.<br />\\nSee the section about [running tests](https://facebook.github.io/create-react-app/docs/running-tests) for more information.\\n\\n### `npm run build`\\n\\nBuilds the app for production to the `build` folder.<br />\\nIt correctly bundles React in production mode and optimizes the build for the best performance.\\n\\nThe build is minified and the filenames include the hashes.<br />\\nYour app is ready to be deployed!\\n\\nSee the section about [deployment](https://facebook.github.io/create-react-app/docs/deployment) for more information.\\n\\n### `npm run eject`\\n\\n**Note: this is a one-way operation. Once you `eject`, you can\\xe2\\x80\\x99t go back!**\\n\\nIf you aren\\xe2\\x80\\x99t satisfied with the build tool and configuration choices, you can `eject` at any time. This command will remove the single build dependency from your project.\\n\\nInstead, it will copy all the configuration files and the transitive dependencies (webpack, Babel, ESLint, etc) right into your project so you have full control over them. All of the commands except `eject` will still work, but they will point to the copied scripts so you can tweak them. At this point you\\xe2\\x80\\x99re on your own.\\n\\nYou don\\xe2\\x80\\x99t have to ever use `eject`. The curated feature set is suitable for small and middle deployments, and you shouldn\\xe2\\x80\\x99t feel obligated to use this feature. However we understand that this tool wouldn\\xe2\\x80\\x99t be useful if you couldn\\xe2\\x80\\x99t customize it when you are ready for it.\\n\\n## Learn More\\n\\nYou can learn more in the [Create React App documentation](https://facebook.github.io/create-react-app/docs/getting-started).\\n\\nTo learn React, check out the [React documentation](https://reactjs.org/).\\n\\n### Code Splitting\\n\\nThis section has moved here: https://facebook.github.io/create-react-app/docs/code-splitting\\n\\n### Analyzing the Bundle Size\\n\\nThis section has moved here: https://facebook.github.io/create-react-app/docs/analyzing-the-bundle-size\\n\\n### Making a Progressive Web App\\n\\nThis section has moved here: https://facebook.github.io/create-react-app/docs/making-a-progressive-web-app\\n\\n### Advanced Configuration\\n\\nThis section has moved here: https://facebook.github.io/create-react-app/docs/advanced-configuration\\n\\n### Deployment\\n\\nThis section has moved here: https://facebook.github.io/create-react-app/docs/deployment\\n\\n### `npm run build` fails to minify\\n\\nThis section has moved here: https://facebook.github.io/create-react-app/docs/troubleshooting#npm-run-build-fails-to-minify\\n'\n",
      "0\n",
      "COVID-Vaccine-Tracker\n",
      "b\"# COVID Vaccine Tracker\\nThis project was created to asses when US states would reach 80% vaccination and their overall vaccine progress.\\\\\\nThe code pulls from vaccine databases as factors in State Population.\\\\\\nThis was made back in March of 2021 but could see modification for future use with the COVID Delta Variant. \\\\\\nThe predictions for 80% didn't quite work out as the curve went logarithimic at about 50% which was earlier than expetced, but until that point it provided fairly vaccine predictions.\\n\\n\\n#Google Colabatory Link\\nhttps://drive.google.com/file/d/1nm6L6jDxKM4pm2l0Elrzsv8SC84NHP5E/view?usp=sharing\\n\\n## example Screeshot from April 2021\\n![image1](https://elasticbeanstalk-us-east-1-858154033039.s3.amazonaws.com/resources/covid-tracker.PNG)\\n\"\n",
      "0\n",
      "danikhan632\n",
      "edamam-python\n",
      "b'# edamam-python\\n\\nPython library to interact with the edamam online food database\\n'\n",
      "0\n",
      "FastChat\n",
      "b'# FastChat\\nAn open platform for training, serving, and evaluating large language model based chatbots.\\n\\n## Release\\n\\n<p align=\"center\">\\n<a href=\"https://vicuna.lmsys.org\"><img src=\"assets/vicuna-logo.jpeg\" width=\"20%\"></a>\\n</p>\\n\\n- \\xf0\\x9f\\x94\\xa5 We released **Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90% ChatGPT Quality**. Checkout the blog [post](https://vicuna.lmsys.org) and [demo](https://chat.lmsys.org/).\\n\\n<a href=\"https://chat.lmsys.org\"><img src=\"assets/demo-narrow.gif\" width=\"70%\"></a>\\n\\nJoin our [Discord](https://discord.gg/h6kCZb72G7) server and follow our [Twitter](https://twitter.com/lmsysorg) to get the latest updates.\\n\\n## Contents\\n- [Install](#install)\\n- [Vicuna Weights](#vicuna-weights)\\n- [Serving](#serving)\\n- [Evaluation](#evaluation)\\n- [Fine-tuning](#fine-tuning)\\n\\n## Install\\n\\n### Method 1: With pip\\n\\n```bash\\n# Install FastChat\\npip3 install fschat\\n\\n# Install a specific commit of huggingface/transformers\\n# Our released weights do not work with commits after this due to some upstream changes in the tokenizer.\\npip3 install git+https://github.com/huggingface/transformers@c612628045822f909020f7eb6784c79700813eda\\n```\\n\\n### Method 2: From source\\n\\n1. Clone this repository and navigate to FastChat folder\\n```bash\\ngit clone https://github.com/lm-sys/FastChat.git\\ncd FastChat\\n```\\n\\n2. Install Package\\n```bash\\npip3 install --upgrade pip  # enable PEP 660 support\\npip3 install -e .\\n```\\n\\n## Vicuna Weights\\nWe release [Vicuna](https://vicuna.lmsys.org/) weights as delta weights to comply with the LLaMA model license.\\nYou can add our delta to the original LLaMA weights to obtain the Vicuna weights. Instructions:\\n\\n1. Get the original LLaMA weights in the huggingface format by following the instructions [here](https://huggingface.co/docs/transformers/main/model_doc/llama).\\n2. Use the following scripts to get Vicuna weights by applying our delta. It will automatically download delta weights from our Hugging Face account.\\n\\n**NOTE**:\\nOur released weights are only compatible with huggingface/transformers commit: `c612628045822f909020f7eb6784c79700813eda`.\\nThe weights do not work with commits after this due to some upstream changes in the tokenizer. We install the correct version of\\ntransformers when fastchat is installed.\\n\\n### Vicuna-13B\\nThis conversion command needs around 60 GB of CPU RAM.\\n```bash\\npython3 -m fastchat.model.apply_delta \\\\\\n    --base /path/to/llama-13b \\\\\\n    --target /output/path/to/vicuna-13b \\\\\\n    --delta lmsys/vicuna-13b-delta-v0\\n```\\n\\n### Vicuna-7B\\nComing soon.\\n\\n## Serving\\n\\n### Command Line Interface\\n\\n#### Single GPU\\nThe command below requires around 28GB of GPU memory for Vicuna-13B.\\n```\\npython3 -m fastchat.serve.cli --model-name /path/to/vicuna/weights\\n```\\n\\n#### Multi GPU\\nIf you do not have enough GPU memory, you can use model parallelism to aggregate memory from multiple GPUs on the same machine.\\n```\\npython3 -m fastchat.serve.cli --model-name /path/to/vicuna/weights --num-gpus 2\\n```\\n\\n#### CPU Only\\nThis runs on the CPU only and does not require GPU. It requires around 60GB of CPU memory for Vicuna-13B.\\n```\\npython3 -m fastchat.serve.cli --model-name /path/to/vicuna/weights --device cpu\\n```\\n\\n#### Others (Quantization, More Platforms)\\nCurrently, we only provide some basic commands for running the model.\\nWe are actively exploring methods to make the model easier to run on more platforms.\\nContributions and pull requests are welcome.\\n\\n### Web UI\\n\\n#### Launch a controller\\n```bash\\npython3 -m fastchat.serve.controller\\n```\\n\\n#### Launch a model worker\\n```bash\\npython3 -m fastchat.serve.model_worker --model-path /path/to/vicuna/weights\\n```\\nWait until the process finishes loading the model and you see \"Uvicorn running on ...\".\\n\\n#### Send a test message\\n```bash\\npython3 -m fastchat.serve.test_message --model-name vicuna-13b\\n```\\n\\n#### Launch a gradio web server.\\n```bash\\npython3 -m fastchat.serve.gradio_web_server\\n```\\n#### You can open your browser and chat with a model now.\\n\\n## Evaluation\\n\\nOur AI-enhanced [evaluation](fastchat/eval) pipeline is based on GPT-4. Here are some high-level instructions for using the pipeline:\\n\\nFirst, generate answers from different models. Use `qa_baseline_gpt35.py` for ChatGPT, or specify the model checkpoint and run `model_qa.py` for Vicuna and other models.\\n\\nThen, use GPT-4 to generate reviews automatically, which can be done manually if the GPT-4 API is not available to you. Once you have your evaluation data, visualize the results by running `generate_webpage_data_from_table.py`, which generates data for a static website.\\n\\nFinally, serve a static website under the `webpage` directory. You can simply use `python3 -m http.server` to serve the website locally.\\n\\nBesides the evaluation workflow, we also document the data format used for evaluation, which is encoded with JSON Lines and includes information on models, prompts, reviewers, questions, answers, and reviews. You can customize the evaluation process or contribute to our project by accessing relevant [data](fastchat/eval/table/).\\n\\nCheck [evaluation](fastchat/eval) for detailed instructions.\\n\\n## Fine-tuning\\n### Data\\n\\nVicuna is created by fine-tuning a LLaMA base model using approximately 70K user-shared conversations gathered from ShareGPT.com with public APIs. To ensure data quality, we convert the HTML back to markdown and filter out some inappropriate or low-quality samples. Additionally, we divide lengthy conversations into smaller segments that fit the model\\'s maximum context length. For detailed instructions to clean the ShareGPT data, check out [here](docs/commands/data_cleaning.md).\\n\\nDue to some concerns, we may not release the data at the moment. If you would like to try the fine-tuning code, you can try to run it with our [preprocessed alpaca dataset](playground/data/alpaca-data-conversation.json) (originally from [here](https://github.com/tatsu-lab/stanford_alpaca)).\\n\\n### Code and Hyperparameters\\nWe fine-tune the model using the code from [Stanford Alpaca](https://github.com/tatsu-lab/stanford_alpaca), with some modifications to support gradient checkpointing and [Flash Attention](https://github.com/HazyResearch/flash-attention). We use similar hyperparameters as the Stanford Alpaca.\\n\\n| Hyperparameter | Global Batch Size | Learning rate | Epochs | Max length | Weight decay |\\n| --- | ---: | ---: | ---: | ---: | ---: |\\n| Vicuna-13B | 128 | 2e-5 | 3 | 2048 | 0 |\\n\\n### Fine-tuning on Any Cloud with SkyPilot\\n[SkyPilot](https://github.com/skypilot-org/skypilot) is a framework built by UC Berkeley for easily and cost effectively running ML workloads on any cloud (AWS, GCP, Azure, Lambda, etc.). \\nTo use SkyPilot, install it with the following command and setup the cloud credentials locally following the instructions [here](https://skypilot.readthedocs.io/en/latest/getting-started/installation.html).\\n```bash\\n# Install skypilot from the master branch\\npip install git+https://github.com/skypilot-org/skypilot.git\\n```\\n#### Vicuna\\nVicuna can be trained on 8 A100 GPUs with 80GB memory. The following command will automatically launch a node satisfying the requirement, setup and run the training job on it.\\n```bash\\nsky launch -c vicuna -s scripts/train-vicuna.yaml --env WANDB_API_KEY\\n```\\nOther options are also valid:\\n```bash\\n# Launch it on managed spot to save 3x cost (train Vicuna-13B with around $300)\\nsky spot launch -n vicuna scripts/train-vicuna.yaml --env WANDB_API_KEY\\n\\n# Train a 7B model\\nsky launch -c vicuna -s scripts/train-vicuna.yaml --env WANDB_API_KEY --env MODEL_SIZE=7\\n```\\nNote: Please make sure the `WANDB_API_KEY` has been setup on your local machine. You can find the API key on your [wandb profile page](https://wandb.ai/authorize). If you would like to train the model without using wandb, you can replace the `--env WANDB_API_KEY` flag with `--env WANDB_MODE=offline`.\\n\\n#### Alpaca\\nLaunch the training job with the following line (will be launched on a single node with 4 A100-80GB GPUs)\\n```\\nsky launch -c alpaca -s scripts/train-alpaca.yaml --env WANDB_API_KEY\\n```\\n\\n### Fine-tuning with Local GPUs\\nVicuna can also be trained on 8 A100 GPUs with 80GB memory with the following code. To train on fewer GPUs, you can reduce the `per_device_train_batch_size` and increase the `gradient_accumulation_steps` accordingly to keep the global batch size the same. To setup the environment, please see the setup section in [scripts/train-vicuna.yaml](scripts/train-vicuna.yaml).\\n```bash\\ntorchrun --nnodes=1 --nproc_per_node=8 --master_port=<your_random_port> \\\\\\n    fastchat/train/train_mem.py \\\\\\n    --model_name_or_path <path-to-llama-model-weight> \\\\\\n    --data_path <path-to-data> \\\\\\n    --bf16 True \\\\\\n    --output_dir ./checkpoints \\\\\\n    --num_train_epochs 3 \\\\\\n    --per_device_train_batch_size 4 \\\\\\n    --per_device_eval_batch_size 4 \\\\\\n    --gradient_accumulation_steps 1 \\\\\\n    --evaluation_strategy \"no\" \\\\\\n    --save_strategy \"steps\" \\\\\\n    --save_steps 1200 \\\\\\n    --save_total_limit 100 \\\\\\n    --learning_rate 2e-5 \\\\\\n    --weight_decay 0. \\\\\\n    --warmup_ratio 0.03 \\\\\\n    --lr_scheduler_type \"cosine\" \\\\\\n    --logging_steps 1 \\\\\\n    --fsdp \"full_shard auto_wrap\" \\\\\\n    --fsdp_transformer_layer_cls_to_wrap \\'LlamaDecoderLayer\\' \\\\\\n    --tf32 True \\\\\\n    --model_max_length 2048 \\\\\\n    --gradient_checkpointing True \\\\\\n    --lazy_preprocess True\\n```\\n'\n",
      "0\n",
      "FlutterTowerDefense\n",
      "b'# tower_defense\\r\\n\\r\\nA new Flutter project.\\r\\n\\r\\n## Getting Started\\r\\n\\r\\nThis project is a starting point for a Flutter application.\\r\\n\\r\\nA few resources to get you started if this is your first Flutter project:\\r\\n\\r\\n- [Lab: Write your first Flutter app](https://flutter.dev/docs/get-started/codelab)\\r\\n- [Cookbook: Useful Flutter samples](https://flutter.dev/docs/cookbook)\\r\\n\\r\\nFor help getting started with Flutter, view our\\r\\n[online documentation](https://flutter.dev/docs), which offers tutorials,\\r\\nsamples, guidance on mobile development, and a full API reference.\\r\\n#\\x00 \\x00t\\x00o\\x00w\\x00e\\x00r\\x00_\\x00d\\x00e\\x00f\\x00e\\x00n\\x00s\\x00e\\x00\\r\\x00\\n\\x00'\n",
      "0\n",
      "GAN-Pokemon\n",
      "b'# pokeGAN\\n\\n## Overview\\nGoogle Colabatory Link: https://colab.research.google.com/drive/1CVpzWmrZuc_vxuqhOI2D0EpMeLUifdlo?usp=sharing \\\\\\nThis code utilizes a General Adversarial network that takes existing images of Pokemon to create new images of Pokemon\\n\\n## Dependencies (pip install) \\n```\\ncv2\\ntensorflow( >=1.0)\\nscipy\\nnumpy\\n```\\n## Usage\\n```\\ncd pokeGAN\\npython resize.py\\npython RGBA2RGB.py\\npython pokeGAN.py\\n```\\nOr just use the Google Colab Link : https://colab.research.google.com/drive/1CVpzWmrZuc_vxuqhOI2D0EpMeLUifdlo?usp=sharing\\n\\n## example pokemon\\n![image1](https://github.com/moxiegushi/pokeGAN/raw/master/images/Notes_1500532347861.jpeg)\\n\\n\\n\\nCredits: [moxiegushi]\\n'\n",
      "0\n",
      "MagiskOnWSA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Request GET /repos/danikhan632/MagiskOnWSA/contents/README.md failed with 403: Forbidden\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Malaria-Dectector\n",
      "b'# Malaria-Dectector\\n\\nImplements Computer Vision to classify blood cells as infected or not \\\\\\nThe neural network was 98% accurate at classifying blood cells \\\\\\nAssists medical professionals in diagnosing cases of malaria\\n## Dependencies (pip install) \\n```\\ncv2\\ntensorflow( >=1.0)\\nscipy\\nnumpy\\n```\\n## Usage\\n```\\nrun malaria.ipynb\\n```\\n\\n## example pokemon\\n![image1](https://raw.githubusercontent.com/danikhan632/Malaria-Dectector/main/img.png)\\n'\n",
      "0\n",
      "metal-dialect\n",
      "b'# MLIR metal dialect\\n\\n## Get Source\\nTo download the source code run:\\n```sh\\ngit clone --recursive https://github.com/NicolaLancellotti/metal-dialect.git\\n```\\n\\n## Getting Started\\n\\n- Install [Xcode](https://developer.apple.com/xcode/)\\n\\n- Change the path to the active Xcode developer directory\\n```\\nsudo xcode-select -s <path to Xcode.app>/Contents/Developer\\n```\\n- Install CMake \\n```\\nbrew install cmake\\n```\\n\\n- Install Ninja \\n```\\nbrew install ninja\\n```\\n\\n- Build the project and the examples\\n```\\nmake all\\n```\\n- Run the examples\\n```\\nmake run_examples\\n```\\n'\n",
      "0\n",
      "microservices_app\n",
      "b'# microservices_app\\nThis is a video compression web app\\n'\n",
      "0\n",
      "MyFlutterApp\n",
      "b'# yeezus2\\n\\nA new Flutter project.\\n\\n## Getting Started\\n\\nThis project is a starting point for a Flutter application.\\n\\nA few resources to get you started if this is your first Flutter project:\\n\\n- [Lab: Write your first Flutter app](https://docs.flutter.dev/get-started/codelab)\\n- [Cookbook: Useful Flutter samples](https://docs.flutter.dev/cookbook)\\n\\nFor help getting started with Flutter development, view the\\n[online documentation](https://docs.flutter.dev/), which offers tutorials,\\nsamples, guidance on mobile development, and a full API reference.\\n'\n",
      "0\n",
      "pic8259_x86\n",
      "b'x86 only Abstractions for the 8259 and 8259A interrupt controllers, credit to Eric Kidd <git@randomhacks.net> and Phil Opperman'\n",
      "0\n",
      "react-native-tauri-template\n",
      "robot-path-finder\n",
      "Self-Driving-Car\n",
      "b'# Self-Driving-Car\\n\\n![alt text](https://elasticbeanstalk-us-east-1-858154033039.s3.amazonaws.com/resources/vhjjj.png)\\n\\nThis project went along with this Udemy course:\\nhttps://www.udemy.com/course/applied-deep-learningtm-the-complete-self-driving-car-course/learn/lecture/11442658?start=1005#overview\\n'\n",
      "0\n",
      "text-generation-webui\n",
      "b'# Text generation web UI\\n\\nA gradio web UI for running Large Language Models like LLaMA, llama.cpp, GPT-J, Pythia, OPT, and GALACTICA.\\n\\nIts goal is to become the [AUTOMATIC1111/stable-diffusion-webui](https://github.com/AUTOMATIC1111/stable-diffusion-webui) of text generation.\\n\\n|![Image1](https://github.com/oobabooga/screenshots/raw/main/qa.png) | ![Image2](https://github.com/oobabooga/screenshots/raw/main/cai3.png) |\\n|:---:|:---:|\\n|![Image3](https://github.com/oobabooga/screenshots/raw/main/gpt4chan.png) | ![Image4](https://github.com/oobabooga/screenshots/raw/main/galactica.png) |\\n\\n## Features\\n\\n* 3 interface modes: default, notebook, and chat\\n* Multiple model backends: tranformers, llama.cpp, AutoGPTQ, GPTQ-for-LLaMa, RWKV, FlexGen\\n* Dropdown menu for quickly switching between different models\\n* LoRA: load and unload LoRAs on the fly, load multiple LoRAs at the same time, train a new LoRA\\n* Precise instruction templates for chat mode, including Alpaca, Vicuna, Open Assistant, Dolly, Koala, ChatGLM, MOSS, RWKV-Raven, Galactica, StableLM, WizardLM, Baize, Ziya, Chinese-Vicuna, MPT, INCITE, Wizard Mega, KoAlpaca, Vigogne, Bactrian, h2o, and OpenBuddy\\n* [Multimodal pipelines, including LLaVA and MiniGPT-4](https://github.com/oobabooga/text-generation-webui/tree/main/extensions/multimodal)\\n* 8-bit and 4-bit inference through bitsandbytes\\n* CPU mode for transformers models\\n* [DeepSpeed ZeRO-3 inference](docs/DeepSpeed.md)\\n* [Extensions](docs/Extensions.md)\\n* [Custom chat characters](docs/Chat-mode.md)\\n* Very efficient text streaming\\n* Markdown output with LaTeX rendering, to use for instance with [GALACTICA](https://github.com/paperswithcode/galai)\\n* Nice HTML output for GPT-4chan\\n* API, including endpoints for websocket streaming ([see the examples](https://github.com/oobabooga/text-generation-webui/blob/main/api-examples))\\n\\nTo learn how to use the various features, check out the Documentation: https://github.com/oobabooga/text-generation-webui/tree/main/docs\\n\\n## Installation\\n\\n### One-click installers\\n\\n| Windows | Linux | macOS | WSL |\\n|--------|--------|--------|--------|\\n| [oobabooga-windows.zip](https://github.com/oobabooga/text-generation-webui/releases/download/installers/oobabooga_windows.zip) | [oobabooga-linux.zip](https://github.com/oobabooga/text-generation-webui/releases/download/installers/oobabooga_linux.zip) |[oobabooga-macos.zip](https://github.com/oobabooga/text-generation-webui/releases/download/installers/oobabooga_macos.zip) | [oobabooga-wsl.zip](https://github.com/oobabooga/text-generation-webui/releases/download/installers/oobabooga_wsl.zip) |\\n\\nJust download the zip above, extract it, and double-click on \"start\". The web UI and all its dependencies will be installed in the same folder.\\n\\n* The source codes are here: https://github.com/oobabooga/one-click-installers\\n* There is no need to run the installers as admin.\\n* AMD doesn\\'t work on Windows.\\n* Huge thanks to [@jllllll](https://github.com/jllllll), [@ClayShoaf](https://github.com/ClayShoaf), and [@xNul](https://github.com/xNul) for their contributions to these installers.\\n\\n### Manual installation using Conda\\n\\nRecommended if you have some experience with the command line.\\n\\n#### 0. Install Conda\\n\\nhttps://docs.conda.io/en/latest/miniconda.html\\n\\nOn Linux or WSL, it can be automatically installed with these two commands:\\n\\n```\\ncurl -sL \"https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\" > \"Miniconda3.sh\"\\nbash Miniconda3.sh\\n```\\nSource: https://educe-ubc.github.io/conda.html\\n\\n#### 1. Create a new conda environment\\n\\n```\\nconda create -n textgen python=3.10.9\\nconda activate textgen\\n```\\n\\n#### 2. Install Pytorch\\n\\n| System | GPU | Command |\\n|--------|---------|---------|\\n| Linux/WSL | NVIDIA | `pip3 install torch torchvision torchaudio` |\\n| Linux | AMD | `pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/rocm5.4.2` |\\n| MacOS + MPS (untested) | Any | `pip3 install torch torchvision torchaudio` |\\n| Windows | NVIDIA | `pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu117` |\\n\\nThe up-to-date commands can be found here: https://pytorch.org/get-started/locally/. \\n\\n#### 2.1 Special instructions\\n\\n* MacOS users: https://github.com/oobabooga/text-generation-webui/pull/393\\n* AMD users: https://rentry.org/eq3hg\\n\\n#### 3. Install the web UI\\n\\n```\\ngit clone https://github.com/oobabooga/text-generation-webui\\ncd text-generation-webui\\npip install -r requirements.txt\\n```\\n\\n#### llama.cpp with GPU acceleration\\n\\nRequires the additional compilation step described here: [GPU acceleration](https://github.com/oobabooga/text-generation-webui/blob/main/docs/llama.cpp-models.md#gpu-acceleration).\\n\\n#### bitsandbytes\\n\\nbitsandbytes >= 0.39 may not work on older NVIDIA GPUs. In that case, to use `--load-in-8bit`, you may have to downgrade like this:\\n\\n* Linux: `pip install bitsandbytes==0.38.1`\\n* Windows: `pip install https://github.com/jllllll/bitsandbytes-windows-webui/raw/main/bitsandbytes-0.38.1-py3-none-any.whl`\\n\\n### Alternative: Docker\\n\\n```\\nln -s docker/{Dockerfile,docker-compose.yml,.dockerignore} .\\ncp docker/.env.example .env\\n# Edit .env and set TORCH_CUDA_ARCH_LIST based on your GPU model\\ndocker compose up --build\\n```\\n\\n* You need to have docker compose v2.17 or higher installed. See [this guide](https://github.com/oobabooga/text-generation-webui/blob/main/docs/Docker.md) for instructions.\\n* For additional docker files, check out [this repository](https://github.com/Atinoda/text-generation-webui-docker).\\n\\n### Updating the requirements\\n\\nFrom time to time, the `requirements.txt` changes. To update, use this command:\\n\\n```\\nconda activate textgen\\ncd text-generation-webui\\npip install -r requirements.txt --upgrade\\n```\\n## Downloading models\\n\\nModels should be placed inside the `models/` folder.\\n\\n[Hugging Face](https://huggingface.co/models?pipeline_tag=text-generation&sort=downloads) is the main place to download models. These are some examples:\\n\\n* [Pythia](https://huggingface.co/models?sort=downloads&search=eleutherai%2Fpythia+deduped)\\n* [OPT](https://huggingface.co/models?search=facebook/opt)\\n* [GALACTICA](https://huggingface.co/models?search=facebook/galactica)\\n* [GPT-J 6B](https://huggingface.co/EleutherAI/gpt-j-6B/tree/main)\\n\\nYou can automatically download a model from HF using the script `download-model.py`:\\n\\n    python download-model.py organization/model\\n\\nFor example:\\n\\n    python download-model.py facebook/opt-1.3b\\n\\nTo download a protected model, set env vars `HF_USER` and `HF_PASS` to your Hugging Face username and password (or [User Access Token](https://huggingface.co/settings/tokens)). The model\\'s terms must first be accepted on the HF website.\\n\\n#### GGML models\\n\\nYou can drop these directly into the `models/` folder, making sure that the file name contains `ggml` somewhere and ends in `.bin`.\\n\\n#### GPT-4chan\\n\\n<details>\\n<summary>\\nInstructions\\n</summary>\\n\\n[GPT-4chan](https://huggingface.co/ykilcher/gpt-4chan) has been shut down from Hugging Face, so you need to download it elsewhere. You have two options:\\n\\n* Torrent: [16-bit](https://archive.org/details/gpt4chan_model_float16) / [32-bit](https://archive.org/details/gpt4chan_model)\\n* Direct download: [16-bit](https://theswissbay.ch/pdf/_notpdf_/gpt4chan_model_float16/) / [32-bit](https://theswissbay.ch/pdf/_notpdf_/gpt4chan_model/)\\n\\nThe 32-bit version is only relevant if you intend to run the model in CPU mode. Otherwise, you should use the 16-bit version.\\n\\nAfter downloading the model, follow these steps:\\n\\n1. Place the files under `models/gpt4chan_model_float16` or `models/gpt4chan_model`.\\n2. Place GPT-J 6B\\'s config.json file in that same folder: [config.json](https://huggingface.co/EleutherAI/gpt-j-6B/raw/main/config.json).\\n3. Download GPT-J 6B\\'s tokenizer files (they will be automatically detected when you attempt to load GPT-4chan):\\n\\n```\\npython download-model.py EleutherAI/gpt-j-6B --text-only\\n```\\n\\nWhen you load this model in default or notebook modes, the \"HTML\" tab will show the generated text in 4chan format.\\n</details>\\n\\n## Starting the web UI\\n\\n    conda activate textgen\\n    cd text-generation-webui\\n    python server.py\\n\\nThen browse to \\n\\n`http://localhost:7860/?__theme=dark`\\n\\nOptionally, you can use the following command-line flags:\\n\\n#### Basic settings\\n\\n| Flag                                       | Description |\\n|--------------------------------------------|-------------|\\n| `-h`, `--help`                             | Show this help message and exit. |\\n| `--notebook`                               | Launch the web UI in notebook mode, where the output is written to the same text box as the input. |\\n| `--chat`                                   | Launch the web UI in chat mode. |\\n| `--character CHARACTER`                    | The name of the character to load in chat mode by default. |\\n| `--model MODEL`                            | Name of the model to load by default. |\\n| `--lora LORA [LORA ...]`                   | The list of LoRAs to load. If you want to load more than one LoRA, write the names separated by spaces. |\\n| `--model-dir MODEL_DIR`                    | Path to directory with all the models. |\\n| `--lora-dir LORA_DIR`                      | Path to directory with all the loras. |\\n| `--model-menu`                             | Show a model menu in the terminal when the web UI is first launched. |\\n| `--no-stream`                              | Don\\'t stream the text output in real time. |\\n| `--settings SETTINGS_FILE`                 | Load the default interface settings from this yaml file. See `settings-template.yaml` for an example. If you create a file called `settings.yaml`, this file will be loaded by default without the need to use the `--settings` flag. |\\n| `--extensions EXTENSIONS [EXTENSIONS ...]` | The list of extensions to load. If you want to load more than one extension, write the names separated by spaces. |\\n| `--verbose`                                | Print the prompts to the terminal. |\\n\\n#### Accelerate/transformers\\n\\n| Flag                                        | Description |\\n|---------------------------------------------|-------------|\\n| `--cpu`                                     | Use the CPU to generate text. Warning: Training on CPU is extremely slow.|\\n| `--auto-devices`                            | Automatically split the model across the available GPU(s) and CPU. |\\n|  `--gpu-memory GPU_MEMORY [GPU_MEMORY ...]` | Maximum GPU memory in GiB to be allocated per GPU. Example: `--gpu-memory 10` for a single GPU, `--gpu-memory 10 5` for two GPUs. You can also set values in MiB like `--gpu-memory 3500MiB`. |\\n| `--cpu-memory CPU_MEMORY`                   | Maximum CPU memory in GiB to allocate for offloaded weights. Same as above.|\\n| `--disk`                                    | If the model is too large for your GPU(s) and CPU combined, send the remaining layers to the disk. |\\n| `--disk-cache-dir DISK_CACHE_DIR`           | Directory to save the disk cache to. Defaults to `cache/`. |\\n| `--load-in-8bit`                            | Load the model with 8-bit precision (using bitsandbytes).|\\n| `--bf16`                                    | Load the model with bfloat16 precision. Requires NVIDIA Ampere GPU. |\\n| `--no-cache`                                | Set `use_cache` to False while generating text. This reduces the VRAM usage a bit with a performance cost. |\\n| `--xformers`                                | Use xformer\\'s memory efficient attention. This should increase your tokens/s. |\\n| `--sdp-attention`                           | Use torch 2.0\\'s sdp attention. |\\n| `--trust-remote-code`                       | Set trust_remote_code=True while loading a model. Necessary for ChatGLM and Falcon. |\\n\\n#### Accelerate 4-bit\\n\\n\\xe2\\x9a\\xa0\\xef\\xb8\\x8f Requires minimum compute of 7.0 on Windows at the moment.\\n\\n| Flag                                        | Description |\\n|---------------------------------------------|-------------|\\n| `--load-in-4bit`                            | Load the model with 4-bit precision (using bitsandbytes). |\\n| `--compute_dtype COMPUTE_DTYPE`             | compute dtype for 4-bit. Valid options: bfloat16, float16, float32. |\\n| `--quant_type QUANT_TYPE`                   | quant_type for 4-bit. Valid options: nf4, fp4. |\\n| `--use_double_quant`                        | use_double_quant for 4-bit. |\\n\\n#### llama.cpp\\n\\n| Flag        | Description |\\n|-------------|-------------|\\n| `--threads` | Number of threads to use. |\\n| `--n_batch` | Maximum number of prompt tokens to batch together when calling llama_eval. |\\n| `--no-mmap` | Prevent mmap from being used. |\\n| `--mlock`   | Force the system to keep the model in RAM. |\\n| `--cache-capacity CACHE_CAPACITY`   | Maximum cache capacity. Examples: 2000MiB, 2GiB. When provided without units, bytes will be assumed. |\\n| `--n-gpu-layers N_GPU_LAYERS` | Number of layers to offload to the GPU. Only works if llama-cpp-python was compiled with BLAS. Set this to 1000000000 to offload all layers to the GPU. |\\n| `--n_ctx N_CTX` | Size of the prompt context. |\\n| `--llama_cpp_seed SEED` | Seed for llama-cpp models. Default 0 (random). |\\n\\n#### AutoGPTQ\\n\\n| Flag             | Description |\\n|------------------|-------------|\\n| `--triton`       | Use triton. |\\n| `--desc_act`     | For models that don\\'t have a quantize_config.json, this parameter is used to define whether to set desc_act or not in BaseQuantizeConfig. |\\n\\n#### GPTQ-for-LLaMa\\n\\n| Flag                      | Description |\\n|---------------------------|-------------|\\n| `--gptq-for-llama` | Use GPTQ-for-LLaMa to load the GPTQ model instead of AutoGPTQ. |\\n| `--wbits WBITS`           | Load a pre-quantized model with specified precision in bits. 2, 3, 4 and 8 are supported. |\\n| `--model_type MODEL_TYPE` | Model type of pre-quantized model. Currently LLaMA, OPT, and GPT-J are supported. |\\n| `--groupsize GROUPSIZE`   | Group size. |\\n| `--pre_layer PRE_LAYER [PRE_LAYER ...]`  | The number of layers to allocate to the GPU. Setting this parameter enables CPU offloading for 4-bit models. For multi-gpu, write the numbers separated by spaces, eg `--pre_layer 30 60`. |\\n| `--checkpoint CHECKPOINT` | The path to the quantized checkpoint file. If not specified, it will be automatically detected. |\\n| `--monkey-patch`          | Apply the monkey patch for using LoRAs with quantized models.\\n| `--quant_attn`         | (triton) Enable quant attention. |\\n| `--warmup_autotune`    | (triton) Enable warmup autotune. |\\n| `--fused_mlp`          | (triton) Enable fused mlp. |\\n\\n#### FlexGen\\n\\n| Flag             | Description |\\n|------------------|-------------|\\n| `--flexgen`                       | Enable the use of FlexGen offloading. |\\n| `--percent PERCENT [PERCENT ...]` | FlexGen: allocation percentages. Must be 6 numbers separated by spaces (default: 0, 100, 100, 0, 100, 0). |\\n| `--compress-weight`               | FlexGen: Whether to compress weight (default: False).|\\n| `--pin-weight [PIN_WEIGHT]`       | FlexGen: whether to pin weights (setting this to False reduces CPU memory by 20%). |\\n\\n#### DeepSpeed\\n\\n| Flag                                  | Description |\\n|---------------------------------------|-------------|\\n| `--deepspeed`                         | Enable the use of DeepSpeed ZeRO-3 for inference via the Transformers integration. |\\n| `--nvme-offload-dir NVME_OFFLOAD_DIR` | DeepSpeed: Directory to use for ZeRO-3 NVME offloading. |\\n| `--local_rank LOCAL_RANK`             | DeepSpeed: Optional argument for distributed setups. |\\n\\n#### RWKV\\n\\n| Flag                            | Description |\\n|---------------------------------|-------------|\\n| `--rwkv-strategy RWKV_STRATEGY` | RWKV: The strategy to use while loading the model. Examples: \"cpu fp32\", \"cuda fp16\", \"cuda fp16i8\". |\\n| `--rwkv-cuda-on`                | RWKV: Compile the CUDA kernel for better performance. |\\n\\n#### Gradio\\n\\n| Flag                                  | Description |\\n|---------------------------------------|-------------|\\n| `--listen`                            | Make the web UI reachable from your local network. |\\n| `--listen-host LISTEN_HOST`           | The hostname that the server will use. |\\n| `--listen-port LISTEN_PORT`           | The listening port that the server will use. |\\n| `--share`                             | Create a public URL. This is useful for running the web UI on Google Colab or similar. |\\n| `--auto-launch`                       | Open the web UI in the default browser upon launch. |\\n| `--gradio-auth USER:PWD`              | set gradio authentication like \"username:password\"; or comma-delimit multiple like \"u1:p1,u2:p2,u3:p3\" |\\n| `--gradio-auth-path GRADIO_AUTH_PATH` | Set the gradio authentication file path. The file should contain one or more user:password pairs in this format: \"u1:p1,u2:p2,u3:p3\" |\\n\\n#### API\\n\\n| Flag                                  | Description |\\n|---------------------------------------|-------------|\\n| `--api`                               | Enable the API extension. |\\n| `--public-api`                        | Create a public URL for the API using Cloudfare. |\\n| `--api-blocking-port BLOCKING_PORT`   | The listening port for the blocking API. |\\n| `--api-streaming-port STREAMING_PORT` | The listening port for the streaming API. |\\n\\n#### Multimodal\\n\\n| Flag                                  | Description |\\n|---------------------------------------|-------------|\\n| `--multimodal-pipeline PIPELINE`      | The multimodal pipeline to use. Examples: `llava-7b`, `llava-13b`. |\\n\\nOut of memory errors? [Check the low VRAM guide](docs/Low-VRAM-guide.md).\\n\\n## Presets\\n\\nInference settings presets can be created under `presets/` as yaml files. These files are detected automatically at startup.\\n\\nBy default, 10 presets based on NovelAI and KoboldAI presets are included. These were selected out of a sample of 43 presets after applying a K-Means clustering algorithm and selecting the elements closest to the average of each cluster: [tSNE visualization](https://user-images.githubusercontent.com/112222186/228956352-1addbdb9-2456-465a-b51d-089f462cd385.png).\\n\\n## Contributing\\n\\n* Pull requests, suggestions, and issue reports are welcome. \\n* Make sure to carefully [search](https://github.com/oobabooga/text-generation-webui/issues) existing issues before starting a new one.\\n* If you have some experience with git, testing an open pull request and leaving a comment on whether it works as expected or not is immensely helpful.\\n* A simple way to contribute, even if you are not a programmer, is to leave a \\xf0\\x9f\\x91\\x8d on an issue or pull request that you find relevant.\\n\\n## Credits\\n\\n- Gradio dropdown menu refresh button, code for reloading the interface: https://github.com/AUTOMATIC1111/stable-diffusion-webui\\n- NovelAI and KoboldAI presets: https://github.com/KoboldAI/KoboldAI-Client/wiki/Settings-Presets\\n- Code for early stopping in chat mode, code for some of the sliders: https://github.com/PygmalionAI/gradio-ui/\\n'\n",
      "0\n",
      "text-generation-webui-extensions\n",
      "b'# text-generation-webui-extensions\\n\\nThis is a directory of extensions for https://github.com/oobabooga/text-generation-webui\\n\\nIf you create your own extension, you are welcome to submit it to this list in a PR.\\n\\n## long_term_memory\\nA sophisticated extension that creates a long term memory for bots in chat mode. \\n\\nhttps://github.com/wawawario2/long_term_memory\\n\\n## complex_memory\\nA KoboldAI-like memory extension. You create memories that are injected into the context of the conversation, for prompting based on keywords. \\n\\nhttps://github.com/theubie/complex_memory\\n\\n## EdgeGPT\\nExtension for Text Generation Webui based on EdgeGPT by acheong08, for a quick Internet access for your bot.\\n\\nhttps://github.com/GiusTex/EdgeGPT\\n\\n## telegram_bot\\nProvides a cai-chat like telegram bot interface. \\n\\nhttps://github.com/innightwolfsleep/text-generation-webui-telegram_bot\\n\\n## oobabot\\n\\nAnother Discord bot, with both command-line and GUI modes.  Easy setup, lots of config options, and customizable characters!\\n\\n- [**`oobabot`**](https://github.com/chrisrude/oobabot) -- command-line mode, uses Oobabooga\\'s API module\\n\\n- [**`oobabot-plugin`**](https://github.com/chrisrude/oobabot-plugin) -- GUI mode, runs inside of Oobabooga itself\\n\\n<details>\\n<summary>\\nOobabot Screenshots!\\n</summary>\\n\\n| Config UI   | In Action! |\\n| ----------- | ----------- |\\n| ![image](https://raw.githubusercontent.com/chrisrude/oobabot-plugin/main/docs/oobabot-plugin.png) | ![image](https://raw.githubusercontent.com/chrisrude/oobabot/main/docs/zombietaytay.png)|\\n\\n</details>\\n\\nhttps://github.com/chrisrude/oobabot-plugin\\n\\n## bark_tts\\nA simple implementation of Suno-AI\\'s Bark Text-To-Speech with implicit multi-language and simple sound effect support.\\n\\nhttps://github.com/minemo/text-generation-webui-barktts\\n\\n## sd_api_pictures_tag_injection\\nAn expanded version of the included sd_api_pictures extension that features injecting character tags or arbitrary tags upon detection of specific strings into SD side prompt. Greatly improves character self-image stability and allows dynamic usage of LORAs.\\n\\nhttps://github.com/GuizzyQC/sd_api_pictures_tag_injection\\n\\n## code_syntax_highlight\\n<img alt=\"Code Syntax Highlight extension for oobabooga text-generation-webui\" src=\"https://www.davg25.com/file/github-media/text-generation-webui-code_syntax_highlight/extension-header-1.png\" width=\"750\" title=\"Code Syntax Highlight extension\">\\n\\nAn extension that adds syntax highlighting to code snippets, along with a toggleable copy-to-clipboard button and a performance mode for minimal impact on CPU usage.  \\n\\nSupports all interface modes and both light and dark themes.\\n\\n<details>\\n  <summary>Click here to show preview images</summary>\\n  <br>\\n  \\n  | <img alt=\"Code Syntax Highlight extension for oobabooga text-generation-webui\" src=\"https://www.davg25.com/file/github-media/text-generation-webui-code_syntax_highlight/extension-preview-1.png\" width=\"100%\" title=\"Code Syntax Highlight extension\"> |\\n  | :----------------------------------------: |\\n  | <img alt=\"Code Syntax Highlight extension for oobabooga text-generation-webui\" src=\"https://www.davg25.com/file/github-media/text-generation-webui-code_syntax_highlight/extension-preview-2.png\" width=\"100%\" title=\"Code Syntax Highlight extension\"> |\\n\\n</details>\\n\\nhttps://github.com/DavG25/text-generation-webui-code_syntax_highlight\\n\\n## multi_translate\\nAn expanded version of the google_translate extension, that provide more translation options (more engines, save options to file, functionality to toggle on/off translations on the fly).\\n\\nhttps://github.com/janvarev/multi_translate\\n\\n## api_advanced\\n\\nAn expanded version of api extension.\\n1. Provide Kobold-like interface (the same way as \"api\" classic extension)\\n2. **Provide advanced logic to auto-translate income prompts:**\\n    - You need to use multi_translate extension: https://github.com/janvarev/multi_translate\\n    - Set up param `\\'is_advanced_translation\\': True`, (set by default)\\n    - ...see the details in console\\n      - Due to advanced logic script splits income prompt by lines, and **cache translation results**\\n      - **Text quality feature:** when it generate English response, it cache it too (so you don\\'t do double-translation English->UserLang->English next time) \\n\\nhttps://github.com/janvarev/api_advanced\\n\\n## discord_bot\\nDiscord integration for the oobabooga\\'s text-generation-webui (Inspired by DavG25\\'s plugin)\\n\\nCurrently it only sends any response from the chatbot to a discord Webhook of your choosing\\n\\nSimply create a Webhook in Discord following this tutorial and paste the webhook URL under the chat box that will show after the plugin is enabled.\\n\\n<details>\\n  <summary>Click to show preview</summary>\\n  <br>\\n  \\n  ![preview-2](https://user-images.githubusercontent.com/45816945/234896222-532ef597-3e26-48cc-8af2-7df33d471e1b.png)![image](https://user-images.githubusercontent.com/45816945/234899114-09381d3d-3deb-4f1f-8328-2567755cfe40.png)\\n\\n\\n</details>\\n\\nhttps://github.com/ChobPT/text-generation-webui-discord_bot\\n\\n## webui_langchain_agent\\nhttps://github.com/ChobPT/oobaboogas-webui-langchain_agent/\\n\\noobaboogas-webui-langchain_agent\\nCreates an Langchain Agent which uses the WebUI\\'s API and Wikipedia to work and do something for you\\n\\nTested to be barely working, I learned python a couple of weeks ago, bear with me.\\n\\nNeeds \\xc2\\xb4api\\xc2\\xb4 and \\xc2\\xb4no_stream\\xc2\\xb4 enabled.\\n<details>\\n  <summary>Click to show preview</summary>\\n  <br>\\n\\n  ![preview-1](https://user-images.githubusercontent.com/45816945/236649969-0d4fbab3-15e9-4cf0-88d0-71419e77d1cb.png)\\n\\n</details>\\n\\n## UI Tweaks\\nAdds options to keep tabs on page (sticky tabs) and to move extensions into a hidden sidebar. Reduces the need for scrolling up and down. \\n\\n<details>\\n  <summary>Click to show preview</summary>\\n  <br>\\n\\n  ![preview-1](https://github.com/xanthousm/text-gen-webui-ui_tweaks/assets/70198941/c5998420-9607-43d1-865f-65ec0f449ec2)\\n\\n</details>\\n\\n#### Sidebar options:\\n- Open sidebar on startup\\n- Dynamic height (shrink to fit)\\n- Custom width\\n\\nRestart interface to apply setting changes. Save settings by editing params in scipt.py or using settings.json\\n\\nhttps://github.com/xanthousm/text-gen-webui-ui_tweaks\\n\\n## dynamic_context\\nA simple extension that replaces {{time}} and {{date}} on the current character\\'s context with the current time and date respectively.\\nAlso adds time context (and optionally date) to the last prompt to add extra context to the AI response.\\n\\nhttps://github.com/elPatrixF/dynamic_context\\n\\n## Playground for Writers\\nThis extension provides an independent advanced notebook that will be always present from the top tab. It has many features not found in the notebook:\\n- Two independent Notebooks A and B that are always present, regardless of the mode\\n- Inline instruct (abilty to ask question or give task from within the text itself)\\n- Select and Insert - generate text in the middle of your text\\n- Perma Memory, Summarization, Paraphrasing\\n- Lora-Rama - shows LoRA checkpoints and ability to switch between them\\n\\n\\nhttps://github.com/FartyPants/Playground\\n\\n## FPreloader\\nAn essential extension for extensions developers - it will reload your extensions without the need to reboot web ui\\n\\nhttps://github.com/FartyPants/FPreloader\\n\\n\\n\\n## Guidance API\\nAn extension that goes with [guidance](https://github.com/microsoft/guidance/pull/221) in order to enable guidance to be used when generating text\\nfor schemaful data\\n\\nhttps://github.com/danikhan632/guidance_api\\n'\n",
      "0\n",
      "tower_defense_game\n",
      "b'# Tower Defense\\n\\n#M5 Code Smells\\nhttps://docs.google.com/document/d/1DzOozj0WIgBDWqljivdWNNxGmG1cOwLHmlRzlKF39x8/edit?usp=sharing\\n\\n\\n\\nThis new update to M5 adds many new upgrades and unit testing\\n\\n#Bet\\xc3\\xbcl Arpinar\\n\\n#TEST 1/2: test the name input\\n\\nUnit test 1/2: tests whether the name input from the username textbox is valid or not.\\nThe unit test group checks whether the name is too\\nshort(less than 2 characters) or consists only of spaces.\\nI tried to test the null case however the textbox is\\nalways initalized to \"\" so its impossible/redundant.\\nThe test works as expected.\\n\\nName = \"\" => returns false as expected\\nName = \" \" => returns false as expected\\nName = \"a\" => returns false as expected\\nName = \"Bet\\xc3\\xbcl\" => returns true as expected\\nName = \"Daniyal\" => returns true as expected\\nName = \"\\xd8\\xaa\\xd9\\x88\\xd9\\x81\\xd9\\x8a\\xd9\\x82\" => returns true as expected\\n\\nFunction works as expected and handles non-ASCII charachters well.\\n#TEST 3/4/5: Health Decrement\\n\\nChecks whether health is decremented when an enemy hits the monument\\n\\nPasses test if health decreases when enemy hits monument.\\nTested for Easy, Medium, and Hard Levels.\\n\\n\\n\\n#Test 6/7: Tests that difficutly affects the starting money\\n\\nThe unit test group tests that easier difficulties give the Player more money\\n\\n\\'Easy\\' => $150\\n\\'Medium\\' => $100\\n\\'Hard\\' => $50\\n\\'Nightmare\\' => error thrown and caught successfully\\n\\nThe code functions as expected and ensures that difficulty affects the player\\'s wallet.\\nNo potential for any issues.\\n\\nThe unit test group tests that easier difficulties give the Player more money\\n\\n\\'Easy\\' => 75\\n\\'Medium\\' => 50\\n\\'Hard\\' => 25\\n\\'Nightmare\\' => error thrown and caught successfully\\n\\nThis calculation is done using the number for wallet and since the wallet code runs\\nperfectly, I have little reason to believe that the health code could break\\nThe code functions as expected and ensures that difficulty affects the player\\'s wallet.\\nNo potential for any issues.\\n\\n\\nTest 8\\nThis test determines the aggregate damage that current towers can do in the medium mode of the game.\\n\\nTest 9, 10\\nThis test determines whether the final boss health when all enemies are defeated in the easy and medium difficulties is accurate.\\n\\n#Arhum Khan (Daniyal Khan\\'s little brother)\\n#Test 11/12/13: Tests game over functionality for Easy, Medium, Hard based on monument health hitting 0.\\n\\nTest 14\\nThis test determines the aggregate damage that current towers can do in the medium mode of the game.\\n\\n\\xd8\\xaa\\xd9\\x88\\xd9\\x81\\xd9\\x8a\\xd9\\x82 \\xd9\\x85\\xd8\\xad\\xd9\\x85\\xd8\\xaf#\\n\\n#Tests 15/16 Tests whether add functionality adds enemy to canvas AND enemyList\\n\\n#Test 17/18: Tests that the inital objects are loaded\\ninto the canvas when the game starts\\n\\nThese intial components include checking whether the first <Enemy>, <Tower>,\\nand necessary TextComponents are loaded into the canvas.\\nBecause these components are sprite data they called\\nint the onLoad() async method so they\\'re placement can\\ncause issues\\n\\nTest 1: Tests whether all six components are\\nrendering in the main canvas. Six components are expected\\nexpected = 6 | Actual = 6\\n\\nTest 2: Tests whether the first enemy component is loaded and\\nrendering in the main canvas. Since this is called from the\\nSpriteSheet, memory access is minimized.\\n\\nexpected = 1 | Actual = 1\\n\\nTest 3: Tests whether the first Tower component is loaded and\\nrendering in the main canvas. Still unclear whether\\nmonument should be its own unique type of object since\\nit has twin mini-guns that will be implented M4\\n\\nexpected = 1 | Actual = 1\\n\\n\\n\\nTests the cost of all towers at all difficulties\\nThese costs are displayed in the side menu and\\ncome from the inital diffculty number passed in\\n\\nEasy:\\nGun Tower = $16\\nMissile Tower = $33\\nLaser Tower = $50\\n\\nMedium:\\nGun Tower = $25\\nMissile Tower = $50\\nLaser Tower = $75\\n\\nHard:\\nGun Tower = $50\\nMissile Tower = $100\\nLaser Tower = $150\\n\\n  \\nTest 19:\\nThis test determines the aggregate damage that current towers can do in the hard mode of the game.\\n  \\nTest 20:\\nThis test determines whether the final boss health when all enemies are defeated in the hard difficulty is accurate.\\n  \\nTest 21:\\n This Test determines whether the upgrade of the tower does more damage to enemies in easy mode.\\n\\n\\n#Tawfiq Aliu\\n\\n#Test 22/23\\nThis is a massive test grouping that tests the functionallity of\\nside menu store such as buying/placing tower. It checks whether\\nthe wallet has been properly decremented and account for\\nresolution scalling for 1280x720, 1600x900, 1920x1080.\\nPrevents buying towers if there is insufficent funds.\\n  \\n\\n#Test 24/25/26\\nAmount of hits before game over changes based on difficulty\\nEasy is 15, Medium is 10, Hard is 5\\n  \\n#Test 27/28/29\\nThese group of tests test the functionality of the mechanism that determines whether an enemy was hit or not.\\nIt uses the status of the first enemy to test this and determines that status of this enemy at the initial phase of the game. \\nThese tests cover the difficulties easy, medium, and hard.\\n  \\n Test 30, 31:\\n  This Test determines whether the upgrade of the tower does more damage to enemies in medium and hard mode.\\n#Daniyal Khan(Arhum\\'s Older Brother)\\n#Test 30/31\\nPasses in Vector2 coordinate objects and calls the\\nbool onPath() Method. Check whether given points are\\non the path or not on the path.\\nIt also tests each point to account resolution\\nscalling for 1280x720, 1600x900, 1920x1080.\\n\\nThis check is important to ensure the game works properly\\non different resolution devices.\\n\\n#Test 32/33\\nTests whether first instance of enemy added to list is removed.\\n  \\n#Test 34/35\\n Tests whether range of tower to shoot at is accurate for easy and medium difficulties.\\n\\n#Daniyal Khan(Arhum\\'s Older Brother)\\n  \\n#Test 36/37\\nPasses in Vector2 coordinate objects and calls the\\nbool onPath() Method. Check whether given points are\\non the path or not on the path.\\nIt also tests each point to account resolution\\nscalling for 1280x720, 1600x900, 1920x1080.\\n\\n#Test 38/39\\n  \\n\\nThis check is important to ensure the game works properly\\non different resolution devices.\\n  \\n#Test 40/41/42\\n Checks to see if upgrade price is correct for easy, medium, hard difficulty\\n#Test 43\\n Tests whether range of tower to shoot at is accurate for hard difficulty\\n  \\n\\n  \\n  \\n \\n\\n\\n'\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for repo in repos:\n",
    "    print(repo.name)\n",
    "    # print(repo.get_issues(state='all').get_page(0))\n",
    "    try:\n",
    "        doc=repo.get_contents(\"README.md\")\n",
    "        print((doc.decoded_content))\n",
    "    except:\n",
    "        continue\n",
    "    print(repo.stargazers_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Fetch all pages of events\n",
    "all_events = []\n",
    "events = user.get_events()\n",
    "for event in events:\n",
    "    all_events.append(event)\n",
    "\n",
    "# Now, all_events contains all the events from all pages\n",
    "for event in all_events:\n",
    " \n",
    "    if event.type == \"PushEvent\":\n",
    "        print(\"Repo Name:\", event.repo.name)\n",
    "        for commit in event.payload['commits']:\n",
    "            print(\"Commit Message:\", commit['message'])\n",
    "        \n",
    "        \n",
    "    elif event.type == \"IssueCommentEvent\":\n",
    "        print(\"Issue Title:\", event.payload['issue']['title'])\n",
    "        print(\"Issue Body:\", event.payload['issue']['body'])\n",
    "        \n",
    "    elif event.type == \"PullRequestEvent\":\n",
    "        print(\"PR Title:\", event.payload['pull_request']['title'])\n",
    "        print(\"PR Body:\", event.payload['pull_request']['body'])\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
